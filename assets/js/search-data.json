{
  
    
        "post0": {
            "title": "Axis Bank Stock Data Analysis Project Blog Post",
            "content": "AxisBank Stock Data Analysis . The project is based on the dataset I obtained from kaggle. The Analysis I am performing is on the &#39;AXISBANK&#39; stock market data from 2019-2021.AXISBANK is one of the stocks listed in NIFTY50 index. The NIFTY 50 is a benchmark Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange. It is one of the two main stock indices used in India, the other being the BSE SENSEX. The Analysis is performed on the stock quote data of &quot;AXIS BANK&quot; from the dataset of NIFTY50 Stock Market data obtained from kaggle repo. . Axis Bank Limited, formerly known as UTI Bank (1993–2007), is an Indian banking and financial services company headquartered in Mumbai, Maharashtra.It sells financial services to large and mid-size companies, SMEs and retail businesses. . The bank was founded on 3 December 1993 as UTI Bank, opening its registered office in Ahmedabad and a corporate office in Mumbai. The bank was promoted jointly by the Administrator of the Unit Trust of India (UTI), Life Insurance Corporation of India (LIC), General Insurance Corporation, National Insurance Company, The New India Assurance Company, The Oriental Insurance Corporation and United India Insurance Company. The first branch was inaugurated on 2 April 1994 in Ahmedabad by Manmohan Singh, then finance minister of India I chose this dataset because of the importance of NIFTY50 listed stocks on Indian economy. In most ways the NIFTY50 presents how well the Indian capital markets are doing. . Downloading the Dataset . In this section of the Jupyter notebook we are going to download an interesting data set from kaggle dataset repositories. We are using python library called OpenDatasets for downloading from kaggle. While downloading we are asked for kaggle user id and API token key for accessing the dataset from kaggle. Kaggle is a platform used for obtaining datasets and various other datascience tasks. . !pip install jovian opendatasets --upgrade --quiet . Let&#39;s begin by downloading the data, and listing the files within the dataset. . dataset_url = &#39;https://www.kaggle.com/rohanrao/nifty50-stock-market-data&#39; . import opendatasets as od od.download(dataset_url) . Skipping, found downloaded files in &#34;./nifty50-stock-market-data&#34; (use force=True to force download) . The dataset has been downloaded and extracted. . data_dir = &#39;./nifty50-stock-market-data&#39; . import os os.listdir(data_dir) . [&#39;HINDUNILVR.csv&#39;, &#39;GRASIM.csv&#39;, &#39;DRREDDY.csv&#39;, &#39;CIPLA.csv&#39;, &#39;ICICIBANK.csv&#39;, &#39;HDFC.csv&#39;, &#39;BAJAJ-AUTO.csv&#39;, &#39;RELIANCE.csv&#39;, &#39;WIPRO.csv&#39;, &#39;HCLTECH.csv&#39;, &#39;BPCL.csv&#39;, &#39;TECHM.csv&#39;, &#39;COALINDIA.csv&#39;, &#39;MM.csv&#39;, &#39;HINDALCO.csv&#39;, &#39;TATASTEEL.csv&#39;, &#39;INDUSINDBK.csv&#39;, &#39;HDFCBANK.csv&#39;, &#39;VEDL.csv&#39;, &#39;NESTLEIND.csv&#39;, &#39;LT.csv&#39;, &#39;ONGC.csv&#39;, &#39;UPL.csv&#39;, &#39;ITC.csv&#39;, &#39;BRITANNIA.csv&#39;, &#39;ZEEL.csv&#39;, &#39;BAJAJFINSV.csv&#39;, &#39;EICHERMOT.csv&#39;, &#39;TITAN.csv&#39;, &#39;TATAMOTORS.csv&#39;, &#39;NIFTY50_all.csv&#39;, &#39;IOC.csv&#39;, &#39;stock_metadata.csv&#39;, &#39;INFY.csv&#39;, &#39;POWERGRID.csv&#39;, &#39;MARUTI.csv&#39;, &#39;NTPC.csv&#39;, &#39;HEROMOTOCO.csv&#39;, &#39;SHREECEM.csv&#39;, &#39;ASIANPAINT.csv&#39;, &#39;ULTRACEMCO.csv&#39;, &#39;INFRATEL.csv&#39;, &#39;GAIL.csv&#39;, &#39;BAJFINANCE.csv&#39;, &#39;JSWSTEEL.csv&#39;, &#39;ADANIPORTS.csv&#39;, &#39;AXISBANK.csv&#39;, &#39;SUNPHARMA.csv&#39;, &#39;TCS.csv&#39;, &#39;BHARTIARTL.csv&#39;, &#39;KOTAKBANK.csv&#39;, &#39;SBIN.csv&#39;] . Let us save and upload our work to Jovian before continuing. . project_name = &quot;nifty50-stockmarket-data&quot; # change this (use lowercase letters and hyphens only) . !pip install jovian --upgrade -q . import jovian . jovian.commit(project=project_name) . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Data Preparation and Cleaning . Data Preparation and Cleansing constitutes the first part of the Data Analysis project for any dataset. We do this process inorder to obtain retain valuable data from the data frame, one that is relevant for our analysis. The process is also used to remove erroneous values from the dataset(ex. NaN to 0). After the preparation of data and cleansing, the data can be used for analysis.&lt;/br&gt; In our dataframe we have a lot of non-releavant information, so we are going to drop few columns in the dataframe and fix some of the elements in data frame for better analysis. We are also going to change the Date column into DateTime format which can be further used to group the data by months/year. . import pandas as pd import numpy as np . axis_df= pd.read_csv(data_dir + &quot;/AXISBANK.csv&quot;) . axis_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5306 entries, 0 to 5305 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 Date 5306 non-null object 1 Symbol 5306 non-null object 2 Series 5306 non-null object 3 Prev Close 5306 non-null float64 4 Open 5306 non-null float64 5 High 5306 non-null float64 6 Low 5306 non-null float64 7 Last 5306 non-null float64 8 Close 5306 non-null float64 9 VWAP 5306 non-null float64 10 Volume 5306 non-null int64 11 Turnover 5306 non-null float64 12 Trades 2456 non-null float64 13 Deliverable Volume 4797 non-null float64 14 %Deliverble 4797 non-null float64 dtypes: float64(11), int64(1), object(3) memory usage: 621.9+ KB . axis_df.describe() . Prev Close Open High Low Last Close VWAP Volume Turnover Trades Deliverable Volume %Deliverble . count 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5.306000e+03 | 5.306000e+03 | 2456.000000 | 4.797000e+03 | 4797.000000 | . mean 585.763852 | 586.507388 | 596.476187 | 575.571598 | 585.897399 | 585.893931 | 586.077778 | 4.527938e+06 | 2.739871e+14 | 120602.231678 | 1.990907e+06 | 0.466962 | . std 436.714128 | 436.602194 | 443.044833 | 430.108921 | 436.609147 | 436.649765 | 436.611987 | 8.101940e+06 | 4.122431e+14 | 96106.654046 | 3.264587e+06 | 0.161808 | . min 22.150000 | 21.000000 | 23.700000 | 21.000000 | 22.150000 | 22.150000 | 22.170000 | 2.850000e+03 | 8.275250e+09 | 2698.000000 | 5.809000e+03 | 0.075000 | . 25% 230.950000 | 232.000000 | 235.125000 | 227.075000 | 230.550000 | 230.975000 | 231.115000 | 2.842172e+05 | 5.868745e+12 | 62228.250000 | 2.573130e+05 | 0.347500 | . 50% 519.450000 | 520.100000 | 528.400000 | 512.025000 | 519.425000 | 519.500000 | 519.505000 | 1.656966e+06 | 1.653257e+14 | 93186.500000 | 7.687680e+05 | 0.459800 | . 75% 877.312500 | 880.075000 | 897.987500 | 852.762500 | 877.275000 | 877.312500 | 875.807500 | 5.515245e+06 | 3.456528e+14 | 144973.250000 | 2.652520e+06 | 0.573900 | . max 2023.350000 | 2034.400000 | 2043.050000 | 2002.600000 | 2022.550000 | 2023.350000 | 2020.310000 | 1.205419e+08 | 7.179550e+15 | 990737.000000 | 9.490116e+07 | 0.983000 | . axis_df . Date Symbol Series Prev Close Open High Low Last Close VWAP Volume Turnover Trades Deliverable Volume %Deliverble . 0 2000-01-03 | UTIBANK | EQ | 24.70 | 26.7 | 26.70 | 26.70 | 26.70 | 26.70 | 26.70 | 112100 | 2.993070e+11 | NaN | NaN | NaN | . 1 2000-01-04 | UTIBANK | EQ | 26.70 | 27.0 | 28.70 | 26.50 | 27.00 | 26.85 | 27.24 | 234500 | 6.387275e+11 | NaN | NaN | NaN | . 2 2000-01-05 | UTIBANK | EQ | 26.85 | 26.0 | 27.75 | 25.50 | 26.40 | 26.30 | 26.24 | 170100 | 4.462980e+11 | NaN | NaN | NaN | . 3 2000-01-06 | UTIBANK | EQ | 26.30 | 25.8 | 27.00 | 25.80 | 25.90 | 25.95 | 26.27 | 102100 | 2.681730e+11 | NaN | NaN | NaN | . 4 2000-01-07 | UTIBANK | EQ | 25.95 | 25.0 | 26.00 | 24.25 | 25.00 | 24.80 | 25.04 | 62600 | 1.567220e+11 | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | EQ | 671.35 | 694.0 | 703.80 | 684.50 | 699.50 | 700.45 | 695.33 | 21646184 | 1.505120e+15 | 286480.0 | 5949937.0 | 0.2749 | . 5302 2021-04-27 | AXISBANK | EQ | 700.45 | 691.1 | 703.90 | 684.10 | 700.90 | 699.55 | 692.83 | 46559967 | 3.225830e+15 | 289445.0 | 18080082.0 | 0.3883 | . 5303 2021-04-28 | AXISBANK | EQ | 699.55 | 708.0 | 712.50 | 688.15 | 705.95 | 708.15 | 701.92 | 54060587 | 3.794635e+15 | 507747.0 | 17851331.0 | 0.3302 | . 5304 2021-04-29 | AXISBANK | EQ | 708.15 | 712.0 | 726.90 | 707.00 | 717.10 | 719.40 | 717.41 | 25939327 | 1.860920e+15 | 312079.0 | 7357520.0 | 0.2836 | . 5305 2021-04-30 | AXISBANK | EQ | 719.40 | 705.0 | 729.85 | 705.00 | 711.65 | 714.90 | 719.36 | 23011654 | 1.655365e+15 | 232879.0 | 6786072.0 | 0.2949 | . 5306 rows × 15 columns . axis_df[&#39;Symbol&#39;] = np.where(axis_df[&#39;Symbol&#39;] == &#39;UTIBANK&#39;, &#39;AXISBANK&#39;, axis_df[&#39;Symbol&#39;]) axis_df . Date Symbol Series Prev Close Open High Low Last Close VWAP Volume Turnover Trades Deliverable Volume %Deliverble . 0 2000-01-03 | AXISBANK | EQ | 24.70 | 26.7 | 26.70 | 26.70 | 26.70 | 26.70 | 26.70 | 112100 | 2.993070e+11 | NaN | NaN | NaN | . 1 2000-01-04 | AXISBANK | EQ | 26.70 | 27.0 | 28.70 | 26.50 | 27.00 | 26.85 | 27.24 | 234500 | 6.387275e+11 | NaN | NaN | NaN | . 2 2000-01-05 | AXISBANK | EQ | 26.85 | 26.0 | 27.75 | 25.50 | 26.40 | 26.30 | 26.24 | 170100 | 4.462980e+11 | NaN | NaN | NaN | . 3 2000-01-06 | AXISBANK | EQ | 26.30 | 25.8 | 27.00 | 25.80 | 25.90 | 25.95 | 26.27 | 102100 | 2.681730e+11 | NaN | NaN | NaN | . 4 2000-01-07 | AXISBANK | EQ | 25.95 | 25.0 | 26.00 | 24.25 | 25.00 | 24.80 | 25.04 | 62600 | 1.567220e+11 | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | EQ | 671.35 | 694.0 | 703.80 | 684.50 | 699.50 | 700.45 | 695.33 | 21646184 | 1.505120e+15 | 286480.0 | 5949937.0 | 0.2749 | . 5302 2021-04-27 | AXISBANK | EQ | 700.45 | 691.1 | 703.90 | 684.10 | 700.90 | 699.55 | 692.83 | 46559967 | 3.225830e+15 | 289445.0 | 18080082.0 | 0.3883 | . 5303 2021-04-28 | AXISBANK | EQ | 699.55 | 708.0 | 712.50 | 688.15 | 705.95 | 708.15 | 701.92 | 54060587 | 3.794635e+15 | 507747.0 | 17851331.0 | 0.3302 | . 5304 2021-04-29 | AXISBANK | EQ | 708.15 | 712.0 | 726.90 | 707.00 | 717.10 | 719.40 | 717.41 | 25939327 | 1.860920e+15 | 312079.0 | 7357520.0 | 0.2836 | . 5305 2021-04-30 | AXISBANK | EQ | 719.40 | 705.0 | 729.85 | 705.00 | 711.65 | 714.90 | 719.36 | 23011654 | 1.655365e+15 | 232879.0 | 6786072.0 | 0.2949 | . 5306 rows × 15 columns . axis_new_df = axis_df.drop([&#39;Last&#39;,&#39;Series&#39;, &#39;VWAP&#39;, &#39;Trades&#39;,&#39;Deliverable Volume&#39;,&#39;%Deliverble&#39;], axis=1) axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover . 0 2000-01-03 | AXISBANK | 24.70 | 26.7 | 26.70 | 26.70 | 26.70 | 112100 | 2.993070e+11 | . 1 2000-01-04 | AXISBANK | 26.70 | 27.0 | 28.70 | 26.50 | 26.85 | 234500 | 6.387275e+11 | . 2 2000-01-05 | AXISBANK | 26.85 | 26.0 | 27.75 | 25.50 | 26.30 | 170100 | 4.462980e+11 | . 3 2000-01-06 | AXISBANK | 26.30 | 25.8 | 27.00 | 25.80 | 25.95 | 102100 | 2.681730e+11 | . 4 2000-01-07 | AXISBANK | 25.95 | 25.0 | 26.00 | 24.25 | 24.80 | 62600 | 1.567220e+11 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | . 5302 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | . 5303 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | . 5304 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | . 5305 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | . 5306 rows × 9 columns . def getIndexes(dfObj, value): &#39;&#39;&#39; Get index positions of value in dataframe i.e. dfObj.&#39;&#39;&#39; listOfPos = list() # Get bool dataframe with True at positions where the given value exists result = dfObj.isin([value]) # Get list of columns that contains the value seriesObj = result.any() columnNames = list(seriesObj[seriesObj == True].index) # Iterate over list of columns and fetch the rows indexes where value exists for col in columnNames: rows = list(result[col][result[col] == True].index) for row in rows: listOfPos.append((row, col)) # Return a list of tuples indicating the positions of value in the dataframe return listOfPos . listOfPosition_axis = getIndexes(axis_df, &#39;2019-01-01&#39;) listOfPosition_axis . [(4729, &#39;Date&#39;)] . axis_new_df.drop(axis_new_df.loc[0:4728].index, inplace = True) . axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover . 4729 2019-01-01 | AXISBANK | 619.90 | 621.9 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | . 4730 2019-01-02 | AXISBANK | 627.30 | 623.0 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | . 4731 2019-01-03 | AXISBANK | 620.05 | 621.4 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | . 4732 2019-01-04 | AXISBANK | 607.95 | 612.0 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | . 4733 2019-01-07 | AXISBANK | 619.60 | 626.0 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | . 5302 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | . 5303 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | . 5304 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | . 5305 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | . 577 rows × 9 columns . Summary of the operations done till now: . we have taken a csv file containing stock data of AXIS BANK from the data set of nifty50 stocks and performed data cleansing operations on them.&lt;/br&gt; | Originally, the data from the data set is noticed as stock price quotations from the year 2001 but for our analysis we have taken data for the years 2019-2021&lt;/br&gt; | Then we have dropped the columns that are not relevant for our analysis by using pandas dataframe operations. | axis_new_df.reset_index(drop=True, inplace=True) axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover . 0 2019-01-01 | AXISBANK | 619.90 | 621.9 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | . 1 2019-01-02 | AXISBANK | 627.30 | 623.0 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | . 2 2019-01-03 | AXISBANK | 620.05 | 621.4 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | . 3 2019-01-04 | AXISBANK | 607.95 | 612.0 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | . 4 2019-01-07 | AXISBANK | 619.60 | 626.0 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 572 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | . 573 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | . 574 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | . 575 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | . 576 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | . 577 rows × 9 columns . axis_new_df[&#39;Date&#39;] = pd.to_datetime(axis_new_df[&#39;Date&#39;]) # we changed the Dates into Datetime format from the object format axis_new_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 577 entries, 0 to 576 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Date 577 non-null datetime64[ns] 1 Symbol 577 non-null object 2 Prev Close 577 non-null float64 3 Open 577 non-null float64 4 High 577 non-null float64 5 Low 577 non-null float64 6 Close 577 non-null float64 7 Volume 577 non-null int64 8 Turnover 577 non-null float64 dtypes: datetime64[ns](1), float64(6), int64(1), object(1) memory usage: 40.7+ KB . axis_new_df[&#39;Daily Lag&#39;] = axis_new_df[&#39;Close&#39;].shift(1) # Added a new column Daily Lag to calculate daily returns of the stock axis_new_df[&#39;Daily Returns&#39;] = (axis_new_df[&#39;Daily Lag&#39;]/axis_new_df[&#39;Close&#39;]) -1 . axis_dailyret_df = axis_new_df.drop([&#39;Prev Close&#39;, &#39;Open&#39;,&#39;High&#39;, &#39;Low&#39;,&#39;Close&#39;,&#39;Daily Lag&#39;], axis = 1) . axis_dailyret_df . Date Symbol Volume Turnover Daily Returns . 0 2019-01-01 | AXISBANK | 12179223 | 7.609460e+14 | NaN | . 1 2019-01-02 | AXISBANK | 12386281 | 7.720546e+14 | 0.011693 | . 2 2019-01-03 | AXISBANK | 13228602 | 8.092834e+14 | 0.019903 | . 3 2019-01-04 | AXISBANK | 8381367 | 5.178678e+14 | -0.018802 | . 4 2019-01-07 | AXISBANK | 11735286 | 7.463059e+14 | -0.028002 | . ... ... | ... | ... | ... | ... | . 572 2021-04-26 | AXISBANK | 21646184 | 1.505120e+15 | -0.041545 | . 573 2021-04-27 | AXISBANK | 46559967 | 3.225830e+15 | 0.001287 | . 574 2021-04-28 | AXISBANK | 54060587 | 3.794635e+15 | -0.012144 | . 575 2021-04-29 | AXISBANK | 25939327 | 1.860920e+15 | -0.015638 | . 576 2021-04-30 | AXISBANK | 23011654 | 1.655365e+15 | 0.006295 | . 577 rows × 5 columns . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Exploratory Analysis and Visualization . Here we compute the mean, max/min stock quotes of the stock AXISBANK. We specifically compute the mean of the Daily returns column. we are going to do the analysis by first converting the index datewise to month wise to have a good consolidated dataframe to analyze in broad timeline. we are going to divide the data frame into three for the years 2019, 2020, 2021 respectively, in order to analyze the yearly performance of the stock. . Let&#39;s begin by importingmatplotlib.pyplot and seaborn. . import seaborn as sns import matplotlib import matplotlib.pyplot as plt %matplotlib inline sns.set_style(&#39;darkgrid&#39;) matplotlib.rcParams[&#39;font.size&#39;] = 10 matplotlib.rcParams[&#39;figure.figsize&#39;] = (15, 5) matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#00000000&#39; . Here we are going to explore the daily Returns column by plotting a line graph of daily returns v/s Months. Now we can see that daily returns are growing across months in the years 2019-2021. . axis_dailyret_plot=axis_dailyret_df.groupby(axis_dailyret_df[&#39;Date&#39;].dt.strftime(&#39;%B&#39;))[&#39;Daily Returns&#39;].sum().sort_values() plt.plot(axis_dailyret_plot) . [&lt;matplotlib.lines.Line2D at 0x7fd4abea4400&gt;] . axis_new_df[&#39;Year&#39;] = pd.DatetimeIndex(axis_new_df[&#39;Date&#39;]).year axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2019-01-01 | AXISBANK | 619.90 | 621.9 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | NaN | NaN | 2019 | . 1 2019-01-02 | AXISBANK | 627.30 | 623.0 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | 627.30 | 0.011693 | 2019 | . 2 2019-01-03 | AXISBANK | 620.05 | 621.4 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | 620.05 | 0.019903 | 2019 | . 3 2019-01-04 | AXISBANK | 607.95 | 612.0 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | 607.95 | -0.018802 | 2019 | . 4 2019-01-07 | AXISBANK | 619.60 | 626.0 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | 619.60 | -0.028002 | 2019 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 572 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | 671.35 | -0.041545 | 2021 | . 573 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | 700.45 | 0.001287 | 2021 | . 574 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | 699.55 | -0.012144 | 2021 | . 575 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | 708.15 | -0.015638 | 2021 | . 576 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | 719.40 | 0.006295 | 2021 | . 577 rows × 12 columns . axis2019_df = axis_new_df[axis_new_df.Year == 2019 ] axis2020_df = axis_new_df[axis_new_df.Year == 2020 ] axis2021_df = axis_new_df[axis_new_df.Year == 2021 ] . axis2019_df.reset_index(drop = True, inplace = True) axis2019_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2019-01-01 | AXISBANK | 619.90 | 621.90 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | NaN | NaN | 2019 | . 1 2019-01-02 | AXISBANK | 627.30 | 623.00 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | 627.30 | 0.011693 | 2019 | . 2 2019-01-03 | AXISBANK | 620.05 | 621.40 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | 620.05 | 0.019903 | 2019 | . 3 2019-01-04 | AXISBANK | 607.95 | 612.00 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | 607.95 | -0.018802 | 2019 | . 4 2019-01-07 | AXISBANK | 619.60 | 626.00 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | 619.60 | -0.028002 | 2019 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 240 2019-12-24 | AXISBANK | 743.15 | 744.50 | 744.70 | 737.80 | 740.65 | 3642916 | 2.698164e+14 | 743.15 | 0.003375 | 2019 | . 241 2019-12-26 | AXISBANK | 740.65 | 737.50 | 740.65 | 733.90 | 736.50 | 7919368 | 5.836254e+14 | 740.65 | 0.005635 | 2019 | . 242 2019-12-27 | AXISBANK | 736.50 | 739.00 | 762.00 | 736.30 | 760.15 | 10736285 | 8.089429e+14 | 736.50 | -0.031112 | 2019 | . 243 2019-12-30 | AXISBANK | 760.15 | 760.90 | 765.75 | 751.05 | 754.10 | 10034206 | 7.598128e+14 | 760.15 | 0.008023 | 2019 | . 244 2019-12-31 | AXISBANK | 754.10 | 753.85 | 765.85 | 751.40 | 754.10 | 11660781 | 8.842549e+14 | 754.10 | 0.000000 | 2019 | . 245 rows × 12 columns . axis2020_df.reset_index(drop = True, inplace = True) axis2020_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2020-01-01 | AXISBANK | 754.10 | 754.90 | 759.95 | 747.20 | 748.70 | 4917748 | 3.697221e+14 | 754.10 | 0.007213 | 2020 | . 1 2020-01-02 | AXISBANK | 748.70 | 750.00 | 759.00 | 747.60 | 756.95 | 5156046 | 3.886739e+14 | 748.70 | -0.010899 | 2020 | . 2 2020-01-03 | AXISBANK | 756.95 | 753.15 | 756.25 | 740.50 | 742.95 | 8489729 | 6.336165e+14 | 756.95 | 0.018844 | 2020 | . 3 2020-01-06 | AXISBANK | 742.95 | 739.45 | 739.60 | 721.70 | 723.25 | 6356198 | 4.631502e+14 | 742.95 | 0.027238 | 2020 | . 4 2020-01-07 | AXISBANK | 723.25 | 728.00 | 738.00 | 721.05 | 725.75 | 9103360 | 6.630907e+14 | 723.25 | -0.003445 | 2020 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 247 2020-12-24 | AXISBANK | 592.45 | 595.00 | 614.00 | 594.15 | 610.20 | 15488083 | 9.422775e+14 | 592.45 | -0.029089 | 2020 | . 248 2020-12-28 | AXISBANK | 610.20 | 614.00 | 620.80 | 614.00 | 617.65 | 8444506 | 5.220025e+14 | 610.20 | -0.012062 | 2020 | . 249 2020-12-29 | AXISBANK | 617.65 | 620.70 | 632.40 | 618.60 | 630.20 | 13765454 | 8.632919e+14 | 617.65 | -0.019914 | 2020 | . 250 2020-12-30 | AXISBANK | 630.20 | 632.25 | 634.00 | 618.20 | 625.10 | 10262221 | 6.407812e+14 | 630.20 | 0.008159 | 2020 | . 251 2020-12-31 | AXISBANK | 625.10 | 622.40 | 625.95 | 616.00 | 620.45 | 12306502 | 7.636264e+14 | 625.10 | 0.007495 | 2020 | . 252 rows × 12 columns . axis2021_df.reset_index(drop=True, inplace=True) axis2021_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2021-01-01 | AXISBANK | 620.45 | 620.25 | 625.45 | 617.55 | 623.80 | 6047062 | 3.762202e+14 | 620.45 | -0.005370 | 2021 | . 1 2021-01-04 | AXISBANK | 623.80 | 627.80 | 633.00 | 621.30 | 624.70 | 14068156 | 8.811802e+14 | 623.80 | -0.001441 | 2021 | . 2 2021-01-05 | AXISBANK | 624.70 | 618.00 | 667.90 | 618.00 | 664.45 | 37973963 | 2.467637e+15 | 624.70 | -0.059824 | 2021 | . 3 2021-01-06 | AXISBANK | 664.45 | 662.00 | 667.15 | 649.15 | 654.25 | 20829645 | 1.373394e+15 | 664.45 | 0.015590 | 2021 | . 4 2021-01-07 | AXISBANK | 654.25 | 659.00 | 676.50 | 659.00 | 671.10 | 17887570 | 1.196294e+15 | 654.25 | -0.025108 | 2021 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 75 2021-04-26 | AXISBANK | 671.35 | 694.00 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | 671.35 | -0.041545 | 2021 | . 76 2021-04-27 | AXISBANK | 700.45 | 691.10 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | 700.45 | 0.001287 | 2021 | . 77 2021-04-28 | AXISBANK | 699.55 | 708.00 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | 699.55 | -0.012144 | 2021 | . 78 2021-04-29 | AXISBANK | 708.15 | 712.00 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | 708.15 | -0.015638 | 2021 | . 79 2021-04-30 | AXISBANK | 719.40 | 705.00 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | 719.40 | 0.006295 | 2021 | . 80 rows × 12 columns . Summary of above exploratory Analysis: . In the above code cells, we performed plotting of the data by exploring a column from the data. We have divided the DataFrame into three data frames containing the stock quote data from year-wise i.e., for the years 2019, 2020, 2021. For dividing the DataFrame year-wise we have added a new column called &#39;Year&#39; which is generated from the DataTime values of the column &quot;Date&quot;. . axis_range_df = axis_dailyret_df[&#39;Daily Returns&#39;].max() - axis_dailyret_df[&#39;Daily Returns&#39;].min() axis_range_df . 0.55021480949789 . axis_mean_df = axis_dailyret_df[&#39;Daily Returns&#39;].mean() axis_mean_df . 0.0002473086431186587 . In the above two code cells, we have computed the range i.e. the difference between maximum and minimum value of the column. We have also calculated the mean of the daily returns of the Axis Bank stock. . Exploratory Analysis of stock quotes year-wise for Axis Bank: . In this section we have plotted the Closing values of the stock throughout the year for the years 2019,2020,2021. We have only partial data for 2021(i.e. till Apr 2021). We have also done a plot to compare the performance throughout the year for the years 2019 and 2020(since we had full data for the respective years). . plt.plot(axis2019_df[&#39;Date&#39;],axis2019_df[&#39;Close&#39;] ) plt.title(&#39;Closing Values of stock for the year 2019&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . plt.plot(axis2020_df[&#39;Date&#39;],axis2020_df[&#39;Close&#39;]) plt.title(&#39;Closing Values of stock for the year 2020&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . plt.plot(axis2021_df[&#39;Date&#39;],axis2021_df[&#39;Close&#39;]) plt.title(&#39;Closing Values of stock for the year 2021 Till April Month&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . TODO - Explore one or more columns by plotting a graph below, and add some explanation about it . plt.style.use(&#39;fivethirtyeight&#39;) plt.plot(axis2019_df[&#39;Date&#39;], axis2019_df[&#39;Close&#39;],linewidth=3, label = &#39;2019&#39;) plt.plot(axis2020_df[&quot;Date&quot;],axis2020_df[&#39;Close&#39;],linewidth=3, label = &#39;2020&#39;) plt.legend(loc=&#39;best&#39; ) plt.title(&#39;Closing Values of stock for the years 2019 and 2020&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . print(plt.style.available) . [&#39;Solarize_Light2&#39;, &#39;_classic_test_patch&#39;, &#39;bmh&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;fast&#39;, &#39;fivethirtyeight&#39;, &#39;ggplot&#39;, &#39;grayscale&#39;, &#39;seaborn&#39;, &#39;seaborn-bright&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-dark&#39;, &#39;seaborn-dark-palette&#39;, &#39;seaborn-darkgrid&#39;, &#39;seaborn-deep&#39;, &#39;seaborn-muted&#39;, &#39;seaborn-notebook&#39;, &#39;seaborn-paper&#39;, &#39;seaborn-pastel&#39;, &#39;seaborn-poster&#39;, &#39;seaborn-talk&#39;, &#39;seaborn-ticks&#39;, &#39;seaborn-white&#39;, &#39;seaborn-whitegrid&#39;, &#39;tableau-colorblind10&#39;] . Let us save and upload our work to Jovian before continuing . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Asking and Answering Questions . In this section, we are going to answer some of the questions regarding the dataset using various data analysis libraries like Numpy, Pandas, Matplotlib and seaborn. By using the tools we can see how useful the libraries come in handy while doing Inference on a dataset. . Instructions (delete this cell) . Ask at least 5 interesting questions about your dataset | Answer the questions either by computing the results using Numpy/Pandas or by plotting graphs using Matplotlib/Seaborn | Create new columns, merge multiple dataset and perform grouping/aggregation wherever necessary | Wherever you&#39;re using a library function from Pandas/Numpy/Matplotlib etc. explain briefly what it does | . Q1: What was the change in price and volume of the stock traded overtime? . plt.plot(axis2019_df[&#39;Date&#39;], axis2019_df[&#39;Close&#39;],linewidth=3, label = &#39;2019&#39;) plt.plot(axis2020_df[&quot;Date&quot;],axis2020_df[&#39;Close&#39;],linewidth=3, label = &#39;2020&#39;) plt.plot(axis2021_df[&quot;Date&quot;], axis2021_df[&#39;Close&#39;],linewidth = 3, label = &#39;2021&#39;) plt.legend(loc=&#39;best&#39; ) plt.title(&#39;Closing Price of stock for the years 2019-2021(Till April)&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . print(&#39;The Maximum closing price of the stock during 2019-2021 is&#39;,axis_new_df[&#39;Close&#39;].max()) print(&#39;The Minimum closing price of the stock during 2019-2021 is&#39;,axis_new_df[&#39;Close&#39;].min()) print(&#39;The Index for the Maximum closing price in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Close&#39;].max())) print(&#39;The Index for the Minimum closing price in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Close&#39;].min())) print(axis_new_df.iloc[104]) print(axis_new_df.iloc[303]) . The Maximum closing price of the stock during 2019-2021 is 822.8 The Minimum closing price of the stock during 2019-2021 is 303.15 The Index for the Maximum closing price in the dataframe is [(105, &#39;Prev Close&#39;), (104, &#39;Close&#39;), (105, &#39;Daily Lag&#39;)] The Index for the Minimum closing price in the dataframe is [(304, &#39;Prev Close&#39;), (303, &#39;Close&#39;), (304, &#39;Daily Lag&#39;)] Date 2019-06-04 00:00:00 Symbol AXISBANK Prev Close 812.65 Open 807.55 High 827.75 Low 805.5 Close 822.8 Volume 9515354 Turnover 778700415970000.0 Daily Lag 812.65 Daily Returns -0.012336 Year 2019 Name: 104, dtype: object Date 2020-03-24 00:00:00 Symbol AXISBANK Prev Close 308.65 Open 331.95 High 337.5 Low 291.0 Close 303.15 Volume 50683611 Turnover 1578313503950000.0 Daily Lag 308.65 Daily Returns 0.018143 Year 2020 Name: 303, dtype: object . As we can see from the above one of the two plots there was a dip in the closing price during the year 2020. The Maximum Closing price occurred on 2019-06-04(Close = 822.8). The lowest of closing price during the years occurred on 2020-03-24(Close = 303.15). This can say that the start of the pandemic has caused the steep down curve for the stock&#39;s closing price. | . plt.plot(axis2019_df[&quot;Date&quot;],axis2019_df[&quot;Volume&quot;],linewidth=2, label = &#39;2019&#39;) plt.plot(axis2020_df[&quot;Date&quot;],axis2020_df[&quot;Volume&quot;],linewidth=2, label = &#39;2020&#39;) plt.plot(axis2021_df[&quot;Date&quot;],axis2021_df[&quot;Volume&quot;],linewidth=2, label = &#39;2021&#39;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Volume of stock traded in the years 2019-2021(till April)&#39;) plt.ylabel(&#39;Volume&#39;) plt.xlabel(None) . Text(0.5, 0, &#39;&#39;) . print(&#39;The Maximum volume of the stock traded during 2019-2021 is&#39;,axis_new_df[&#39;Volume&#39;].max()) print(&#39;The Minimum volume of the stock traded during 2019-2021 is&#39;,axis_new_df[&#39;Volume&#39;].min()) print(&#39;The Index for the Maximum volume stock traded in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Volume&#39;].max())) print(&#39;The Index for the Minimum volume stock traded in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Volume&#39;].min())) print(axis_new_df.iloc[357]) print(axis_new_df.iloc[200]) . The Maximum volume of the stock traded during 2019-2021 is 96190274 The Minimum volume of the stock traded during 2019-2021 is 965772 The Index for the Maximum volume stock traded in the dataframe is [(357, &#39;Volume&#39;)] The Index for the Minimum volume stock traded in the dataframe is [(200, &#39;Volume&#39;)] Date 2020-06-16 00:00:00 Symbol AXISBANK Prev Close 389.6 Open 404.9 High 405.0 Low 360.4 Close 381.55 Volume 96190274 Turnover 3654065942305001.0 Daily Lag 389.6 Daily Returns 0.021098 Year 2020 Name: 357, dtype: object Date 2019-10-27 00:00:00 Symbol AXISBANK Prev Close 708.6 Open 711.0 High 715.05 Low 708.55 Close 710.1 Volume 965772 Turnover 68696126654999.992188 Daily Lag 708.6 Daily Returns -0.002112 Year 2019 Name: 200, dtype: object . As we can see from the above graph a lot of volume of trade happened during 2020. That means the stock was transacted a lot during the year 2020. The highest Volumed of stock is traded on 2020-06-16(Volume =96190274) and the Minimum volume of the stock traded during 2019-2021 is on 2019-10-27(Volume = 965772) . Q2: What was the daily return of the stock on average? . The daily return measures the price change in a stock&#39;s price as a percentage of the previous day&#39;s closing price. A positive return means the stock has grown in value, while a negative return means it has lost value. we will also attempt to calculate the maximum daily return of the stock during 2019-2021. . plt.plot(axis_new_df[&#39;Date&#39;],axis_new_df[&#39;Daily Returns&#39;], linewidth=2 ,label = &#39;Daily Returns&#39;) plt.legend(loc=&#39;best&#39; ) plt.title(&#39;Daily Returns of stock for the years 2019-2021(Till April)&#39;) plt.xlabel(None) plt.ylabel(&#39;Daily Returns of the stock&#39;) . Text(0, 0.5, &#39;Daily Returns of the stock&#39;) . plt.plot(axis_new_df[&#39;Date&#39;],axis_new_df[&#39;Daily Returns&#39;], linestyle=&#39;--&#39;, marker=&#39;o&#39;) plt.title(&#39;Daily Returns of stock for the years 2019-2021(Till April)&#39;) plt.xlabel(None) plt.ylabel(&#39;Daily Returns of the stock&#39;) . Text(0, 0.5, &#39;Daily Returns of the stock&#39;) . print(&#39;The Maximum daily return during the years 2020 is&#39;,axis_new_df[&#39;Daily Returns&#39;].max()) index = getIndexes(axis_new_df, axis_new_df[&#39;Daily Returns&#39;].max()) axis_new_df.iloc[302] . The Maximum daily return during the years 2020 is 0.3871699335817269 . Date 2020-03-23 00:00:00 Symbol AXISBANK Prev Close 428.15 Open 385.35 High 392.0 Low 302.0 Close 308.65 Volume 37622791 Turnover 1253563689110000.0 Daily Lag 428.15 Daily Returns 0.38717 Year 2020 Name: 302, dtype: object . def getIndexes(dfObj, value): &#39;&#39;&#39; Get index positions of value in dataframe i.e. dfObj.&#39;&#39;&#39; listOfPos = list() # Get bool dataframe with True at positions where the given value exists result = dfObj.isin([value]) # Get list of columns that contains the value seriesObj = result.any() columnNames = list(seriesObj[seriesObj == True].index) # Iterate over list of columns and fetch the rows indexes where value exists for col in columnNames: rows = list(result[col][result[col] == True].index) for row in rows: listOfPos.append((row, col)) # Return a list of tuples indicating the positions of value in the dataframe return listOfPos . As we can see from the plot there were high daily returns for the stock around late March 2020 and then there was ups and downs from April- July 2020 . we can see that the most changes in daily returns occurred during April 2020 - July 2020 and at other times the daily returns were almost flat. The maximum daily returns for the stock during 2019-2021 occurred on 2020-03-23(observed from the pandas table above). . Avgdailyret_2019 =axis2019_df[&#39;Daily Returns&#39;].sum()/len(axis2019_df[&#39;Daily Returns&#39;]) Avgdailyret_2020 =axis2020_df[&#39;Daily Returns&#39;].sum()/len(axis2020_df[&#39;Daily Returns&#39;]) Avgdailyret_2021 =axis2021_df[&#39;Daily Returns&#39;].sum()/len(axis2021_df[&#39;Daily Returns&#39;]) # create a dataset data_dailyret = {&#39;2019&#39;: Avgdailyret_2019, &#39;2020&#39;:Avgdailyret_2020, &#39;2021&#39;:Avgdailyret_2021} Years = list(data_dailyret.keys()) Avgdailyret = list(data_dailyret.values()) # plotting a bar chart plt.figure(figsize=(10, 7)) plt.bar(Years, Avgdailyret, color =&#39;maroon&#39;,width = 0.3) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Average Daily Returns of the Stock Traded&quot;) plt.title(&quot;Average Daily Returns of the Stock over the years 2019-2021(Till April) (in 10^7)&quot;) plt.show() . plt.figure(figsize=(12, 7)) sns.distplot(axis_new_df[&#39;Daily Returns&#39;].dropna(), bins=100, color=&#39;purple&#39;) plt.title(&#39; Histogram of Daily Returns&#39;) plt.tight_layout() . /opt/conda/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . Q3: What is the Average Trading volume of the stock for past three years? . Avgvol_2019 =axis2019_df[&#39;Volume&#39;].sum()/len(axis2019_df[&#39;Volume&#39;]) Avgvol_2020 =axis2020_df[&#39;Volume&#39;].sum()/len(axis2020_df[&#39;Volume&#39;]) Avgvol_2021 =axis2021_df[&#39;Volume&#39;].sum()/len(axis2021_df[&#39;Volume&#39;]) # create a dataset data_volume = {&#39;2019&#39;: Avgvol_2019, &#39;2020&#39;:Avgvol_2020, &#39;2021&#39;:Avgvol_2021} Years = list(data_volume.keys()) AvgVol = list(data_volume.values()) # plotting a bar chart plt.figure(figsize=(13, 7)) plt.bar(Years, AvgVol, color =&#39;maroon&#39;,width = 0.3) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Average Volume of the Stock Traded&quot;) plt.title(&quot;Average Trading volume of the Stock over the years 2019-2021(Till April) (in 10^7)&quot;) plt.show() . From the above plot we can say that more volume of the Axis Bank stock is traded during the year 2020. We can see a significant rise in the trading volume of the stock from 2019 to 2020. . Q4: What is the Average Closing price of the stock for past three years? . Avgclose_2019 =axis2019_df[&#39;Close&#39;].sum()/len(axis2019_df[&#39;Close&#39;]) Avgclose_2020 =axis2020_df[&#39;Close&#39;].sum()/len(axis2020_df[&#39;Close&#39;]) Avgclose_2021 =axis2021_df[&#39;Close&#39;].sum()/len(axis2021_df[&#39;Close&#39;]) # create a dataset data_volume = {&#39;2019&#39;: Avgclose_2019, &#39;2020&#39;:Avgclose_2020, &#39;2021&#39;:Avgclose_2021} Years = list(data_volume.keys()) AvgClose = list(data_volume.values()) # plotting a bar chart plt.figure(figsize=(13, 7)) plt.bar(Years, AvgClose, color =&#39;maroon&#39;,width = 0.3) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Average Closding Price of the Stock Traded&quot;) plt.title(&quot;Average Closing price of the Stock over the years 2019-2021(Till April) (in 10^7)&quot;) plt.show() . We have seen the Trading Volume of the stock is more during the year 2020. In contrast, the Year 2020 has the lowest average closing price among the other two. But for the years 2019 and 2021 the Average closing price is almost same, there is not much change in the value. . Let us save and upload our work to Jovian before continuing. . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Inferences and Conclusion . Inferences : The above data analysis is done on the data set of stock quotes for AXIS BANK during the years 2019-2021. From the Analysis we can say that during the year 2020 there has been a lot of unsteady growth, there has been rise in the volume of stock traded on the exchange, that means there has been a lot of transactions of the stock. The stock has seen a swift traffic in buy/sell during the year 2020 and has fallen back to normal in the year 2021. In contrast to the volume of the stock the closing price of the stock has decreased during the year 2020, which can be concluded as the volume of the stock traded has no relation to the price change of the stock(while most people think there can be a correlation among the two values). The price decrease for the stock may have been due to the pandemic rise in India during the year 2020. . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . References and Future Work . Future Ideas for the Analyis: . I am planning to go forward with this basic Analysis of the AXISBANK stock quotes and build a Machine Learning model predicting the future stock prices. | I plan to automate the Data Analysis process for every stock in the NIFTY50 Index by defining reusable functions and automating the Analysis procedures. | Study more strong correlations between the different quotes of the stock and analyze how and why they are related in that fashion. | . REFRENCES/LINKS USED FOR THIS PROJECT : . https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html | https://stackoverflow.com/questions/16683701/in-pandas-how-to-get-the-index-of-a-known-value | https://towardsdatascience.com/working-with-datetime-in-pandas-dataframe-663f7af6c587 | https://thispointer.com/python-find-indexes-of-an-element-in-pandas-dataframe/ | https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#timeseries-friendly-merging | https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html | https://towardsdatascience.com/financial-analytics-exploratory-data-analysis-of-stock-data-d98cbadf98b9 | https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html | https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html | https://pandas.pydata.org/docs/reference/api/pandas.merge.html | https://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe | https://www.interviewqs.com/ddi-code-snippets/extract-month-year-pandas | https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value | https://queirozf.com/entries/matplotlib-examples-displaying-and-configuring-legends | https://jakevdp.github.io/PythonDataScienceHandbook/04.06-customizing-legends.html | https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html | https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html | https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html | https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib | https://www.investopedia.com/articles/investing/093014/stock-quotes-explained.asp | https://stackoverflow.com/questions/44908383/how-can-i-group-by-month-from-a-datefield-using-python-pandas | https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html | https://note.nkmk.me/en/python-pandas-dataframe-rename/ | https://stackoverflow.com/questions/24748848/pandas-find-the-maximum-range-in-all-the-columns-of-dataframe | https://stackoverflow.com/questions/29233283/plotting-multiple-lines-in-different-colors-with-pandas-dataframe | https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html | https://www.geeksforgeeks.org/python-pandas-extracting-rows-using-loc/ | . import jovian . jovian.commit() .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2022/02/04/data-analysis-course-project.html",
            "relUrl": "/jupyter/2022/02/04/data-analysis-course-project.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Qiskit Introductory Quantum Computing Tutorial",
            "content": "Qiskit Introductory Quantum Computing Course . import qiskit qiskit.__qiskit_version__ . {&#39;qiskit-terra&#39;: &#39;0.19.1&#39;, &#39;qiskit-aer&#39;: &#39;0.10.2&#39;, &#39;qiskit-ignis&#39;: &#39;0.7.0&#39;, &#39;qiskit-ibmq-provider&#39;: &#39;0.18.3&#39;, &#39;qiskit-aqua&#39;: None, &#39;qiskit&#39;: &#39;0.34.1&#39;, &#39;qiskit-nature&#39;: None, &#39;qiskit-finance&#39;: None, &#39;qiskit-optimization&#39;: None, &#39;qiskit-machine-learning&#39;: None} . from qiskit import QuantumCircuit qc = QuantumCircuit(3,3) qc.draw() . from qiskit import QuantumCircuit qc = QuantumCircuit(3,3) qc.measure([0,1,2],[0,1,2]) qc.draw() . from qiskit.providers.aer import AerSimulator sim = AerSimulator() . job = sim.run(qc) result = job.result() result.get_counts() . {&#39;000&#39;: 1024} . qc = QuantumCircuit(3,3) qc.x([0,1]) qc.measure([0,1,2],[0,1,2]) qc.draw() . job = sim.run(qc) results = job.result() results.get_counts() . {&#39;011&#39;: 1024} . qc = QuantumCircuit(2,2) qc.x(0) qc.cx(0,1) qc.measure([0,1],[0,1]) display(qc.draw()) job = sim.run(qc) result = job.result() print(&quot;Result: &quot;, result.get_counts()) . Result: {&#39;11&#39;: 1024} . test_qc = QuantumCircuit(4,2) test_qc.x(0) test_qc.x(1) test_qc.cx(0,2) test_qc.cx(1,2) test_qc.ccx(0,1,3) test_qc.measure(2,0) test_qc.measure(3,1) test_qc.draw() . job = sim.run(test_qc) result = job.result() result.get_counts() . {&#39;10&#39;: 1024} . job = sim.run(test_qc) result = job.result() result.get_counts() . {&#39;10&#39;: 1024} . test_qc = QuantumCircuit(4,2) test_qc.x(0) test_qc.x(1) test_qc.cx(0,2) test_qc.cx(1,2) test_qc.ccx(0,1,3) test_qc.measure(2,0) test_qc.measure(3,1) test_qc.draw() . job = sim.run(test_qc) result = job.result() result.get_counts() . {&#39;10&#39;: 1024} . The notebook is yet to be updated. This is not the final tutorial. Updates in the notebook will be done in future. .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2022/01/01/Qiskit-Introductory-QC-Module.html",
            "relUrl": "/jupyter/2022/01/01/Qiskit-Introductory-QC-Module.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Logistic Regression with a Neural Network Mindset",
            "content": "Logistic Regression with a Neural Network mindset . This notebook demonstrates, how to build a logistic regression classifier to recognize cats. This notebook will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning. This notebook is a modified version of the assignment I had done for the course: Neural Netoworks and Deep learning . Instructions: . Use np.dot(X,Y) to calculate dot products. | . Roadmap: . Build the general architecture of a learning algorithm, including: Initializing parameters | Calculating the cost function and its gradient | Using an optimization algorithm (gradient descent) | . | Gather all three functions above into a main model function, in the right order. | . . 1 - Packages . First, let&#39;s run the cell below to import all the packages that you will need during this assignment. . numpy is the fundamental package for scientific computing with Python. | h5py is a common package to interact with a dataset that is stored on an H5 file. | matplotlib is a famous library to plot graphs in Python. | PIL and scipy are used here to test your model with your own picture at the end. | . import numpy as np import copy import matplotlib.pyplot as plt import h5py import scipy from PIL import Image from scipy import ndimage from lr_utils import load_dataset from public_tests import * %matplotlib inline %load_ext autoreload %autoreload 2 . . 2 - Overview . Problem Statement: Given a dataset (&quot;data.h5&quot;) containing: . - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). . build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. . Let&#39;s get more familiar with the dataset. Load the data by running the following code. . train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() . NameError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_20868/377641723.py in &lt;module&gt; 1 # Loading the data (cat/non-cat) -&gt; 2 train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() NameError: name &#39;load_dataset&#39; is not defined . We added &quot;_orig&quot; at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don&#39;t need any preprocessing). . Each line of our train_set_x_orig and test_set_x_orig is an array representing an image. We can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images. . index = 25 plt.imshow(train_set_x_orig[index]) print (&quot;y = &quot; + str(train_set_y[:, index]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . Many software bugs in deep learning come from having matrix/vector dimensions that don&#39;t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. . . Exercise 1 . Find the values for: . - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) . Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]. . m_train = train_set_x_orig.shape[0] num_px = train_set_x_orig.shape[1] m_test = test_set_x_orig.shape[0] print (&quot;Number of training examples: m_train = &quot; + str(m_train)) print (&quot;Number of testing examples: m_test = &quot; + str(m_test)) print (&quot;Height/Width of each image: num_px = &quot; + str(num_px)) print (&quot;Each image is of size: (&quot; + str(num_px) + &quot;, &quot; + str(num_px) + &quot;, 3)&quot;) print (&quot;train_set_x shape: &quot; + str(train_set_x_orig.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x shape: &quot; + str(test_set_x_orig.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) . Expected Output for m_train, m_test and num_px: . m_train | 209 | . m_test | 50 | . num_px | 64 | . For convenience, we should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns. . . Exercise 2 . Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $*$ num_px $*$ 3, 1). . A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: . X_flatten = X.reshape(X.shape[0], -1).T # X.T is the transpose of X . train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T test_set_x_flatten = test_set_x_orig.reshape( test_set_x_orig.shape[0],-1).T # Check that the first 10 pixels of the second image are in the correct place assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), &quot;Wrong solution. Use (X.shape[0], -1).T.&quot; assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), &quot;Wrong solution. Use (X.shape[0], -1).T.&quot; print (&quot;train_set_x_flatten shape: &quot; + str(train_set_x_flatten.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x_flatten shape: &quot; + str(test_set_x_flatten.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) . Expected Output: . train_set_x_flatten shape | (12288, 209) | . train_set_y shape | (1, 209) | . test_set_x_flatten shape | (12288, 50) | . test_set_y shape | (1, 50) | . To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255. . One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). . Let&#39;s standardize our dataset. . train_set_x = train_set_x_flatten / 255. test_set_x = test_set_x_flatten / 255. . What you need to remember: . Common steps for pre-processing a new dataset are: . Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...) | Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) | &quot;Standardize&quot; the data | . . 3 - General Architecture of the learning algorithm . It&#39;s time to design a simple algorithm to distinguish cat images from non-cat images. . You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network! . Mathematical expression of the algorithm: . For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b tag{1}$$ $$ hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) tag{2}$$ $$ mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} log(a^{(i)}) - (1-y^{(i)} ) log(1-a^{(i)}) tag{3}$$ . The cost is then computed by summing over all training examples: $$ J = frac{1}{m} sum_{i=1}^m mathcal{L}(a^{(i)}, y^{(i)}) tag{6}$$ . Key steps: In this exercise, you will carry out the following steps: . - Initialize the parameters of the model - Learn the parameters for the model by minimizing the cost - Use the learned parameters to make predictions (on the test set) - Analyse the results and conclude . . 4 - Building the parts of our algorithm . The main steps for building a Neural Network are: . Define the model structure (such as number of input features) | Initialize the model&#39;s parameters | Loop: Calculate current loss (forward propagation) | Calculate current gradient (backward propagation) | Update parameters (gradient descent) | . | You often build 1-3 separately and integrate them into one function we call model(). . . 4.1 - Helper functions . Exercise 3 - sigmoid . Using your code from &quot;Python Basics&quot;, implement sigmoid(). As you&#39;ve seen in the figure above, you need to compute $sigmoid(z) = frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp(). . def sigmoid(z): &quot;&quot;&quot; Compute the sigmoid of z Arguments: z -- A scalar or numpy array of any size. Return: s -- sigmoid(z) &quot;&quot;&quot; dummy = (1 + np.exp(-z)) s = 1/dummy return s . print (&quot;sigmoid([0, 2]) = &quot; + str(sigmoid(np.array([0,2])))) sigmoid_test(sigmoid) . x = np.array([0.5, 0, 2.0]) output = sigmoid(x) print(output) . . 4.2 - Initializing parameters . Exercise 4 - initialize_with_zeros . Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don&#39;t know what numpy function to use, look up np.zeros() in the Numpy library&#39;s documentation. . def initialize_with_zeros(dim): &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) of type float &quot;&quot;&quot; w = (np.zeros((dim,1))) b = 0.0 return w, b . dim = 2 w, b = initialize_with_zeros(dim) assert type(b) == float print (&quot;w = &quot; + str(w)) print (&quot;b = &quot; + str(b)) initialize_with_zeros_test(initialize_with_zeros) . . 4.3 - Forward and Backward propagation . Now that your parameters are initialized, you can do the &quot;forward&quot; and &quot;backward&quot; propagation steps for learning the parameters. . . Exercise 5 - propagate . Implement a function propagate() that computes the cost function and its gradient. . Hints: . Forward Propagation: . You get X | You compute $A = sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ | You calculate the cost function: $J = - frac{1}{m} sum_{i=1}^{m}(y^{(i)} log(a^{(i)})+(1-y^{(i)}) log(1-a^{(i)}))$ | . Here are the two formulas you will be using: . $$ frac{ partial J}{ partial w} = frac{1}{m}X(A-Y)^T tag{7}$$ $$ frac{ partial J}{ partial b} = frac{1}{m} sum_{i=1}^m (a^{(i)}-y^{(i)}) tag{8}$$ . def propagate(w, b, X, Y): &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation. np.log(), np.dot() &quot;&quot;&quot; m = X.shape[1] # FORWARD PROPAGATION (FROM X TO COST) # compute activation # compute cost by using np.dot to perform multiplication. A = sigmoid(np.dot(w.T,X) +b) cost = (-1/m)* np.sum((Y*(np.log(A)) + (1-Y)* np.log(1-A))) # BACKWARD PROPAGATION (TO FIND GRAD) dw = (1/m)* np.dot(X,((A-Y).T)) db = (1/m)* np.sum((A-Y)) cost = np.squeeze(np.array(cost)) grads = {&quot;dw&quot;: dw, &quot;db&quot;: db} return grads, cost . w = np.array([[1.], [2]]) b = 1.5 X = np.array([[1., -2., -1.], [3., 0.5, -3.2]]) Y = np.array([[1, 1, 0]]) grads, cost = propagate(w, b, X, Y) assert type(grads[&quot;dw&quot;]) == np.ndarray assert grads[&quot;dw&quot;].shape == (2, 1) assert type(grads[&quot;db&quot;]) == np.float64 print (&quot;dw = &quot; + str(grads[&quot;dw&quot;])) print (&quot;db = &quot; + str(grads[&quot;db&quot;])) print (&quot;cost = &quot; + str(cost)) propagate_test(propagate) . Expected output . dw = [[ 0.25071532] [-0.06604096]] db = -0.1250040450043965 cost = 0.15900537707692405 . . 4.4 - Optimization . You have initialized your parameters. | You are also able to compute a cost function and its gradient. | Now, you want to update the parameters using gradient descent. | . . Exercise 6 - optimize . Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $ theta$, the update rule is $ theta = theta - alpha text{ } d theta$, where $ alpha$ is the learning rate. . def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False): &quot;&quot;&quot; This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b. &quot;&quot;&quot; w = copy.deepcopy(w) b = copy.deepcopy(b) costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w,b,X,Y) # Retrieve derivatives from grads dw = grads[&quot;dw&quot;] db = grads[&quot;db&quot;] # update rule w = w - (learning_rate * dw) b = b - (learning_rate * db) # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training iterations if print_cost: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) params = {&quot;w&quot;: w, &quot;b&quot;: b} grads = {&quot;dw&quot;: dw, &quot;db&quot;: db} return params, grads, costs . params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False) print (&quot;w = &quot; + str(params[&quot;w&quot;])) print (&quot;b = &quot; + str(params[&quot;b&quot;])) print (&quot;dw = &quot; + str(grads[&quot;dw&quot;])) print (&quot;db = &quot; + str(grads[&quot;db&quot;])) print(&quot;Costs = &quot; + str(costs)) optimize_test(optimize) . . Exercise 7 - predict . The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions: . Calculate $ hat{Y} = A = sigma(w^T X + b)$ . | Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this). . | def predict(w, b, X): &#39;&#39;&#39; Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X &#39;&#39;&#39; m = X.shape[1] Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture A = sigmoid(np.dot(w.T, X) + b) for i in range(A.shape[1]): # Convert probabilities A[0,i] to actual predictions p[0,i] # if A[0, i] &gt; ____ : # Y_prediction[0,i] = # else: # Y_prediction[0,i] = if A[0,i]&gt; 0.5: Y_prediction[0,i]= 1 else: Y_prediction[0,i] = 0 return Y_prediction . w = np.array([[0.1124579], [0.23106775]]) b = -0.3 X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]]) print (&quot;predictions = &quot; + str(predict(w, b, X))) predict_test(predict) . What to remember: . You&#39;ve implemented several functions that: . Initialize (w,b) | Optimize the loss iteratively to learn parameters (w,b): Computing the cost and its gradient | Updating the parameters using gradient descent | . | Use the learned (w,b) to predict the labels for a given set of examples | . . 5 - Merge all functions into a model . You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order. . . Exercise 8 - model . Implement the model function. Use the following notation: . - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize() . def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False): &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to True to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # initialize parameters with zeros w,b = initialize_with_zeros(X_train.shape[0]) # Gradient descent params ,grads ,costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary &quot;params&quot; w = params[&quot;w&quot;] b = params[&quot;b&quot;] # Predict test/train set examples Y_prediction_test = predict(w,b,X_test) Y_prediction_train = predict(w,b,X_train) # Print train/test Errors if print_cost: print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: costs, &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b, &quot;learning_rate&quot; : learning_rate, &quot;num_iterations&quot;: num_iterations} return d . from public_tests import * model_test(model) . If you pass all the tests, run the following cell to train your model. . logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True) . Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you&#39;ll build an even better classifier next week! . Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set. . index = 1 plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3))) print (&quot;y = &quot; + str(test_set_y[0,index]) + &quot;, you predicted that it is a &quot;&quot; + classes[int(logistic_regression_model[&#39;Y_prediction_test&#39;][0,index])].decode(&quot;utf-8&quot;) + &quot; &quot; picture.&quot;) . Let&#39;s also plot the cost function and the gradients. . costs = np.squeeze(logistic_regression_model[&#39;costs&#39;]) plt.plot(costs) plt.ylabel(&#39;cost&#39;) plt.xlabel(&#39;iterations (per hundreds)&#39;) plt.title(&quot;Learning rate =&quot; + str(logistic_regression_model[&quot;learning_rate&quot;])) plt.show() . Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. . . 6 - Further analysis . We built our first image classification model. Let&#39;s analyze it further, and examine possible choices for the learning rate $ alpha$. . Choice of learning rate . Reminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $ alpha$ determines how rapidly we update the parameters. If the learning rate is too large we may &quot;overshoot&quot; the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That&#39;s why it is crucial to use a well-tuned learning rate. . Let&#39;s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens. . learning_rates = [0.01, 0.001, 0.0001] models = {} for lr in learning_rates: print (&quot;Training a model with learning rate: &quot; + str(lr)) models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False) print (&#39; n&#39; + &quot;-&quot; + &#39; n&#39;) for lr in learning_rates: plt.plot(np.squeeze(models[str(lr)][&quot;costs&quot;]), label=str(models[str(lr)][&quot;learning_rate&quot;])) plt.ylabel(&#39;cost&#39;) plt.xlabel(&#39;iterations (hundreds)&#39;) legend = plt.legend(loc=&#39;upper center&#39;, shadow=True) frame = legend.get_frame() frame.set_facecolor(&#39;0.90&#39;) plt.show() . Interpretation: . Different learning rates give different costs and thus different predictions results. | If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). | A lower cost doesn&#39;t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. | In deep learning, we usually recommend that you: Choose the learning rate that better minimizes the cost function. | If your model overfits, use other techniques to reduce overfitting. (We&#39;ll talk about this in later videos.) | . | . . 7 - Test with your own image . You can use your own image and see the output of your model. To do that: 1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook&#39;s directory, in the &quot;images&quot; folder 3. Change your image&#39;s name in the following code 4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)! . my_image = &quot;docpic.jpg&quot; # We preprocess the image to fit your algorithm. fname = &quot;images/&quot; + my_image image = np.array(Image.open(fname).resize((num_px, num_px))) plt.imshow(image) image = image / 255. image = image.reshape((1, num_px * num_px * 3)).T my_predicted_image = predict(logistic_regression_model[&quot;w&quot;], logistic_regression_model[&quot;b&quot;], image) print(&quot;y = &quot; + str(np.squeeze(my_predicted_image)) + &quot;, your algorithm predicts a &quot;&quot; + classes[int(np.squeeze(my_predicted_image)),].decode(&quot;utf-8&quot;) + &quot; &quot; picture.&quot;) . What to remember : . Preprocessing the dataset is important. | You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). | Tuning the learning rate (which is an example of a &quot;hyperparameter&quot;) can make a big difference to the algorithm. You will see more examples of this later in this course! | Bibliography: . http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ | https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2021/11/20/Logistic-Regression-with-a-Neural-Network-mindset.html",
            "relUrl": "/jupyter/2021/11/20/Logistic-Regression-with-a-Neural-Network-mindset.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Pandas Data Analysis Basics Tutorial",
            "content": "Pandas Data Analysis short tutorial: . This is an assignment, a part of the course &quot;Data Analysis with Python: Zero to Pandas&quot; . This tutorial demonstrates data analysis using example from two data sets using the Pandas Library. All the important operations are described in markdown cells. . !pip install pandas --upgrade . Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.3.4) Requirement already satisfied: numpy&gt;=1.17.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.20.3) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0) . import pandas as pd . In this tutorial, we&#39;re going to analyze an operate on data from a CSV file. Let&#39;s begin by downloading the CSV file. . from urllib.request import urlretrieve urlretrieve(&#39;https://hub.jovian.ml/wp-content/uploads/2020/09/countries.csv&#39;, &#39;countries.csv&#39;) . (&#39;countries.csv&#39;, &lt;http.client.HTTPMessage at 0x7ff1c18db130&gt;) . Let&#39;s load the data from the CSV file into a Pandas data frame. . countries_df = pd.read_csv(&#39;countries.csv&#39;) . countries_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | . ... ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | . 210 rows × 6 columns . Q1: How many countries does the dataframe contain? . num_countries,colparameters = countries_df.shape . print(&#39;There are {} countries in the dataset&#39;.format(num_countries)) . There are 210 countries in the dataset . Q2: Retrieve a list of continents from the dataframe? . continents = pd.Series(countries_df.continent).unique() . continents . array([&#39;Asia&#39;, &#39;Europe&#39;, &#39;Africa&#39;, &#39;North America&#39;, &#39;South America&#39;, &#39;Oceania&#39;], dtype=object) . Q3: What is the total population of all the countries listed in this dataset? . total_population = countries_df.population.sum() . print(&#39;The total population is {}.&#39;.format(int(total_population))) . The total population is 7757980095. . Q: What is the overall life expectancy across in the world? . print((countries_df.population*countries_df.life_expectancy).sum()/countries_df.population.sum()) . 72.72165193409664 . overall_life = countries_df[&#39;life_expectancy&#39;].mean() . overall_life . 73.52985507246376 . Q4: Create a dataframe containing 10 countries with the highest population. . most_populous_df = (countries_df.sort_values(by=[&#39;population&#39;],ascending = False)).head(10) . most_populous_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita . 41 China | Asia | 1.439324e+09 | 76.91 | 4.34 | 15308.712 | . 90 India | Asia | 1.380004e+09 | 69.66 | 0.53 | 6426.674 | . 199 United States | North America | 3.310026e+08 | 78.86 | 2.77 | 54225.446 | . 91 Indonesia | Asia | 2.735236e+08 | 71.72 | 1.04 | 11188.744 | . 145 Pakistan | Asia | 2.208923e+08 | 67.27 | 0.60 | 5034.708 | . 27 Brazil | South America | 2.125594e+08 | 75.88 | 2.20 | 14103.452 | . 141 Nigeria | Africa | 2.061396e+08 | 54.69 | NaN | 5338.454 | . 15 Bangladesh | Asia | 1.646894e+08 | 72.59 | 0.80 | 3523.984 | . 157 Russia | Europe | 1.459345e+08 | 72.58 | 8.05 | 24765.954 | . 125 Mexico | North America | 1.289328e+08 | 75.05 | 1.38 | 17336.469 | . Q5: Add a new column in countries_df to record the overall GDP per country (product of population &amp; per capita GDP). . countries_df[&#39;gdp&#39;] = countries_df.population * countries_df.gdp_per_capita . countries_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | 7.022622e+10 | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | 3.396791e+10 | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | 6.101364e+11 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | 1.912651e+11 | . ... ... | ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | 6.007624e+11 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | NaN | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | 4.411699e+10 | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | 6.782303e+10 | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | 2.823622e+10 | . 210 rows × 7 columns . Q6: Create a data frame that counts the number countries in each continent? . country_counts_df = countries_df.groupby(&#39;continent&#39;).count() . country_counts_df . location population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp . continent . Africa 55 | 55 | 55 | 40 | 53 | 53 | . Asia 47 | 47 | 47 | 43 | 45 | 45 | . Europe 51 | 51 | 48 | 43 | 42 | 42 | . North America 36 | 36 | 36 | 23 | 27 | 27 | . Oceania 8 | 8 | 8 | 3 | 4 | 4 | . South America 13 | 13 | 13 | 12 | 12 | 12 | . Q7: Create a data frame showing the total population of each continent. . continent_populations_df = countries_df.groupby(&#39;continent&#39;).sum() . continent_populations_df = continent_populations_df[&#39;population&#39;] . continent_populations_df . continent Africa 1.339424e+09 Asia 4.607388e+09 Europe 7.485062e+08 North America 5.912425e+08 Oceania 4.095832e+07 South America 4.304611e+08 Name: population, dtype: float64 . Let&#39;s download another CSV file containing overall Covid-19 stats for various countires, and read the data into another Pandas data frame. . urlretrieve(&#39;https://hub.jovian.ml/wp-content/uploads/2020/09/covid-countries-data.csv&#39;, &#39;covid-countries-data.csv&#39;) . (&#39;covid-countries-data.csv&#39;, &lt;http.client.HTTPMessage at 0x7ff1789d6c10&gt;) . covid_data_df = pd.read_csv(&#39;covid-countries-data.csv&#39;) . covid_data_df . location total_cases total_deaths total_tests . 0 Afghanistan | 38243.0 | 1409.0 | NaN | . 1 Albania | 9728.0 | 296.0 | NaN | . 2 Algeria | 45158.0 | 1525.0 | NaN | . 3 Andorra | 1199.0 | 53.0 | NaN | . 4 Angola | 2729.0 | 109.0 | NaN | . ... ... | ... | ... | ... | . 207 Western Sahara | 766.0 | 1.0 | NaN | . 208 World | 26059065.0 | 863535.0 | NaN | . 209 Yemen | 1976.0 | 571.0 | NaN | . 210 Zambia | 12415.0 | 292.0 | NaN | . 211 Zimbabwe | 6638.0 | 206.0 | 97272.0 | . 212 rows × 4 columns . Q8: Count the number of countries for which the total_tests data is missing. . Hint: Use the .isna method. . total_tests_miss = covid_data_df.isna() #total_tests_missing = (total_tests_miss[total_tests_miss.total_tests == True]).count() total_tests_missing = total_tests_miss[&#39;total_tests&#39;].values.sum() . print(&quot;The data for total tests is missing for {} countries.&quot;.format(int(total_tests_missing))) . The data for total tests is missing for 122 countries. . Let&#39;s merge the two data frames, and compute some more metrics. . Q9: Merge countries_df with covid_data_df on the location column. . combined_df = countries_df.merge(covid_data_df,on=&#39;location&#39;) . combined_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | 7.022622e+10 | 38243.0 | 1409.0 | NaN | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | 3.396791e+10 | 9728.0 | 296.0 | NaN | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | 6.101364e+11 | 45158.0 | 1525.0 | NaN | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | 1199.0 | 53.0 | NaN | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | 1.912651e+11 | 2729.0 | 109.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | 6.007624e+11 | 1046.0 | 35.0 | 261004.0 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | NaN | 766.0 | 1.0 | NaN | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | 4.411699e+10 | 1976.0 | 571.0 | NaN | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | 6.782303e+10 | 12415.0 | 292.0 | NaN | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | 2.823622e+10 | 6638.0 | 206.0 | 97272.0 | . 210 rows × 10 columns . Q10: Add columns tests_per_million, cases_per_million and deaths_per_million into combined_df. . combined_df[&#39;tests_per_million&#39;] = combined_df[&#39;total_tests&#39;] * 1e6 / combined_df[&#39;population&#39;] . combined_df[&#39;cases_per_million&#39;] = combined_df[&#39;total_cases&#39;] * 1e6 / combined_df[&#39;population&#39;] . combined_df[&#39;deaths_per_million&#39;] = combined_df[&#39;total_deaths&#39;] * 1e6 / combined_df[&#39;population&#39;] . combined_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | 7.022622e+10 | 38243.0 | 1409.0 | NaN | NaN | 982.394806 | 36.194710 | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | 3.396791e+10 | 9728.0 | 296.0 | NaN | NaN | 3380.359997 | 102.856349 | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | 6.101364e+11 | 45158.0 | 1525.0 | NaN | NaN | 1029.804468 | 34.776824 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | 1199.0 | 53.0 | NaN | NaN | 15518.022390 | 685.950948 | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | 1.912651e+11 | 2729.0 | 109.0 | NaN | NaN | 83.033462 | 3.316470 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | 6.007624e+11 | 1046.0 | 35.0 | 261004.0 | 2681.403324 | 10.745996 | 0.359570 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | NaN | 766.0 | 1.0 | NaN | NaN | 1282.373228 | 1.674116 | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | 4.411699e+10 | 1976.0 | 571.0 | NaN | NaN | 66.250993 | 19.144391 | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | 6.782303e+10 | 12415.0 | 292.0 | NaN | NaN | 675.317108 | 15.883415 | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | 2.823622e+10 | 6638.0 | 206.0 | 97272.0 | 6544.605918 | 446.614587 | 13.859989 | . 210 rows × 13 columns . Q11: Create a dataframe with 10 countires that have highest number of tests per million people. . highest_tests_df = combined_df.nlargest(10,[&quot;tests_per_million&quot;]) . highest_tests_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 197 United Arab Emirates | Asia | 9890400.0 | 77.97 | 1.200 | 67293.483 | 6.655595e+11 | 71540.0 | 387.0 | 7177430.0 | 725696.635121 | 7233.276713 | 39.128852 | . 14 Bahrain | Asia | 1701583.0 | 77.29 | 2.000 | 43290.705 | 7.366273e+10 | 52440.0 | 190.0 | 1118837.0 | 657527.137965 | 30818.361490 | 111.660730 | . 115 Luxembourg | Europe | 625976.0 | 82.25 | 4.510 | 94277.965 | 5.901574e+10 | 7928.0 | 124.0 | 385820.0 | 616349.508607 | 12665.022301 | 198.090662 | . 122 Malta | Europe | 441539.0 | 82.53 | 4.485 | 36513.323 | 1.612206e+10 | 1931.0 | 13.0 | 188539.0 | 427004.183096 | 4373.339614 | 29.442473 | . 53 Denmark | Europe | 5792203.0 | 80.90 | 2.500 | 46682.515 | 2.703946e+11 | 17195.0 | 626.0 | 2447911.0 | 422621.755488 | 2968.645954 | 108.076323 | . 96 Israel | Asia | 8655541.0 | 82.97 | 2.990 | 33132.320 | 2.867782e+11 | 122539.0 | 969.0 | 2353984.0 | 271962.665303 | 14157.289533 | 111.951408 | . 89 Iceland | Europe | 341250.0 | 82.99 | 2.910 | 46482.958 | 1.586231e+10 | 2121.0 | 10.0 | 88829.0 | 260304.761905 | 6215.384615 | 29.304029 | . 157 Russia | Europe | 145934460.0 | 72.58 | 8.050 | 24765.954 | 3.614206e+12 | 1005000.0 | 17414.0 | 37176827.0 | 254750.159763 | 6886.653091 | 119.327539 | . 199 United States | North America | 331002647.0 | 78.86 | 2.770 | 54225.446 | 1.794877e+13 | 6114406.0 | 185744.0 | 83898416.0 | 253467.507769 | 18472.377957 | 561.155633 | . 10 Australia | Oceania | 25499881.0 | 83.44 | 3.840 | 44648.710 | 1.138537e+12 | 25923.0 | 663.0 | 6255797.0 | 245326.517406 | 1016.592979 | 26.000121 | . Q12: Create a dataframe with 10 countires that have highest number of positive cases per million people. . highest_cases_df = combined_df.nlargest(10,[&quot;cases_per_million&quot;]) . highest_cases_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 155 Qatar | Asia | 2881060.0 | 80.23 | 1.20 | 116935.600 | 3.368985e+11 | 119206.0 | 199.0 | 634745.0 | 220316.480740 | 41375.743650 | 69.071800 | . 14 Bahrain | Asia | 1701583.0 | 77.29 | 2.00 | 43290.705 | 7.366273e+10 | 52440.0 | 190.0 | 1118837.0 | 657527.137965 | 30818.361490 | 111.660730 | . 147 Panama | North America | 4314768.0 | 78.51 | 2.30 | 22267.037 | 9.607710e+10 | 94084.0 | 2030.0 | 336345.0 | 77952.047480 | 21805.112117 | 470.477208 | . 40 Chile | South America | 19116209.0 | 80.18 | 2.11 | 22767.037 | 4.352194e+11 | 414739.0 | 11344.0 | 2458762.0 | 128621.841287 | 21695.671982 | 593.423100 | . 162 San Marino | Europe | 33938.0 | 84.97 | 3.80 | 56861.470 | 1.929765e+09 | 735.0 | 42.0 | NaN | NaN | 21657.139490 | 1237.550828 | . 9 Aruba | North America | 106766.0 | 76.29 | NaN | 35973.781 | 3.840777e+09 | 2211.0 | 12.0 | NaN | NaN | 20708.839893 | 112.395332 | . 105 Kuwait | Asia | 4270563.0 | 75.49 | 2.00 | 65530.537 | 2.798523e+11 | 86478.0 | 535.0 | 621616.0 | 145558.325682 | 20249.789079 | 125.276222 | . 150 Peru | South America | 32971846.0 | 76.74 | 1.60 | 12236.706 | 4.034668e+11 | 663437.0 | 29259.0 | 584232.0 | 17719.117092 | 20121.318048 | 887.393445 | . 27 Brazil | South America | 212559409.0 | 75.88 | 2.20 | 14103.452 | 2.997821e+12 | 3997865.0 | 123780.0 | 4797948.0 | 22572.268255 | 18808.224105 | 582.331314 | . 199 United States | North America | 331002647.0 | 78.86 | 2.77 | 54225.446 | 1.794877e+13 | 6114406.0 | 185744.0 | 83898416.0 | 253467.507769 | 18472.377957 | 561.155633 | . Q13: Create a dataframe with 10 countires that have highest number of deaths cases per million people? . highest_deaths_df = combined_df.nlargest(10,[&quot;deaths_per_million&quot;]) . highest_deaths_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 162 San Marino | Europe | 33938.0 | 84.97 | 3.80 | 56861.470 | 1.929765e+09 | 735.0 | 42.0 | NaN | NaN | 21657.139490 | 1237.550828 | . 150 Peru | South America | 32971846.0 | 76.74 | 1.60 | 12236.706 | 4.034668e+11 | 663437.0 | 29259.0 | 584232.0 | 17719.117092 | 20121.318048 | 887.393445 | . 18 Belgium | Europe | 11589616.0 | 81.63 | 5.64 | 42658.576 | 4.943965e+11 | 85817.0 | 9898.0 | 2281853.0 | 196887.713967 | 7404.645676 | 854.040375 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | 1199.0 | 53.0 | NaN | NaN | 15518.022390 | 685.950948 | . 177 Spain | Europe | 46754783.0 | 83.56 | 2.97 | 34272.360 | 1.602397e+12 | 479554.0 | 29194.0 | 6416533.0 | 137238.001939 | 10256.790198 | 624.406705 | . 198 United Kingdom | Europe | 67886004.0 | 81.32 | 2.54 | 39753.244 | 2.698689e+12 | 338676.0 | 41514.0 | 13447568.0 | 198090.434075 | 4988.892850 | 611.525168 | . 40 Chile | South America | 19116209.0 | 80.18 | 2.11 | 22767.037 | 4.352194e+11 | 414739.0 | 11344.0 | 2458762.0 | 128621.841287 | 21695.671982 | 593.423100 | . 97 Italy | Europe | 60461828.0 | 83.51 | 3.18 | 35220.084 | 2.129471e+12 | 271515.0 | 35497.0 | 5214766.0 | 86248.897403 | 4490.684602 | 587.097697 | . 27 Brazil | South America | 212559409.0 | 75.88 | 2.20 | 14103.452 | 2.997821e+12 | 3997865.0 | 123780.0 | 4797948.0 | 22572.268255 | 18808.224105 | 582.331314 | . 182 Sweden | Europe | 10099270.0 | 82.80 | 2.22 | 46949.283 | 4.741535e+11 | 84532.0 | 5820.0 | NaN | NaN | 8370.109919 | 576.279276 | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2021/10/05/Pandas-Data-Analysis-Basics-Tutorial.html",
            "relUrl": "/jupyter/2021/10/05/Pandas-Data-Analysis-Basics-Tutorial.html",
            "date": " • Oct 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Guide to Numpy Operations",
            "content": "A Short Guide to Numpy Array operations. . Introducing Numpy Arrays: . Numpy is a python library used to create, modify and interact with Arrays. It is a very vast and powerful library which is very essential for data analysis tasks. The Numpy library basic routines are very simple and easy to learn. Numpy is also known for scientific computation since it can handle large amount of data in the form of arrays. Numpy is mostly written in C which makes it run time fast and execute operations faster. It is very useful for Mathematics since it offers a vast collection of mathematical functions and random number generators. In this notebook, I try to demonstrate some standard numpy operations essential for operating on matrices. Below are the five operations that I am going to demonstrate: . np.matmul | np.linalg.solve | np.column_stack(tup) | np.bmat | np.log | . The demonstrating includes the syntax of using these operations and how to pass arguments for these functions. It also demonstrates in what special cases do we get errors using these operations and also how to solve fix the erorrs efficiently. . Let&#39;s begin by importing Numpy and listing out the functions covered in this notebook. . import numpy as np . function1 = np.matmul function2 = np.linalg.solve function3 = np.column_stack function4 = np.bmat function5 = np.emath . Function 1 - np.matmul . The np.matmul operator is a matrix multiplication operator used to multiply arrays.The @ is short form for using this operator. The function has the following syntax: np.matmul(a,b)-- where a,b are arrays that are to be multiplied. . arr1 = [[15, 24], [33, 4.]] arr2 = [[5, 16, 37], [18, 9, 20]] mult=np.matmul(arr1,arr2) print(mult) . [[ 507. 456. 1035.] [ 237. 564. 1301.]] . Explanation: The matmul operator takes two 2d arrays i.e. arr1 and arr2 abd multiplied by matrix multiplication. Inorder for arrays to be able to perform matrix multiplication the 2nd dimension of arr1 should be equal to first dimension of arr2. Here they satisfy the condition and the output is generated. If the dimensions of two arrays do not satisfy the property we get a traceback error msg telling us there is a mismatch in dimensions. . np.matmul([2j, 3j], [2j, 3j]) . (-13+0j) . The vector vector gives us scalar inner product. Given two vector arrays the np.matmul operation does dot product or also called as scalar product. The 0j in the output is the complex part of the output since there are no complex numbers involved we get zero complex part in our output . arr1 = [[15,24,32], [3, 4.,12]] arr2 = [[5, 6, 7], [8, 9, 10]] np.matmul(arr1,arr2) . ValueError Traceback (most recent call last) /tmp/ipykernel_36/2247344183.py in &lt;module&gt; 6 [8, 9, 10]] 7 -&gt; 8 np.matmul(arr1,arr2) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . It breaks because matrix multiplication takes place only when the second dimension of the first array is equal to first dimension of the second array. . This function is used to multiply arrays, for dot product of two vectors and multiplication of 3d arrays etc. . Function 2 - np.linalg.solve . This operator is used to solve linear equations and give out the value of unknowns from the system of linear equations. For example ax = b be a system of linear equations and the above function solves the equation for x. a, b are matrices formed from the system of linear equations.In mathematical terms, a is the coefficient matrix and x is the vector containing unknowns and b is the right hand side values of the system of equations. The function has the following syntax: np.linalg.solve(a,b) where a,b are arrays coefficient matrix , dependant variable matrix. . a = [[5,6], [7,8]] b = [12,16] x = np.linalg.solve(a,b) print(x) . [0. 2.] . Here we found the solution for the system of linear equations where a is coefficient matrix and b is the solution matrix. the system of linear equations are of the form: 5m + 6n = 12 7m + 8n = 16 and the matrix a is made from the coefficients of the equations the output gives a vector x of two elements that are the solutions of m and n respectively. . Example 2 - working . a = [[2,3,4],[8,4,2],[5,3,4]] b = [12,16,18] x = np.linalg.solve(a,b) print(x) . [ 2. -1.6 3.2] . The above example is based on three linear equations in three variables. the np.linalg.solve all kinds of systems of linear equations. . a = [[1,2,3],[4,5,6],[7,8,9]] b = [12,16,18] x = np.linalg.solve(a,b) . LinAlgError Traceback (most recent call last) /tmp/ipykernel_36/3857377319.py in &lt;module&gt; 2 a = [[1,2,3],[4,5,6],[7,8,9]] 3 b = [12,16,18] -&gt; 4 x = np.linalg.solve(a,b) &lt;__array_function__ internals&gt; in solve(*args, **kwargs) /opt/conda/lib/python3.9/site-packages/numpy/linalg/linalg.py in solve(a, b) 391 signature = &#39;DD-&gt;D&#39; if isComplexType(t) else &#39;dd-&gt;d&#39; 392 extobj = get_linalg_error_extobj(_raise_linalgerror_singular) --&gt; 393 r = gufunc(a, b, signature=signature, extobj=extobj) 394 395 return wrap(r.astype(result_t, copy=False)) /opt/conda/lib/python3.9/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag) 86 87 def _raise_linalgerror_singular(err, flag): &gt; 88 raise LinAlgError(&#34;Singular matrix&#34;) 89 90 def _raise_linalgerror_nonposdef(err, flag): LinAlgError: Singular matrix . It breaks down above because solving of linear equations has some conditions. If the matrix is singular the output does not give any solution. singular matrix is one that is not invertible. This means that the system of equations you are trying to solve does not have a unique solution. . This function proves very useful when we need to solve n number of linear equations in n variables and gives the solution. There are a lot of uses for the function Mathematically since it reduced the load of solving linear equations by row reduction process, which is very tedious and most times do not provide accurate solution. . Function 3 - np.column_stack . np.column_stack is used to stack two 1-D arrays into a 2D array of column wise.It is used in the following format np.column_stack(tuple). The parameter is given as tuple type. The 1D array are entered as tuples. The function has the following syntax: np.column_stack(tuple) . a = np.array((4,5,6)) b = np.array((12,14,17)) c = np.column_stack((a,b)) print(c) . [[ 4 12] [ 5 14] [ 6 17]] . The two arrays a and b are stacked side by side / column wise and the output array is a 2D array containing one column(a) and the other column(b) . a = np.array((3,34,53,72,65,81)) b = np.array((2,5,43,76,83,92)) c = np.column_stack((a,b)) print(c) . [[ 3 2] [34 5] [53 43] [72 76] [65 83] [81 92]] . The above example stacks two given arrays in the form of tuples and stacks them side by side. . a = np.array((1,2,3,4)) b = np.array((12,13,14)) c = np.column_stack((a,b)) print(c) . ValueError Traceback (most recent call last) /tmp/ipykernel_36/354580763.py in &lt;module&gt; 2 a = np.array((1,2,3,4)) 3 b = np.array((12,13,14)) -&gt; 4 c = np.column_stack((a,b)) 5 print(c) &lt;__array_function__ internals&gt; in column_stack(*args, **kwargs) /opt/conda/lib/python3.9/site-packages/numpy/lib/shape_base.py in column_stack(tup) 654 arr = array(arr, copy=False, subok=True, ndmin=2).T 655 arrays.append(arr) --&gt; 656 return _nx.concatenate(arrays, 1) 657 658 &lt;__array_function__ internals&gt; in concatenate(*args, **kwargs) ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 4 and the array at index 1 has size 3 . The function breaks down because one of the array is longer in size than the other. The concatenation or stacking takes place only when both the arrays to be stacked has same size. . The function is very useful when we need to stack two list .Similar to appending but here we are appending columns. The function can be used to add the column to pre-existing ones. . Function 4 - np.bmat . It builds a matrix object from a string, nested sequence or an array. The function has the following syntax: np.bmat(object,ldict,gdict)where ldict and gdict are dictionaries and they are optional parameters and object is either string or array-like. . A = np.mat(&#39;1 1; 1 1&#39;) B = np.mat(&#39;2 2; 2 2&#39;) C = np.mat(&#39;3 4; 5 6&#39;) D = np.mat(&#39;7 8; 9 0&#39;) np.bmat([[A, B], [C, D]]) . matrix([[1, 1, 2, 2], [1, 1, 2, 2], [3, 4, 7, 8], [5, 6, 9, 0]]) . The above example takes four arrays A,B,C,D and builds up a matrix by stacking A and B, C and D column wise and then stacking the arrays row wise one top of the another. . A = np.mat(&#39;12 13; 14 15&#39;) B = np.mat(&#39;23 28; 24 22&#39;) C = np.mat(&#39;34 45; 57 69&#39;) D = np.mat(&#39;7 8; 9 0&#39;) np.bmat([[B, A], [D, C]]) . matrix([[23, 28, 12, 13], [24, 22, 14, 15], [ 7, 8, 34, 45], [ 9, 0, 57, 69]]) . The above example takes the arrays A,B,C,D and builds the matrix by stacking them as per the order of the parameters given in the paranthesis of the function. . A = np.mat(&#39;12 13; 14 15 18&#39;) B = np.mat(&#39;23 28; 24 22&#39;) C = np.mat(&#39;34 45; 57 69&#39;) D = np.mat(&#39;7 8; 9 0 12&#39;) np.bmat([[B, A], [D, C]]) . ValueError Traceback (most recent call last) /tmp/ipykernel_36/1923516388.py in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) -&gt; 2 A = np.mat(&#39;12 13; 14 15 18&#39;) 3 B = np.mat(&#39;23 28; 24 22&#39;) 4 C = np.mat(&#39;34 45; 57 69&#39;) 5 D = np.mat(&#39;7 8; 9 0 12&#39;) /opt/conda/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py in asmatrix(data, dtype) 67 68 &#34;&#34;&#34; &gt; 69 return matrix(data, dtype=dtype, copy=False) 70 71 /opt/conda/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py in __new__(subtype, data, dtype, copy) 140 141 if isinstance(data, str): --&gt; 142 data = _convert_from_string(data) 143 144 # now convert data to an array /opt/conda/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py in _convert_from_string(data) 28 Ncols = len(newrow) 29 elif len(newrow) != Ncols: &gt; 30 raise ValueError(&#34;Rows not the same size.&#34;) 31 count += 1 32 newdata.append(newrow) ValueError: Rows not the same size. . It breaks because there the sizes of the rows are different so they cannot be stacked. The sizes of the rows or columns that are getting stacked must be of the same size inorder to stack them and build the matrix. To fix this we need to remove the extra rows in the arrays or add NaN element to others to make the rows of same size. . The function is useful when we have some groups of arrays and we want to stack them into a matrix to perform operations and gain insights from that build matrix. . Function 5 - np.log . The function is used to calculate log values of the elements of the array The sytax for the function is: np.log(x)-- x is array-like . np.log([1, np.e, np.e**2]) . array([0., 1., 2.]) . In the above example the function took the input parameters and calculated the individual log values of the elements and gives the output. . np.log([1, 0, np.e]) . /tmp/ipykernel_36/1939558749.py:2: RuntimeWarning: divide by zero encountered in log np.log([1,0, np.e]) . array([ 0., -inf, 1.]) . The above example demonstrates that the np.log function even takes 0 and gives the -inf as the output. It gives a warning because the function detects a zero division because of log (0) computation. . np.log([1, np.e, a]) . /tmp/ipykernel_36/1874325113.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. np.log([1, np.e, a]) . AttributeError Traceback (most recent call last) AttributeError: &#39;int&#39; object has no attribute &#39;log&#39; The above exception was the direct cause of the following exception: TypeError Traceback (most recent call last) /tmp/ipykernel_36/1874325113.py in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) -&gt; 2 np.log([1, np.e, a]) TypeError: loop of ufunc does not support argument 0 of type int which has no callable log method . The above example breaks down because log doesn&#39;t support strings. To fix this replace string by integer or float value. . This function is useful to calculate the log values of an array so that we can manipulate the array and do log operations on the array . Conclusion . The Notebook covers the following: np.matmul function: which is used for dot product and multiplication of arrays, given the two arrays satisfies the size condition for matrix multiplication. np.linalg.solve : which is used to solve system of linear equations given the coefficient matrix is non-singular. np.column_stack: which is used to stack or concatenate two 1D arrays column-wise and give a 2D array, np.bmat: builds a matrix from strings or arrays. np.log: computed the log values of the elements in the array. The following functions are very useful while manipulating matrices and computation of matrices. . Reference Links . Provide links to your references and other interesting articles about Numpy arrays: . Numpy official tutorial : https://numpy.org/doc/stable/user/quickstart.html | Numpy offical documnetation :https://numpy.org/doc/stable/reference/ | Numpy documentation for the following functions: | 1.np.matmul : https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul | 2.np.linalg.solve: https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve | 3.np.column_stack: https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html | 4.np.bmat: https://numpy.org/doc/stable/reference/generated/numpy.bmat.html#numpy.bmat | 5.np.log: https://numpy.org/doc/stable/reference/generated/numpy.log.html#numpy.log | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2021/10/03/Guide-to-Numpy-Operations.html",
            "relUrl": "/jupyter/2021/10/03/Guide-to-Numpy-Operations.html",
            "date": " • Oct 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an Undergraduate student majoring in physics with a minor in math. My primary research interests are in Quantum Computing and Quantum Information Theory. I am also interested in working in the domains of Quantum Machine Learning, Scientific Machine learning and other interdisciplinary domains. I am equipped with strong programming skills working with Python, C++, Julia and MATLAB . I am experience in machine learning project particularly in Deep learning domain. I have done various project using time-series datasets and analyzing the data using standard Data science and ML Techniques. I am currently working on Scientific Machine learning projects using Julia Programming language. .",
          "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}