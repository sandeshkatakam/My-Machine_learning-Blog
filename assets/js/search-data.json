{
  
    
        "post0": {
            "title": "Neural ODEs - An Introduction (Live Blog post)",
            "content": "Introduction to the concept of NeuralODEs and Implicit layers . In any standard Neural network, take for an example an MLP(multi layer perceptron) we have discrete set of layers that transform the input vector(called as tensor) and apply non-linear function to the transformed vector and use classifier at the end of the network like softmax etc to make the prediction in any classification problem. The concept of implicit layers will be thinking of neural network not as explicitly defined set of layers but the depth of the model will be decided by the dynamics we introduce to the model(the ODE or any differential equation we use). So, our objective is to solve this differential equation to model the neural network(which is basically a function). . import jax.numpy as jnp def mlp(params, inputs): # A multi-layer perceptron, i.e. a fully-connected neural network. for w, b in params: outputs = jnp.dot(inputs, w) + b # Linear transform inputs = jnp.tanh(outputs) # Nonlinearity return outputs . def resnet(params, inputs, depth): for i in range(depth): outputs = mlp(params, inputs) + inputs return outputs . import numpy.random as npr from jax import jit, grad resnet_depth = 3 def resnet_squared_loss(params, inputs, targets): preds = resnet(params, inputs, resnet_depth) return jnp.mean(jnp.sum((preds - targets)**2, axis=1)) def init_random_params(scale, layer_sizes, rng=npr.RandomState(0)): return [(scale * rng.randn(m, n), scale * rng.randn(n)) for m, n, in zip(layer_sizes[:-1], layer_sizes[1:])] # A simple gradient-descent optimizer. @jit def resnet_update(params, inputs, targets): grads = grad(resnet_squared_loss)(params, inputs, targets) return [(w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip(params, grads)] . inputs = jnp.reshape(jnp.linspace(-2.0, 2.0, 10), (10, 1)) targets = inputs**3 + 0.1 * inputs # Hyperparameters. layer_sizes = [1, 20, 1] param_scale = 1.0 step_size = 0.01 train_iters = 1000 # Initialize and train. resnet_params = init_random_params(param_scale, layer_sizes) for i in range(train_iters): resnet_params = resnet_update(resnet_params, inputs, targets) # Plot results. import matplotlib.pyplot as plt fig = plt.figure(figsize=(6, 4), dpi=150) ax = fig.gca() ax.scatter(inputs, targets, lw=0.5, color=&#39;green&#39;) fine_inputs = jnp.reshape(jnp.linspace(-3.0, 3.0, 100), (100, 1)) ax.plot(fine_inputs, resnet(resnet_params, fine_inputs, resnet_depth), lw=0.5, color=&#39;blue&#39;) ax.set_xlabel(&#39;input&#39;) ax.set_ylabel(&#39;output&#39;) . WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) . Text(0, 0.5, &#39;output&#39;) . def nn_dynamics(state, time, params): state_and_time = jnp.hstack([state, jnp.array(time)]) return mlp(params, state_and_time) . from jax.experimental.ode import odeint def odenet(params, input): start_and_end_times = jnp.array([0.0, 1.0]) init_state, final_state = odeint(nn_dynamics, input, start_and_end_times, params) return final_state . from jax import vmap batched_odenet = vmap(odenet, in_axes=(None, 0)) . odenet_layer_sizes = [2, 20, 1] def odenet_loss(params, inputs, targets): preds = batched_odenet(params, inputs) return jnp.mean(jnp.sum((preds - targets)**2, axis=1)) @jit def odenet_update(params, inputs, targets): grads = grad(odenet_loss)(params, inputs, targets) return [(w - step_size * dw, b - step_size * db) for (w, b), (dw, db) in zip(params, grads)] # Initialize and train ODE-Net. odenet_params = init_random_params(param_scale, odenet_layer_sizes) for i in range(train_iters): odenet_params = odenet_update(odenet_params, inputs, targets) # Plot resulting model. fig = plt.figure(figsize=(6, 4), dpi=150) ax = fig.gca() ax.scatter(inputs, targets, lw=0.5, color=&#39;green&#39;) fine_inputs = jnp.reshape(jnp.linspace(-3.0, 3.0, 100), (100, 1)) ax.plot(fine_inputs, resnet(resnet_params, fine_inputs, resnet_depth), lw=0.5, color=&#39;blue&#39;) ax.plot(fine_inputs, batched_odenet(odenet_params, fine_inputs), lw=0.5, color=&#39;red&#39;) ax.set_xlabel(&#39;input&#39;) ax.set_ylabel(&#39;output&#39;) plt.legend((&#39;Resnet predictions&#39;, &#39;ODE Net predictions&#39;)) . &lt;matplotlib.legend.Legend at 0x7f81ba35ad50&gt; . import matplotlib.pyplot as plt fig = plt.figure(figsize=(6, 4), dpi=150) ax = fig.gca() @jit def odenet_times(params, input, times): def dynamics_func(state, time, params): return mlp(params, jnp.hstack([state, jnp.array(time)])) return odeint(dynamics_func, input, times, params) times = jnp.linspace(0.0, 1.0, 200) for i in fine_inputs: ax.plot(odenet_times(odenet_params, i, times), times, lw=0.5) ax.set_xlabel(&#39;input / output&#39;) ax.set_ylabel(&#39;time / depth&#39;) . Text(0, 0.5, &#39;time / depth&#39;) .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/jax_library/neuralodes/resnets/2022/05/31/NeuralODEs-Tutorial.html",
            "relUrl": "/deep%20learning/neuralnetworks/jax_library/neuralodes/resnets/2022/05/31/NeuralODEs-Tutorial.html",
            "date": " • May 31, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Natural Language Processing(NLP) with TensorFlow - Live Blog post",
            "content": "Introduction to Natural Language Processing (NLP) Fundamentals in TensorFlow . NLP has the goal of deriving information out of natural language(could be a sequences test or speech). . Another common term for NLP problems is Sequence Models/ Sequence to Sequence problems . some common applications of NLP: . Classification of articles into labels | Text Generation | Machine Translation | Voice Assistants. | . All of there are also referred to as Sequence Problems . Different Types of Sequence Problems: . This Notebook covers: . Downloading and preparing a text dataset | How to prepare text data for modelling(tokenization and embedding) | Setting up multiple modelling experiments with recurrent neural networks(RNNs) | Building a text feature extraction model using TensorFlow Hub | Finding the most wrong prediction examples | Using a model we&#39;ve built to make predictions on text from the wild. | . What is a Recurrent Neural Network? . Answer goes here... . Architecture of an RNN: . Hyperparamter/Layer type What does it do? Typical values . Input text(s) | Target texts/sequences you&#39;d like to discover patterns in | Whatever you can represent as a text or a sequence | . Input layer | Takes in a target sequence | input_shape = [batch_size, embeddding_size] or [batch_size, sequence_shape] | . Text Vectorization layer | Maps input sequences to numbers | Multiple, can create with tf.keras.layers.experimental.preprocessing.TextVectorization | . Embedding | Turns mapping of text vectors to embedding matrix(representation of how words realate) | Multiple, can create with tf.keras.layers.Embedding | . RNN Cell(s) | Finds patterns in sequences | Simple RNN, LSTM, GRU | . Hidden activation | Adds non-linearity to learned features(non-straight lines) | Usually Tanh(hyperbolic tangent)(tf.keras.activations.tanh) | . Pooling layer | Reduces the dimensionality of learned sequence features (usually Conv1D models) | Average(tf.keras.layers.GlobalAveragePooling1D or Max(tf.keras.layers.GlobalMaxPool1D) | . Fully connected layer | Further refines learned features from recurrent layers | tf.keras.layers.Dense | . Output layer | Takes learned features and outputs them in shape of target labels | output_shape = [number_of_classes](e.g. 2 for Disaster/Not Disaster example) | . Output activation | Adds non-linearities to output layer | tf.keras.activations.sigmoid(binary classification) or tf.keras.activations.softmax | . Example TensorFlow code for RNN Model: . #1. Create LSTM model from tensorflow.keras import layers inputs = layers.Input(shape = (1,), dtype = &quot;string&quot;) x = text_vectorizer(inputs) # turn inputs sequence to numbers x = embedding(x) # Create embedding matrix x = layers.LSTM(64, activation = &quot;tanh&quot;)(x) outputs = layers.Dense(1, activation = &quot;sigmoid&quot;)(x) model = tf.keras.Model(inputs,outputs, name = &quot;LSTM_model&quot;) # 2. Compile the Model model.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimzer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history = model.fit(train_sentences, train_labels, epochs = 5) . !nvidia-smi -L . GPU 0: Tesla K80 (UUID: GPU-b8f44027-0c2b-aba8-bdea-b2db6b2c6f28) . Get Helper Functions . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py # Import a series of helper functions for the notebook from helper_functions import unzip_data, create_tensorboard_callback,plot_loss_curves,compare_historys . --2022-03-11 07:41:53-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py.1’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2022-03-11 07:41:53 (74.3 MB/s) - ‘helper_functions.py.1’ saved [10246/10246] . Get Text Dataset . The dataset we&#39;re going to be using is Kaggle&#39;s introduction to NLP dataset (text samples of Tweets labelled as disaster or not disaster) . Source : Natural Language Processing with Disaster Tweets . !wget https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip # Unzip the dataset unzip_data(&quot;nlp_getting_started.zip&quot;) . --2022-03-11 07:41:53-- https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.133.128, 108.177.15.128, 173.194.76.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.133.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 607343 (593K) [application/zip] Saving to: ‘nlp_getting_started.zip.1’ nlp_getting_started 100%[===================&gt;] 593.11K --.-KB/s in 0.007s 2022-03-11 07:41:53 (85.5 MB/s) - ‘nlp_getting_started.zip.1’ saved [607343/607343] . Visualizing a text dataset . To visualize our text samples, we have to read them in, one way to do this is to be use python. . import pandas as pd train_df = pd.read_csv(&quot;train.csv&quot;) test_df = pd.read_csv(&quot;test.csv&quot;) train_df.head() . id keyword location text target . 0 1 | NaN | NaN | Our Deeds are the Reason of this #earthquake M... | 1 | . 1 4 | NaN | NaN | Forest fire near La Ronge Sask. Canada | 1 | . 2 5 | NaN | NaN | All residents asked to &#39;shelter in place&#39; are ... | 1 | . 3 6 | NaN | NaN | 13,000 people receive #wildfires evacuation or... | 1 | . 4 7 | NaN | NaN | Just got sent this photo from Ruby #Alaska as ... | 1 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_df_shuffled = train_df.sample(frac = 1, random_state=42) train_df_shuffled.head() . id keyword location text target . 2644 3796 | destruction | NaN | So you have a new weapon that can cause un-ima... | 1 | . 2227 3185 | deluge | NaN | The f$&amp;amp;@ing things I do for #GISHWHES Just... | 0 | . 5448 7769 | police | UK | DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe... | 1 | . 132 191 | aftershock | NaN | Aftershock back to school kick off was great. ... | 0 | . 6845 9810 | trauma | Montgomery County, MD | in response to trauma Children of Addicts deve... | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; test_df.head() . id keyword location text . 0 0 | NaN | NaN | Just happened a terrible car crash | . 1 2 | NaN | NaN | Heard about #earthquake is different cities, s... | . 2 3 | NaN | NaN | there is a forest fire at spot pond, geese are... | . 3 9 | NaN | NaN | Apocalypse lighting. #Spokane #wildfires | . 4 11 | NaN | NaN | Typhoon Soudelor kills 28 in China and Taiwan | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_df.target.value_counts() . 0 4342 1 3271 Name: target, dtype: int64 . len(train_df), len(test_df) . (7613, 3263) . import random random_index = random.randint(0, len(train_df)-5) # Create random indexes for row in train_df_shuffled[[&quot;text&quot;, &quot;target&quot;]][random_index: random_index+5].itertuples(): _, text, target = row print(f&quot;Target:{target}&quot;, &quot;(real disaster)&quot; if target &gt; 0 else &quot;(not real disaster)&quot;) print(f&quot;Text: n {text} n&quot;) print(&quot; n&quot;) . Target:1 (real disaster) Text: News Update Huge cliff landslide on road in China - Watch the moment a cliff collapses as huge chunks of rock fall... http://t.co/gaBd0cjmAG Target:1 (real disaster) Text: #Politics DemocracyÛªs hatred for hate: Û_ Dawabsha threaten to erode Israeli democracy. Homegrown terrorism ha... http://t.co/q8n5Tn8WME Target:0 (not real disaster) Text: Be Trynna smoke TJ out but he a hoe Target:1 (real disaster) Text: California LawÛÓNegligence and Fireworks Explosion Incidents http://t.co/d5w2zynP7b Target:1 (real disaster) Text: USGS EQ: M 1.2 - 23km S of Twentynine Palms California: Time2015-08-05 23:54:09 UTC2015-08-05 16:... http://t.co/T97JmbzOBO #EarthQuake . Split data into training and validation sets . from sklearn.model_selection import train_test_split . train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled[&quot;text&quot;].to_numpy(), train_df_shuffled[&quot;target&quot;].to_numpy(), test_size = 0.1,# use 10% of training data for validation random_state = 42) . len(train_sentences), len(val_sentences), len(train_labels), len(val_labels) . (6851, 762, 6851, 762) . train_sentences[:10], train_labels[:10] . (array([&#39;@mogacola @zamtriossu i screamed after hitting tweet&#39;, &#39;Imagine getting flattened by Kurt Zouma&#39;, &#39;@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....&#39;, &#34;@shakjn @C7 @Magnums im shaking in fear he&#39;s gonna hack the planet&#34;, &#39;Somehow find you and I collide http://t.co/Ee8RpOahPk&#39;, &#39;@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao&#39;, &#39;destroy the free fandom honestly&#39;, &#39;Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE&#39;, &#39;@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.&#39;, &#39;Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt&#39;], dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])) . Converting text into numbers . When dealing with a text problem, one of the first things you&#39;ll have to do before you can build a model is to convert your text to numbers. . There are a few ways to do this, namely: . Tokenization: Straight mapping from token to number(can be modelled but quickly gets too big) . | Embedding: richer representation of relationships between tokens (can limit size + can be learned) . | . Tokenization vs Embedding . E.g. I am a Human . I = 0 am = 1 a = 2 Human = 3 . or using one-hot enconding . [[1,0,0,0], [0,1,0,0] [0,0,1,0] [0,0,0,1]] . or by creating an Embedding . [[0.492, 0.005, 0.019], [0.060, 0.233, 0.899], [0.741, 0.983, 0.567]] . There are a few ways to do this, namely: . Tokenization: Straight mapping from token to number(can be modelled but quickly gets too big) . | Embedding: richer representation of relationships between tokens (can limit size + can be learned) . | . Text Vectorization(Tokenization) . train_sentences[:5] . array([&#39;@mogacola @zamtriossu i screamed after hitting tweet&#39;, &#39;Imagine getting flattened by Kurt Zouma&#39;, &#39;@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....&#39;, &#34;@shakjn @C7 @Magnums im shaking in fear he&#39;s gonna hack the planet&#34;, &#39;Somehow find you and I collide http://t.co/Ee8RpOahPk&#39;], dtype=object) . import tensorflow as tf from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # Use the default TextVectorization parameters(just to demonstrate the default values of this instance) text_vectorizer = TextVectorization(max_tokens = None, # how many words in the vocabulary(automatically add &lt;OOV?) standardize = &quot;lower_and_strip_punctuation&quot;, split = &quot;whitespace&quot;, # or SPLIT_WHITESPACE also works ngrams = None, # Create groups of n-words output_mode =&quot;int&quot;, # How to map token to numbers output_sequence_length = None) # how long do you want the sequences to be #pad_to_max_tokens = True) not valid if using max_tokens=None . len(train_sentences[0].split()) . 7 . round(sum([len(i.split()) for i in train_sentences])/len(train_sentences)) . 15 . max_vocab_length = 10000 # max number of words to have in our vocabulary max_length = 15 # max lenght our sequences will be (e.g. how many words from a tweet our model see) text_vectorizer = TextVectorization(max_tokens = max_vocab_length, output_mode =&quot;int&quot;, output_sequence_length = max_length) . text_vectorizer.adapt(train_sentences) . sample_sentence = &quot;There&#39;s a flood in my street!&quot; text_vectorizer([sample_sentence]) . &lt;tf.Tensor: shape=(1, 15), dtype=int64, numpy= array([[264, 3, 232, 4, 13, 698, 0, 0, 0, 0, 0, 0, 0, 0, 0]])&gt; . # Choose a random sentence from the training dataset and tokenize it random_sentence = random.choice(train_sentences) print(f&quot;Original text: n {random_sentence} n n Vectorized version: &quot;) text_vectorizer([random_sentence]) . Original text: @AlfaPedia It might have come out ONLY too burst as a Bomb making him suicide bomber Vectorized version: . &lt;tf.Tensor: shape=(1, 15), dtype=int64, numpy= array([[ 1, 15, 843, 24, 220, 36, 126, 150, 2174, 26, 3, 108, 572, 158, 87]])&gt; . words_in_vocab = text_vectorizer.get_vocabulary() # get all of the unique words in our training data top_5_words = words_in_vocab[:5] # get the most common words bottom_5_words= words_in_vocab[-5:] # get the least common words print(f&quot;Number of words in vocab: {len(words_in_vocab)}&quot;) print(f&quot;5 most common words: {top_5_words}&quot;) print(f&quot;5 least common words: {bottom_5_words}&quot;) . Number of words in vocab: 10000 5 most common words: [&#39;&#39;, &#39;[UNK]&#39;, &#39;the&#39;, &#39;a&#39;, &#39;in&#39;] 5 least common words: [&#39;pages&#39;, &#39;paeds&#39;, &#39;pads&#39;, &#39;padres&#39;, &#39;paddytomlinson1&#39;] . Creating and Embedding using an Emedding Layer . To make our embedding we are going to use TensorFlow&#39;s Embedding layer. . The parameters we care most about for our embedding layer: . input_dim = the size of our vocabulary | output_dim = the size of the output embedding vector, for example, a value of 100 would mean each token gets represented by a vector 100 long | input_length = length of sequences being passed to the embedding layer. | . from tensorflow.keras import layers embedding = layers.Embedding(input_dim = max_vocab_length, # set input size output_dim = 128, embeddings_initializer = &#39;uniform&#39;, input_length = max_length # how long is each input ) embedding . &lt;keras.layers.embeddings.Embedding at 0x7f6a9734a110&gt; . # Get a random sentence from the training set random_sentence = random.choice(train_sentences) print(f&quot;Original text: n {random_sentence} n nEmbedded version: &quot;) # Embed the random sentence (turn it into dense vectors of fixed size) sample_embed = embedding(text_vectorizer([random_sentence])) sample_embed . Original text: Fall back this first break homebuyer miscalculation that could destruction thousands: MwjCdk Embedded version: . &lt;tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy= array([[[ 0.00859234, -0.02991412, 0.00175644, ..., 0.02193626, 0.04184195, -0.02619057], [-0.00470889, -0.04967961, -0.01436696, ..., -0.00270484, -0.00828482, 0.01314512], [ 0.00405866, 0.02752711, 0.01645878, ..., -0.00964943, 0.02267227, 0.00371256], ..., [ 0.00350211, -0.04788604, 0.00196681, ..., 0.02803201, 0.00803728, 0.02167306], [ 0.00350211, -0.04788604, 0.00196681, ..., 0.02803201, 0.00803728, 0.02167306], [ 0.00350211, -0.04788604, 0.00196681, ..., 0.02803201, 0.00803728, 0.02167306]]], dtype=float32)&gt; . . # Check out a single token&#39;s embedding sample_embed[0][0], sample_embed[0][0].shape, random_sentence[0] . (&lt;tf.Tensor: shape=(128,), dtype=float32, numpy= array([ 0.00859234, -0.02991412, 0.00175644, -0.03367729, -0.03769684, -0.0291563 , -0.02644087, -0.03562082, 0.04090923, -0.02225679, -0.01017957, -0.04141713, -0.01146468, -0.04587493, 0.02797664, -0.02889217, 0.03275966, -0.02597183, 0.03522148, -0.04480093, 0.0389016 , -0.0169893 , 0.0142766 , -0.03043303, 0.04030218, -0.04211314, 0.03645163, 0.02257297, 0.02544535, -0.00259332, -0.01840631, -0.02087172, -0.03521866, -0.01772154, -0.04674302, -0.00397594, 0.03044703, -0.00820515, -0.04558386, -0.02431409, 0.041382 , 0.02238775, 0.00051622, -0.01694447, -0.01824627, 0.03566995, -0.04934913, -0.00467784, 0.02524788, 0.02154641, -0.0166956 , -0.00147361, 0.02120248, 0.0378341 , 0.00150269, -0.02470231, -0.04485737, -0.03325255, -0.0435687 , -0.02844893, 0.04605688, -0.04954116, 0.01102605, -0.03360488, -0.00772928, 0.00679797, 0.03716531, -0.04825834, 0.03694325, -0.04381819, -0.01490177, 0.01195564, 0.04442109, 0.02496224, 0.00903082, -0.00705872, -0.00284895, -0.0252423 , 0.01289615, 0.00021081, 0.01263725, 0.00752894, -0.02753489, 0.03229766, -0.02484094, -0.00039848, -0.02652047, -0.03955548, -0.04918641, -0.02551678, 0.02933357, 0.03285935, 0.03242579, -0.01814985, 0.04405214, -0.03942751, 0.04565073, 0.02142942, 0.04559227, 0.02825892, -0.04690794, 0.04336696, -0.04502993, 0.03199968, -0.01844629, -0.01408292, 0.0381173 , 0.02374512, -0.04499742, 0.00774354, 0.03652367, -0.03725274, -0.0037598 , -0.03174049, -0.00407821, 0.04460347, 0.00373029, 0.01934692, 0.04332647, 0.03005439, -0.00177516, 0.02241433, -0.01339862, -0.00733767, 0.03994515, 0.02193626, 0.04184195, -0.02619057], dtype=float32)&gt;, TensorShape([128]), &#39;F&#39;) . . Modelling a text dataset . Experiment Number Model . 0 | Naive Bayes with TF-IDF encoder(baseline) | . 1 | Feed-forward neural network(dense model) | . 2 | LSTM(RNN) | . 3 | GRU(RNN) | . 4 | Bidirectional-LSTM(RNN) | . 5 | 1D Convolutional Neural Network | . 6 | TensorFlow Hub Pretrained Feature Extractor | . 7 | TensorFlow Hub Pretrained Feature Extractor (10% of data) | . Standards steps involved in running Modelling Experiments: . Create a model | Build a model | Fit a model | Evaluate our model | . Model 0: Naive Bayes with TF-IDF encoder . To create our baseline, we&#39;ll use Sklearn&#39;s Multinomial Naive Bayes using the TF-IDF formula to convert our words to numbers. . Note: It&#39;s common practice to use non-DL algorithms as a baseline because of their speed and later we can use DL algorithms to see if we can improve upon them. . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.naive_bayes import MultinomialNB from sklearn.pipeline import Pipeline # Create tokenization and modelling pipeline model_0 = Pipeline([ (&quot;tfidf&quot;, TfidfVectorizer()), # convert words to numbers using tfidf (&quot;clf&quot;, MultinomialNB()) # Model the text ]) # Fit the pipeline to the training dat model_0.fit(train_sentences, train_labels) . Pipeline(steps=[(&#39;tfidf&#39;, TfidfVectorizer()), (&#39;clf&#39;, MultinomialNB())]) . baseline_score = model_0.score(val_sentences, val_labels) print(f&quot;Our baseline model achieves an accuracy of : {baseline_score*100:.2f}%&quot;) . Our baseline model achieves an accuracy of : 79.27% . train_df.target.value_counts() . 0 4342 1 3271 Name: target, dtype: int64 . So, our model is doing better than guessing, since there are almost 50% of example of both label types in the dataset. . baseline_preds = model_0.predict(val_sentences) baseline_preds[:20] . array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]) . Creating an evaluation function for our model experiments . We can evaluate all of our model&#39;s predictions with different metrics every time, instead of repeating the code, we can create a function so that we can reuse it later for all the model experiments. . The functions should output the following evaluation metrics: . Accuracy | Precision | Recall | F1-Score | . Resource: Metrics and scoring: quantifying the quality of predictions . from sklearn.metrics import accuracy_score, precision_recall_fscore_support def calculate_results(y_true, y_pred): &quot;&quot;&quot; Calculates model accuracy, precision, recall and f1 score of a binary classification model &quot;&quot;&quot; # Calculate the model accuracy model_accuracy = accuracy_score(y_true, y_pred)*100 # Calculate model precision, recall and f1-score using &quot;weighted&quot; average model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true,y_pred, average = &quot;weighted&quot;) model_results = {&quot;accuracy&quot;: model_accuracy, &quot;precision&quot;: model_precision, &quot;recall&quot;: model_recall, &quot;f1&quot;: model_f1} return model_results . baseline_results = calculate_results(y_true = val_labels, y_pred = baseline_preds) baseline_results . {&#39;accuracy&#39;: 79.26509186351706, &#39;f1&#39;: 0.7862189758049549, &#39;precision&#39;: 0.8111390004213173, &#39;recall&#39;: 0.7926509186351706} . Model 1: A Simple Dense Model . from helper_functions import create_tensorboard_callback # Create a directory to save TensorBoard logs SAVE_DIR = &quot;model_logs&quot; . from tensorflow.keras import layers inputs = layers.Input(shape=(1,), dtype=&quot;string&quot;) # inputs are 1-dimensional strings x = text_vectorizer(inputs) # turn the input text into numbers x = embedding(x) # create an embedding of the numerized numbers x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding outputs = layers.Dense(1, activation=&quot;sigmoid&quot;)(x) # create the output layer, want binary outputs so use sigmoid activation model_1 = tf.keras.Model(inputs, outputs, name=&quot;model_1_dense&quot;) # construct the model . model_1.summary() . Model: &#34;model_1_dense&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 1)] 0 text_vectorization_4 (TextV (None, 15) 0 ectorization) embedding_1 (Embedding) (None, 15, 128) 1280000 global_average_pooling1d (G (None, 128) 0 lobalAveragePooling1D) dense (Dense) (None, 1) 129 ================================================================= Total params: 1,280,129 Trainable params: 1,280,129 Non-trainable params: 0 _________________________________________________________________ . model_1.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) . model_1_history = model_1.fit(train_sentences, train_labels, epochs = 5, validation_data = (val_sentences,val_labels), callbacks = [create_tensorboard_callback(dir_name = SAVE_DIR, experiment_name = &quot;model_1_dense&quot; )]) . Saving TensorBoard log files to: model_logs/model_1_dense/20220311-074354 Epoch 1/5 215/215 [==============================] - 5s 7ms/step - loss: 0.6103 - accuracy: 0.6875 - val_loss: 0.5359 - val_accuracy: 0.7598 Epoch 2/5 215/215 [==============================] - 1s 7ms/step - loss: 0.4411 - accuracy: 0.8156 - val_loss: 0.4690 - val_accuracy: 0.7848 Epoch 3/5 215/215 [==============================] - 1s 6ms/step - loss: 0.3472 - accuracy: 0.8600 - val_loss: 0.4571 - val_accuracy: 0.7953 Epoch 4/5 215/215 [==============================] - 1s 7ms/step - loss: 0.2837 - accuracy: 0.8924 - val_loss: 0.4681 - val_accuracy: 0.7927 Epoch 5/5 215/215 [==============================] - 1s 6ms/step - loss: 0.2371 - accuracy: 0.9126 - val_loss: 0.4866 - val_accuracy: 0.7887 . model_1.evaluate(val_sentences, val_labels) . 24/24 [==============================] - 0s 4ms/step - loss: 0.4866 - accuracy: 0.7887 . [0.4866005778312683, 0.7887139320373535] . model_1_pred_probs = model_1.predict(val_sentences) model_1_pred_probs.shape . (762, 1) . model_1_pred_probs[1] . array([0.8119219], dtype=float32) . These are prediction probabilites that came out of the output layer. . model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) model_1_preds[:20] . &lt;tf.Tensor: shape=(20,), dtype=float32, numpy= array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)&gt; . model_1_results = calculate_results(y_true = val_labels, y_pred = model_1_preds) model_1_results . {&#39;accuracy&#39;: 78.87139107611549, &#39;f1&#39;: 0.7848945056280915, &#39;precision&#39;: 0.7964015586347394, &#39;recall&#39;: 0.7887139107611548} . import numpy as np np.array(list(model_1_results.values())) &gt; np.array(list(baseline_results.values())) . array([False, False, False, False]) . Visualizing learned Embeddings: . words_in_vocab = text_vectorizer.get_vocabulary() len(words_in_vocab), words_in_vocab[:10] . (10000, [&#39;&#39;, &#39;[UNK]&#39;, &#39;the&#39;, &#39;a&#39;, &#39;in&#39;, &#39;to&#39;, &#39;of&#39;, &#39;and&#39;, &#39;i&#39;, &#39;is&#39;]) . model_1.summary() . Model: &#34;model_1_dense&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 1)] 0 text_vectorization_4 (TextV (None, 15) 0 ectorization) embedding_1 (Embedding) (None, 15, 128) 1280000 global_average_pooling1d (G (None, 128) 0 lobalAveragePooling1D) dense (Dense) (None, 1) 129 ================================================================= Total params: 1,280,129 Trainable params: 1,280,129 Non-trainable params: 0 _________________________________________________________________ . # (these numerical representations of each token in our training data, which has been learned for 5 epochs) embed_weights = model_1.get_layer(&quot;embedding_1&quot;).get_weights()[0] embed_weights # Same size as vocab size and embedding_dim . array([[ 0.02186958, -0.06763938, 0.02146152, ..., 0.01258572, 0.02883859, 0.00023849], [-0.02620384, -0.02298776, -0.02039188, ..., 0.00896182, 0.03781693, -0.03427465], [ 0.0165944 , 0.01236528, 0.03275075, ..., 0.03906085, 0.05525858, -0.03807979], ..., [-0.0099979 , 0.04100901, -0.04915455, ..., 0.03296768, 0.03509828, 0.02508564], [ 0.01705603, -0.04142731, -0.00240709, ..., -0.05540716, 0.0721622 , -0.03262765], [ 0.06986112, -0.09984355, 0.02866708, ..., -0.02748252, 0.08362035, -0.03691495]], dtype=float32) . print(embed_weights.shape) # same size as vocab size and embedding_dim . (10000, 128) . Now we have got the embedding matrix our model has learned to represent our tokens, let&#39;s see how we can visualize it. To do so, Tensorflow has a tool called projector: https://projector.tensorflow.org/ . And TensorFlow also has an incredible guide on word embeddings: https://www.tensorflow.org/text/guide/word_embeddings . import io out_v = io.open(&#39;vectors.tsv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) out_m = io.open(&#39;metadata.tsv&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) for index, word in enumerate(words_in_vocab): if index == 0: continue # skip 0, it&#39;s padding. vec = embed_weights[index] out_v.write(&#39; t&#39;.join([str(x) for x in vec]) + &quot; n&quot;) out_m.write(word + &quot; n&quot;) out_v.close() out_m.close() . try: from google.colab import files files.download(&#39;vectors.tsv&#39;) files.download(&#39;metadata.tsv&#39;) except Exception: pass . def plot_functions(k_values, m_values, n_values): return create_tensorboard_callback(dirname, dirpath) . tf.keras.utils.text_dataset_from_directory( directory, labels=&#39;inferred&#39;, label_mode=&#39;int&#39;, class_names=None, batch_size=32, max_length=None, shuffle=True, seed=None, validation_split=None, subset=None, follow_links=False ) .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/tensorflow/natural-language-processing(nlp)/2022/05/11/Natural-Language-Processing-with-TensorFlow.html",
            "relUrl": "/deep%20learning/neuralnetworks/tensorflow/natural-language-processing(nlp)/2022/05/11/Natural-Language-Processing-with-TensorFlow.html",
            "date": " • May 11, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "GANs - General Adversarial Networks - Live Blog post",
            "content": "import tensorflow as tf . import glob import imageio import matplotlib.pyplot as plt import numpy as np import os import PIL from tensorflow.keras import layers import time from IPython import display . (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data() . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step 11501568/11490434 [==============================] - 0s 0us/step . train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(&#39;float32&#39;) train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1] . BUFFER_SIZE = 60000 BATCH_SIZE = 256 . train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE) . def make_generator_model(): model = tf.keras.Sequential() model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,))) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Reshape((7, 7, 256))) assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding=&#39;same&#39;, use_bias=False)) assert model.output_shape == (None, 7, 7, 128) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=&#39;same&#39;, use_bias=False)) assert model.output_shape == (None, 14, 14, 64) model.add(layers.BatchNormalization()) model.add(layers.LeakyReLU()) model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=&#39;same&#39;, use_bias=False, activation=&#39;tanh&#39;)) assert model.output_shape == (None, 28, 28, 1) return model . generator = make_generator_model() noise = tf.random.normal([1, 100]) generated_image = generator(noise, training=False) plt.imshow(generated_image[0, :, :, 0], cmap=&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7f1f6cf7b3d0&gt; . def make_discriminator_model(): model = tf.keras.Sequential() model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding=&#39;same&#39;, input_shape=[28, 28, 1])) model.add(layers.LeakyReLU()) model.add(layers.Dropout(0.3)) model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding=&#39;same&#39;)) model.add(layers.LeakyReLU()) model.add(layers.Dropout(0.3)) model.add(layers.Flatten()) model.add(layers.Dense(1)) return model . discriminator = make_discriminator_model() decision = discriminator(generated_image) print (decision) . tf.Tensor([[0.00035544]], shape=(1, 1), dtype=float32) . cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True) . def discriminator_loss(real_output, fake_output): real_loss = cross_entropy(tf.ones_like(real_output), real_output) fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output) total_loss = real_loss + fake_loss return total_loss . def generator_loss(fake_output): return cross_entropy(tf.ones_like(fake_output), fake_output) . generator_optimizer = tf.keras.optimizers.Adam(1e-4) discriminator_optimizer = tf.keras.optimizers.Adam(1e-4) . checkpoint_dir = &#39;./training_checkpoints&#39; checkpoint_prefix = os.path.join(checkpoint_dir, &quot;ckpt&quot;) checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer, discriminator_optimizer=discriminator_optimizer, generator=generator, discriminator=discriminator) . EPOCHS = 50 noise_dim = 100 num_examples_to_generate = 16 # You will reuse this seed overtime (so it&#39;s easier) # to visualize progress in the animated GIF) seed = tf.random.normal([num_examples_to_generate, noise_dim]) . # This annotation causes the function to be &quot;compiled&quot;. @tf.function def train_step(images): noise = tf.random.normal([BATCH_SIZE, noise_dim]) with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape: generated_images = generator(noise, training=True) real_output = discriminator(images, training=True) fake_output = discriminator(generated_images, training=True) gen_loss = generator_loss(fake_output) disc_loss = discriminator_loss(real_output, fake_output) gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables) gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables) generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables)) . def train(dataset, epochs): for epoch in range(epochs): start = time.time() for image_batch in dataset: train_step(image_batch) # Produce images for the GIF as you go display.clear_output(wait=True) generate_and_save_images(generator, epoch + 1, seed) # Save the model every 15 epochs if (epoch + 1) % 15 == 0: checkpoint.save(file_prefix = checkpoint_prefix) print (&#39;Time for epoch {} is {} sec&#39;.format(epoch + 1, time.time()-start)) # Generate after the final epoch display.clear_output(wait=True) generate_and_save_images(generator, epochs, seed) . def generate_and_save_images(model, epoch, test_input): # Notice `training` is set to False. # This is so all layers run in inference mode (batchnorm). predictions = model(test_input, training=False) fig = plt.figure(figsize=(4, 4)) for i in range(predictions.shape[0]): plt.subplot(4, 4, i+1) plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=&#39;gray&#39;) plt.axis(&#39;off&#39;) plt.savefig(&#39;image_at_epoch_{:04d}.png&#39;.format(epoch)) plt.show() . train(train_dataset, EPOCHS) . checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir)) . &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1fdfd4b290&gt; . def display_image(epoch_no): return PIL.Image.open(&#39;image_at_epoch_{:04d}.png&#39;.format(epoch_no)) . display_image(EPOCHS) . anim_file = &#39;dcgan.gif&#39; with imageio.get_writer(anim_file, mode=&#39;I&#39;) as writer: filenames = glob.glob(&#39;image*.png&#39;) filenames = sorted(filenames) for filename in filenames: image = imageio.imread(filename) writer.append_data(image) image = imageio.imread(filename) writer.append_data(image) . from google.colab import files files.download(&#39;dcgan.gif&#39;) . Image(open(&#39;/content/dcgan.gif&#39;,&#39;rb&#39;).read()) .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/tensorflow/general_adversarial_networks/2022/04/03/GANs-Implementation.html",
            "relUrl": "/deep%20learning/neuralnetworks/tensorflow/general_adversarial_networks/2022/04/03/GANs-Implementation.html",
            "date": " • Apr 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Higgs Dataset Signal/Background Detection Model",
            "content": "The Higgs Dataset is one of the largest dataset containing labelled data whether a signal is actual higgs signal or of the background. This is a Classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not. . source of the dataset: Daniel Whiteson daniel &#39;@&#39; uci.edu, Assistant Professor, Physics &amp; Astronomy, Univ. of California Irvine . Downloading the Dataset: . First, we are goint to download the dataset from the UCI Machine Learning repository and since it is in the .gz (zip format) format we are going to unzip the data and get the data ready for processing. . Data Set Information: . The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set. . !wget &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz&quot; . --2022-03-21 13:33:43-- https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252 Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2816407858 (2.6G) [application/x-httpd-php] Saving to: ‘HIGGS.csv.gz’ HIGGS.csv.gz 100%[===================&gt;] 2.62G 56.5MB/s in 49s 2022-03-21 13:34:32 (55.3 MB/s) - ‘HIGGS.csv.gz’ saved [2816407858/2816407858] . !gzip -d HIGGS.csv.gz . Processing the Data: . The most important step after unzipping the data is to process the data inorder to visualize and get to know our data better. . Note: The First column named &#39;labels&#39; has values of 0/1 which denotes signal/background respectively . 1 for Signal 0 for Background . import pandas as pd . columns = [&quot;labels&quot;,&quot;lepton pT&quot;, &quot;lepton eta&quot;, &quot;lepton phi&quot;, &quot;missing energy magnitude&quot;, &quot;missing energy phi&quot;, &quot;jet 1 pt&quot;, &quot;jet 1 eta&quot;, &quot;jet 1 phi&quot;, &quot;jet 1 b-tag&quot;, &quot;jet 2 pt&quot;, &quot;jet 2 eta&quot;, &quot;jet 2 phi&quot;, &quot;jet 2 b-tag&quot;, &quot;jet 3 pt&quot;, &quot;jet 3 eta&quot;, &quot;jet 3 phi&quot;, &quot;jet 3 b-tag&quot;, &quot;jet 4 pt&quot;, &quot;jet 4 eta&quot;, &quot;jet 4 phi&quot;, &quot;jet 4 b-tag&quot;, &quot;m_jj&quot;, &quot;m_jjj&quot;, &quot;m_lv&quot;, &quot;m_jlv&quot;, &quot;m_bb&quot;, &quot;m_wbb&quot;, &quot;m_wwbb&quot;] higgs_df = pd.read_csv(&quot;/content/HIGGS.csv&quot;, header= None, names = columns) . higgs_df . labels lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 0 1.0 | 0.869293 | -0.635082 | 0.225690 | 0.327470 | -0.689993 | 0.754202 | -0.248573 | -1.092064 | 0.000000 | ... | -0.010455 | -0.045767 | 3.101961 | 1.353760 | 0.979563 | 0.978076 | 0.920005 | 0.721657 | 0.988751 | 0.876678 | . 1 1.0 | 0.907542 | 0.329147 | 0.359412 | 1.497970 | -0.313010 | 1.095531 | -0.557525 | -1.588230 | 2.173076 | ... | -1.138930 | -0.000819 | 0.000000 | 0.302220 | 0.833048 | 0.985700 | 0.978098 | 0.779732 | 0.992356 | 0.798343 | . 2 1.0 | 0.798835 | 1.470639 | -1.635975 | 0.453773 | 0.425629 | 1.104875 | 1.282322 | 1.381664 | 0.000000 | ... | 1.128848 | 0.900461 | 0.000000 | 0.909753 | 1.108330 | 0.985692 | 0.951331 | 0.803252 | 0.865924 | 0.780118 | . 3 0.0 | 1.344385 | -0.876626 | 0.935913 | 1.992050 | 0.882454 | 1.786066 | -1.646778 | -0.942383 | 0.000000 | ... | -0.678379 | -1.360356 | 0.000000 | 0.946652 | 1.028704 | 0.998656 | 0.728281 | 0.869200 | 1.026736 | 0.957904 | . 4 1.0 | 1.105009 | 0.321356 | 1.522401 | 0.882808 | -1.205349 | 0.681466 | -1.070464 | -0.921871 | 0.000000 | ... | -0.373566 | 0.113041 | 0.000000 | 0.755856 | 1.361057 | 0.986610 | 0.838085 | 1.133295 | 0.872245 | 0.808487 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 10999995 1.0 | 1.159912 | 1.013847 | 0.108615 | 1.495524 | -0.537545 | 2.342396 | -0.839740 | 1.320683 | 0.000000 | ... | -0.097068 | 1.190680 | 3.101961 | 0.822136 | 0.766772 | 1.002191 | 1.061233 | 0.837004 | 0.860472 | 0.772484 | . 10999996 1.0 | 0.618388 | -1.012982 | 1.110139 | 0.941023 | -0.379199 | 1.004656 | 0.348535 | -1.678593 | 2.173076 | ... | -0.216995 | 1.049177 | 3.101961 | 0.826829 | 0.989809 | 1.029104 | 1.199679 | 0.891481 | 0.938490 | 0.865269 | . 10999997 1.0 | 0.700559 | 0.774251 | 1.520182 | 0.847112 | 0.211230 | 1.095531 | 0.052457 | 0.024553 | 2.173076 | ... | 1.585235 | 1.713962 | 0.000000 | 0.337374 | 0.845208 | 0.987610 | 0.883422 | 1.888438 | 1.153766 | 0.931279 | . 10999998 0.0 | 1.178030 | 0.117796 | -1.276980 | 1.864457 | -0.584370 | 0.998519 | -1.264549 | 1.276333 | 0.000000 | ... | 1.399515 | -1.313189 | 0.000000 | 0.838842 | 0.882890 | 1.201380 | 0.939216 | 0.339705 | 0.759070 | 0.719119 | . 10999999 0.0 | 0.464477 | -0.337047 | 0.229019 | 0.954596 | -0.868466 | 0.430004 | -0.271348 | -1.252278 | 2.173076 | ... | -1.652782 | -0.586254 | 0.000000 | 0.752535 | 0.740727 | 0.986917 | 0.663952 | 0.576084 | 0.541427 | 0.517420 | . 11000000 rows × 29 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; y = higgs_df[&quot;labels&quot;] y . 0 1.0 1 1.0 2 1.0 3 0.0 4 1.0 ... 10999995 1.0 10999996 1.0 10999997 1.0 10999998 0.0 10999999 0.0 Name: labels, Length: 11000000, dtype: float64 . X = higgs_df.drop(columns = &#39;labels&#39;) X . lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag jet 2 pt ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 0 0.869293 | -0.635082 | 0.225690 | 0.327470 | -0.689993 | 0.754202 | -0.248573 | -1.092064 | 0.000000 | 1.374992 | ... | -0.010455 | -0.045767 | 3.101961 | 1.353760 | 0.979563 | 0.978076 | 0.920005 | 0.721657 | 0.988751 | 0.876678 | . 1 0.907542 | 0.329147 | 0.359412 | 1.497970 | -0.313010 | 1.095531 | -0.557525 | -1.588230 | 2.173076 | 0.812581 | ... | -1.138930 | -0.000819 | 0.000000 | 0.302220 | 0.833048 | 0.985700 | 0.978098 | 0.779732 | 0.992356 | 0.798343 | . 2 0.798835 | 1.470639 | -1.635975 | 0.453773 | 0.425629 | 1.104875 | 1.282322 | 1.381664 | 0.000000 | 0.851737 | ... | 1.128848 | 0.900461 | 0.000000 | 0.909753 | 1.108330 | 0.985692 | 0.951331 | 0.803252 | 0.865924 | 0.780118 | . 3 1.344385 | -0.876626 | 0.935913 | 1.992050 | 0.882454 | 1.786066 | -1.646778 | -0.942383 | 0.000000 | 2.423265 | ... | -0.678379 | -1.360356 | 0.000000 | 0.946652 | 1.028704 | 0.998656 | 0.728281 | 0.869200 | 1.026736 | 0.957904 | . 4 1.105009 | 0.321356 | 1.522401 | 0.882808 | -1.205349 | 0.681466 | -1.070464 | -0.921871 | 0.000000 | 0.800872 | ... | -0.373566 | 0.113041 | 0.000000 | 0.755856 | 1.361057 | 0.986610 | 0.838085 | 1.133295 | 0.872245 | 0.808487 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 10999995 1.159912 | 1.013847 | 0.108615 | 1.495524 | -0.537545 | 2.342396 | -0.839740 | 1.320683 | 0.000000 | 1.858587 | ... | -0.097068 | 1.190680 | 3.101961 | 0.822136 | 0.766772 | 1.002191 | 1.061233 | 0.837004 | 0.860472 | 0.772484 | . 10999996 0.618388 | -1.012982 | 1.110139 | 0.941023 | -0.379199 | 1.004656 | 0.348535 | -1.678593 | 2.173076 | 1.002570 | ... | -0.216995 | 1.049177 | 3.101961 | 0.826829 | 0.989809 | 1.029104 | 1.199679 | 0.891481 | 0.938490 | 0.865269 | . 10999997 0.700559 | 0.774251 | 1.520182 | 0.847112 | 0.211230 | 1.095531 | 0.052457 | 0.024553 | 2.173076 | 1.345027 | ... | 1.585235 | 1.713962 | 0.000000 | 0.337374 | 0.845208 | 0.987610 | 0.883422 | 1.888438 | 1.153766 | 0.931279 | . 10999998 1.178030 | 0.117796 | -1.276980 | 1.864457 | -0.584370 | 0.998519 | -1.264549 | 1.276333 | 0.000000 | 0.733136 | ... | 1.399515 | -1.313189 | 0.000000 | 0.838842 | 0.882890 | 1.201380 | 0.939216 | 0.339705 | 0.759070 | 0.719119 | . 10999999 0.464477 | -0.337047 | 0.229019 | 0.954596 | -0.868466 | 0.430004 | -0.271348 | -1.252278 | 2.173076 | 0.449350 | ... | -1.652782 | -0.586254 | 0.000000 | 0.752535 | 0.740727 | 0.986917 | 0.663952 | 0.576084 | 0.541427 | 0.517420 | . 11000000 rows × 28 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; MODEL 1: Dataset with 100,000 labelled examples . Instead of taking our entire dataset, which is very huge we are going to first try the 100,000 examples for better understanding the future prospects of our model. Insted of dividing the dataset serial-wise we are going to use sample method from pandas to randomnly sample 100k examples from our dataset. We are going to use the seed values as 42 for reproducing the results later. . higgs_df_subset =higgs_df.sample(n = 100000, random_state = 42) higgs_df_subset . labels lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 3967303 0.0 | 1.138683 | -0.726635 | -0.005790 | 0.204118 | 0.153842 | 1.585904 | -0.045576 | -1.448527 | 1.086538 | ... | -2.439800 | 0.073642 | 0.000000 | 1.790497 | 1.730592 | 0.980587 | 0.743065 | 2.378752 | 1.534863 | 1.227558 | . 5946179 1.0 | 0.404633 | 1.014821 | -1.050041 | 1.136441 | -1.403536 | 3.218436 | -1.944837 | 0.801788 | 0.000000 | ... | -1.174742 | -0.912542 | 0.000000 | 1.072789 | 0.649697 | 0.981147 | 2.004577 | 0.521853 | 1.246037 | 1.461494 | . 6910558 0.0 | 1.137585 | 0.325251 | 1.453598 | 0.804114 | 0.893516 | 0.418095 | -1.164536 | -0.585919 | 0.000000 | ... | 0.280201 | -0.982461 | 3.101961 | 0.816500 | 0.933867 | 0.988956 | 0.852772 | 0.415455 | 0.737194 | 0.691437 | . 3414332 0.0 | 1.380438 | -0.595149 | -0.727112 | 0.465392 | -0.057453 | 0.399224 | -0.076273 | 1.080084 | 2.173076 | ... | 1.261267 | 1.129085 | 0.000000 | 0.563342 | 0.857068 | 0.992465 | 0.875139 | 0.512037 | 0.686362 | 0.887047 | . 5840458 1.0 | 0.962628 | 1.191110 | -1.161568 | 1.541759 | 0.569159 | 1.337374 | 0.810973 | 0.458075 | 1.086538 | ... | 0.413452 | 1.309431 | 3.101961 | 1.596246 | 1.146864 | 1.687726 | 1.178736 | 0.925320 | 1.094308 | 0.991339 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6940991 1.0 | 1.658062 | -1.514575 | -1.074455 | 1.025077 | -0.764209 | 0.513367 | -0.426815 | 1.167121 | 2.173076 | ... | -1.269683 | 1.141848 | 0.000000 | 0.299132 | 0.709217 | 0.982156 | 1.063902 | 0.713939 | 0.774338 | 0.719043 | . 6173277 1.0 | 1.630794 | -0.020507 | -1.192086 | 0.439244 | -1.028369 | 0.626410 | 0.452510 | -0.017682 | 2.173076 | ... | 0.446765 | 1.228414 | 0.000000 | 0.632595 | 0.753355 | 0.995382 | 0.976956 | 0.799586 | 0.803552 | 0.732366 | . 10306408 1.0 | 0.975988 | -0.768516 | 0.726175 | 1.106153 | -1.148883 | 1.221399 | 0.744627 | -1.521705 | 0.000000 | ... | -1.624466 | 0.309479 | 0.000000 | 0.843619 | 0.920015 | 1.429457 | 1.390645 | 1.525701 | 1.106262 | 0.933373 | . 1976752 0.0 | 1.596022 | -0.063362 | 1.569564 | 0.424958 | -1.012751 | 0.877414 | 0.106919 | -0.186766 | 0.000000 | ... | 0.924806 | 1.053062 | 0.000000 | 1.566598 | 1.086671 | 0.990974 | 1.040644 | 0.995528 | 0.882152 | 0.862665 | . 6832497 1.0 | 0.538962 | 0.348627 | -1.027292 | 1.415204 | -1.403636 | 0.765562 | 0.951586 | -0.131883 | 1.086538 | ... | -0.836615 | 1.085801 | 0.000000 | 0.681391 | 1.156464 | 0.996165 | 1.263725 | 0.957153 | 0.981716 | 1.042192 | . 100000 rows × 29 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; higgs_df_subset.to_csv(&#39;higgs_dataset_subset.csv&#39;, index=False) . We save the randomnly sampled data of 100k examples into a csv file for future use. | . X= higgs_df_subset.drop(columns = &quot;labels&quot;) X . lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag jet 2 pt ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 3967303 1.138683 | -0.726635 | -0.005790 | 0.204118 | 0.153842 | 1.585904 | -0.045576 | -1.448527 | 1.086538 | 1.598471 | ... | -2.439800 | 0.073642 | 0.000000 | 1.790497 | 1.730592 | 0.980587 | 0.743065 | 2.378752 | 1.534863 | 1.227558 | . 5946179 0.404633 | 1.014821 | -1.050041 | 1.136441 | -1.403536 | 3.218436 | -1.944837 | 0.801788 | 0.000000 | 2.238942 | ... | -1.174742 | -0.912542 | 0.000000 | 1.072789 | 0.649697 | 0.981147 | 2.004577 | 0.521853 | 1.246037 | 1.461494 | . 6910558 1.137585 | 0.325251 | 1.453598 | 0.804114 | 0.893516 | 0.418095 | -1.164536 | -0.585919 | 0.000000 | 0.653565 | ... | 0.280201 | -0.982461 | 3.101961 | 0.816500 | 0.933867 | 0.988956 | 0.852772 | 0.415455 | 0.737194 | 0.691437 | . 3414332 1.380438 | -0.595149 | -0.727112 | 0.465392 | -0.057453 | 0.399224 | -0.076273 | 1.080084 | 2.173076 | 0.644878 | ... | 1.261267 | 1.129085 | 0.000000 | 0.563342 | 0.857068 | 0.992465 | 0.875139 | 0.512037 | 0.686362 | 0.887047 | . 5840458 0.962628 | 1.191110 | -1.161568 | 1.541759 | 0.569159 | 1.337374 | 0.810973 | 0.458075 | 1.086538 | 0.549946 | ... | 0.413452 | 1.309431 | 3.101961 | 1.596246 | 1.146864 | 1.687726 | 1.178736 | 0.925320 | 1.094308 | 0.991339 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6940991 1.658062 | -1.514575 | -1.074455 | 1.025077 | -0.764209 | 0.513367 | -0.426815 | 1.167121 | 2.173076 | 0.465213 | ... | -1.269683 | 1.141848 | 0.000000 | 0.299132 | 0.709217 | 0.982156 | 1.063902 | 0.713939 | 0.774338 | 0.719043 | . 6173277 1.630794 | -0.020507 | -1.192086 | 0.439244 | -1.028369 | 0.626410 | 0.452510 | -0.017682 | 2.173076 | 0.661245 | ... | 0.446765 | 1.228414 | 0.000000 | 0.632595 | 0.753355 | 0.995382 | 0.976956 | 0.799586 | 0.803552 | 0.732366 | . 10306408 0.975988 | -0.768516 | 0.726175 | 1.106153 | -1.148883 | 1.221399 | 0.744627 | -1.521705 | 0.000000 | 1.321105 | ... | -1.624466 | 0.309479 | 0.000000 | 0.843619 | 0.920015 | 1.429457 | 1.390645 | 1.525701 | 1.106262 | 0.933373 | . 1976752 1.596022 | -0.063362 | 1.569564 | 0.424958 | -1.012751 | 0.877414 | 0.106919 | -0.186766 | 0.000000 | 1.009872 | ... | 0.924806 | 1.053062 | 0.000000 | 1.566598 | 1.086671 | 0.990974 | 1.040644 | 0.995528 | 0.882152 | 0.862665 | . 6832497 0.538962 | 0.348627 | -1.027292 | 1.415204 | -1.403636 | 0.765562 | 0.951586 | -0.131883 | 1.086538 | 1.300835 | ... | -0.836615 | 1.085801 | 0.000000 | 0.681391 | 1.156464 | 0.996165 | 1.263725 | 0.957153 | 0.981716 | 1.042192 | . 100000 rows × 28 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; y = higgs_df_subset[&quot;labels&quot;] y . 3967303 0.0 5946179 1.0 6910558 0.0 3414332 0.0 5840458 1.0 ... 6940991 1.0 6173277 1.0 10306408 1.0 1976752 0.0 6832497 1.0 Name: labels, Length: 100000, dtype: float64 . X.shape . (100000, 28) . Here, we are going to divide the labelled dataset for supervised learning. We are extracting labels in to another variable &#39;y&#39;. | And we are dropping the labels and taking the other columns of the dataset into variable &#39;X&#39;, that will serve as input data or feature set to use it as input values in our model. | . Splitting the data into Train and Test sets . we use sklearn&#39;s train_test_split method to split the dataset into training set (which we will be using for training of our Neural network) and Test set(on which we can measure the accuracy and other metrics of our model). Test set is also called validation set in this context, it is used to measure how well our Neural network is generalizing for new data that it has not seen before. We are splitting 10 percent of the entire dataset into test set and other for training (since we Neural Networks need a lot of data to train better) . import sklearn from sklearn.model_selection import train_test_split X_train,X_test, y_train, y_test = sklearn.model_selection.train_test_split(X,y, test_size=0.1, random_state=42, shuffle=True) . X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((90000, 28), (10000, 28), (90000,), (10000,)) . We are importing some helper function to help us evaluate and visualize our model&#39;s results. | . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2022-03-21 13:51:24-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2022-03-21 13:51:24 (72.4 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import confusion_matrix, plot_loss_curves, create_tensorboard_callback . Building a baseline model . We are going to build a baseline model with only two hidden layers and train it for 5 epochs. After viewing the results from our baseline model we can improve upon the model and tune our hyperparameters to make better predictions. . import tensorflow as tf tf.random.set_seed(42) higgs_model_1 = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(10, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&#39;accuracy&#39;]) history_1 = higgs_model_1.fit(X_train, y_train, epochs = 5, validation_data = (X_test,y_test), callbacks =[create_tensorboard_callback(dir_name = &quot;higgs_noise_detection_model&quot;, experiment_name=&#39;HIGGS_MODEL_NOISE_DETECTION_100K&#39;)]) . Saving TensorBoard log files to: higgs_noise_detection_model/HIGGS_MODEL_NOISE_DETECTION_100K/20220321-135947 Epoch 1/5 2813/2813 [==============================] - 13s 5ms/step - loss: 0.6426 - accuracy: 0.6257 - val_loss: 0.6196 - val_accuracy: 0.6578 Epoch 2/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.6116 - accuracy: 0.6616 - val_loss: 0.6007 - val_accuracy: 0.6786 Epoch 3/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.5992 - accuracy: 0.6753 - val_loss: 0.6017 - val_accuracy: 0.6716 Epoch 4/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.5917 - accuracy: 0.6825 - val_loss: 0.5948 - val_accuracy: 0.6818 Epoch 5/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.5866 - accuracy: 0.6869 - val_loss: 0.5985 - val_accuracy: 0.6807 . plot_loss_curves(history_1) . The Training accuracy for our baseline model is 68.69%. The Validation accuracy of our baseline model is 68.07% . Even though the metrics turned out not very good. Our model is good at generalizing the features it is leanring from training data so that it can perform well in our Validation data(the data which model has not seen before). Our model is not overfitting . Saving the Model . tf.keras.models.save_model(higgs_model_1,filepath = &quot;/content/drive/MyDrive/higgs_model_100K&quot; ) . INFO:tensorflow:Assets written to: /content/drive/MyDrive/higgs_model_100K/assets . MODEL 2: Dataset with 500,000 labelled examples . Instead of taking our entire dataset, which is very huge we are going to first try the 500,000 examples. We are going to use sample method from pandas to randomnly sample 500k examples from our dataset. We are going to use the seed values as 42 for reproducing the results later. . higgs_df_subset_500k =higgs_df.sample(n = 500000, random_state = 42) higgs_df_subset_500k . labels lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 3967303 0.0 | 1.138683 | -0.726635 | -0.005790 | 0.204118 | 0.153842 | 1.585904 | -0.045576 | -1.448527 | 1.086538 | ... | -2.439800 | 0.073642 | 0.000000 | 1.790497 | 1.730592 | 0.980587 | 0.743065 | 2.378752 | 1.534863 | 1.227558 | . 5946179 1.0 | 0.404633 | 1.014821 | -1.050041 | 1.136441 | -1.403536 | 3.218436 | -1.944837 | 0.801788 | 0.000000 | ... | -1.174742 | -0.912542 | 0.000000 | 1.072789 | 0.649697 | 0.981147 | 2.004577 | 0.521853 | 1.246037 | 1.461494 | . 6910558 0.0 | 1.137585 | 0.325251 | 1.453598 | 0.804114 | 0.893516 | 0.418095 | -1.164536 | -0.585919 | 0.000000 | ... | 0.280201 | -0.982461 | 3.101961 | 0.816500 | 0.933867 | 0.988956 | 0.852772 | 0.415455 | 0.737194 | 0.691437 | . 3414332 0.0 | 1.380438 | -0.595149 | -0.727112 | 0.465392 | -0.057453 | 0.399224 | -0.076273 | 1.080084 | 2.173076 | ... | 1.261267 | 1.129085 | 0.000000 | 0.563342 | 0.857068 | 0.992465 | 0.875139 | 0.512037 | 0.686362 | 0.887047 | . 5840458 1.0 | 0.962628 | 1.191110 | -1.161568 | 1.541759 | 0.569159 | 1.337374 | 0.810973 | 0.458075 | 1.086538 | ... | 0.413452 | 1.309431 | 3.101961 | 1.596246 | 1.146864 | 1.687726 | 1.178736 | 0.925320 | 1.094308 | 0.991339 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8516987 0.0 | 1.198344 | 0.816132 | -1.315820 | 1.703112 | 1.590997 | 1.516191 | -0.098058 | 0.060033 | 0.000000 | ... | -1.689426 | -1.133952 | 0.000000 | 2.399341 | 1.443294 | 0.987323 | 0.837583 | 1.202728 | 1.357914 | 1.363805 | . 5674844 0.0 | 2.187508 | 0.664192 | 0.996393 | 0.352704 | -1.562939 | 0.953631 | 1.452642 | -0.993939 | 1.086538 | ... | 0.347659 | -0.799340 | 0.000000 | 0.778314 | 0.849276 | 0.984151 | 0.952412 | 1.432241 | 1.072137 | 0.849107 | . 8551407 1.0 | 0.461000 | -0.296141 | -1.404043 | 1.213896 | 0.447301 | 0.937691 | 0.091076 | -0.865879 | 0.000000 | ... | 0.296024 | -0.269396 | 0.000000 | 0.792155 | 1.126328 | 1.031360 | 0.828597 | 0.912031 | 0.971815 | 0.864788 | . 7118648 1.0 | 0.780717 | 0.816132 | 1.445830 | 1.954633 | -1.285709 | 1.386109 | 0.427754 | 0.048392 | 2.173076 | ... | 0.094481 | 1.542494 | 3.101961 | 0.915686 | 1.227426 | 1.077625 | 0.561643 | 0.828827 | 0.988427 | 0.864264 | . 4302117 1.0 | 0.771932 | -0.135436 | -0.175577 | 0.687997 | -1.248873 | 0.863581 | 0.512914 | 0.833387 | 0.000000 | ... | -1.004013 | -1.299316 | 1.550981 | 0.433401 | 1.022649 | 0.986578 | 0.587924 | 0.802011 | 0.674747 | 0.612366 | . 500000 rows × 29 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; higgs_df_subset_500k.to_csv(&#39;higgs_dataset_subset_500k.csv&#39;, index=False) . X_1= higgs_df_subset_500k.drop(columns = &quot;labels&quot;) . y_1 =higgs_df_subset_500k[&quot;labels&quot;] . import sklearn from sklearn.model_selection import train_test_split X_train_1,X_test_1, y_train_1, y_test_1 = sklearn.model_selection.train_test_split(X_1,y_1, test_size=0.1, random_state=42, shuffle=True) . X_train_1.shape, X_test_1.shape, y_train_1.shape, y_test_1.shape . ((450000, 28), (50000, 28), (450000,), (50000,)) . import tensorflow as tf tf.random.set_seed(42) higgs_model_1_500k = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(10, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_1_500k.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&#39;accuracy&#39;]) history_1_500k = higgs_model_1_500k.fit(X_train_1, y_train_1, epochs = 5, validation_data = (X_test_1,y_test_1), callbacks =[create_tensorboard_callback(dir_name = &quot;higgs_noise_detection_model&quot;, experiment_name=&#39;HIGGS_MODEL_NOISE_DETECTION_WITH_500K&#39;)]) . Saving TensorBoard log files to: higgs_noise_detection_model/HIGGS_MODEL_NOISE_DETECTION_WITH_500K/20220321-140419 Epoch 1/5 14063/14063 [==============================] - 60s 4ms/step - loss: 0.6103 - accuracy: 0.6623 - val_loss: 0.5893 - val_accuracy: 0.6793 Epoch 2/5 14063/14063 [==============================] - 58s 4ms/step - loss: 0.5810 - accuracy: 0.6915 - val_loss: 0.5740 - val_accuracy: 0.6994 Epoch 3/5 14063/14063 [==============================] - 59s 4ms/step - loss: 0.5690 - accuracy: 0.7014 - val_loss: 0.5649 - val_accuracy: 0.7041 Epoch 4/5 14063/14063 [==============================] - 59s 4ms/step - loss: 0.5617 - accuracy: 0.7082 - val_loss: 0.5565 - val_accuracy: 0.7130 Epoch 5/5 14063/14063 [==============================] - 59s 4ms/step - loss: 0.5563 - accuracy: 0.7120 - val_loss: 0.5574 - val_accuracy: 0.7123 . higgs_model_1_500k.evaluate(X_test_1, y_test_1) . 1563/1563 [==============================] - 4s 3ms/step - loss: 0.5574 - accuracy: 0.7123 . [0.557417094707489, 0.7122799754142761] . plot_loss_curves(history_1_500k) . tf.keras.models.save_model(higgs_model_1_500k,filepath = &quot;/content/drive/MyDrive/higgs_model_500K&quot; ) . INFO:tensorflow:Assets written to: /content/drive/MyDrive/higgs_model_500K/assets . MODEL 3: Dataset with 1 Million labelled examples . Instead of taking our entire dataset, which is very huge we are going to first try the 1,000,000 examples for better understanding the future prospects of our model. Insted of dividing the dataset serial-wise we are going to use sample method from pandas to randomnly sample 1 million examples from our dataset. We are going to use the seed values as 42 for reproducing the results later. . higgs_df_subset_1M =higgs_df.sample(n = 1000000, random_state = 42) higgs_df_subset_1M . labels lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 3967303 0.0 | 1.138683 | -0.726635 | -0.005790 | 0.204118 | 0.153842 | 1.585904 | -0.045576 | -1.448527 | 1.086538 | ... | -2.439800 | 0.073642 | 0.000000 | 1.790497 | 1.730592 | 0.980587 | 0.743065 | 2.378752 | 1.534863 | 1.227558 | . 5946179 1.0 | 0.404633 | 1.014821 | -1.050041 | 1.136441 | -1.403536 | 3.218436 | -1.944837 | 0.801788 | 0.000000 | ... | -1.174742 | -0.912542 | 0.000000 | 1.072789 | 0.649697 | 0.981147 | 2.004577 | 0.521853 | 1.246037 | 1.461494 | . 6910558 0.0 | 1.137585 | 0.325251 | 1.453598 | 0.804114 | 0.893516 | 0.418095 | -1.164536 | -0.585919 | 0.000000 | ... | 0.280201 | -0.982461 | 3.101961 | 0.816500 | 0.933867 | 0.988956 | 0.852772 | 0.415455 | 0.737194 | 0.691437 | . 3414332 0.0 | 1.380438 | -0.595149 | -0.727112 | 0.465392 | -0.057453 | 0.399224 | -0.076273 | 1.080084 | 2.173076 | ... | 1.261267 | 1.129085 | 0.000000 | 0.563342 | 0.857068 | 0.992465 | 0.875139 | 0.512037 | 0.686362 | 0.887047 | . 5840458 1.0 | 0.962628 | 1.191110 | -1.161568 | 1.541759 | 0.569159 | 1.337374 | 0.810973 | 0.458075 | 1.086538 | ... | 0.413452 | 1.309431 | 3.101961 | 1.596246 | 1.146864 | 1.687726 | 1.178736 | 0.925320 | 1.094308 | 0.991339 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6244894 1.0 | 0.721422 | 1.623552 | 1.477457 | 0.686271 | -0.281346 | 1.012260 | 1.695248 | -0.722296 | 0.000000 | ... | 0.872338 | -0.759387 | 0.000000 | 0.503514 | 1.064794 | 0.986955 | 1.028497 | 1.186128 | 1.021383 | 0.876147 | . 2331608 0.0 | 0.331612 | -0.221145 | -0.049624 | 1.204492 | -0.422887 | 0.779944 | -0.436717 | 0.490783 | 2.173076 | ... | -1.564503 | -0.700011 | 0.000000 | 0.783572 | 1.037146 | 0.988166 | 1.016495 | 1.074260 | 0.905516 | 0.832388 | . 2032527 1.0 | 0.400790 | 0.846325 | 0.566930 | 1.019508 | -0.907278 | 0.905446 | -0.064390 | 1.379447 | 0.000000 | ... | 0.252718 | -0.656728 | 3.101961 | 0.675152 | 0.846188 | 0.982969 | 0.755627 | 0.690340 | 0.628058 | 0.613142 | . 10069137 1.0 | 1.094943 | 0.214219 | -0.193333 | 1.558868 | -0.447798 | 2.356229 | -0.188169 | 1.575696 | 0.000000 | ... | 0.600005 | -0.633976 | 0.000000 | 1.553325 | 1.625376 | 1.000506 | 0.909016 | 0.317308 | 1.170940 | 1.174410 | . 9533127 1.0 | 0.814939 | 0.368106 | -1.724198 | 0.787842 | 0.240327 | 1.134739 | 0.088105 | 1.269126 | 2.173076 | ... | -1.293002 | 0.221803 | 3.101961 | 0.774443 | 0.656949 | 1.087967 | 0.849058 | 0.885539 | 0.934052 | 0.817784 | . 1000000 rows × 29 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; higgs_df_subset_1M.to_csv(&#39;higgs_dataset_subset_1M.csv&#39;, index=False) . X_2 = higgs_df_subset_1M.drop(columns = &quot;labels&quot;) . y_2 = higgs_df_subset_1M[&quot;labels&quot;] . import sklearn from sklearn.model_selection import train_test_split X_train_2,X_test_2, y_train_2, y_test_2 = sklearn.model_selection.train_test_split(X_2,y_2, test_size=0.1, random_state=42, shuffle=True) . X_train_2.shape, X_test_2.shape, y_train_2.shape, y_test_2.shape . ((900000, 28), (100000, 28), (900000,), (100000,)) . higgs_model_1.evaluate(X_test, y_test) . 313/313 [==============================] - 1s 3ms/step - loss: 0.5985 - accuracy: 0.6807 . [0.5984857678413391, 0.6807000041007996] . import tensorflow as tf tf.random.set_seed(42) higgs_model_1_1M = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(10, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_1_1M.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&#39;accuracy&#39;]) history_1_1M = higgs_model_1_1M.fit(X_train_2, y_train_2, epochs = 5,validation_data = (X_test_2,y_test_2), callbacks =[create_tensorboard_callback(dir_name = &quot;higgs_noise_detection_model&quot;, experiment_name=&#39;HIGGS_MODEL_NOISE_DETECTION_WITH_1M&#39;)]) . Saving TensorBoard log files to: higgs_noise_detection_model/HIGGS_MODEL_NOISE_DETECTION_WITH_1M/20220321-140919 Epoch 1/5 28125/28125 [==============================] - 97s 3ms/step - loss: 0.5975 - accuracy: 0.6738 - val_loss: 0.5742 - val_accuracy: 0.6960 Epoch 2/5 28125/28125 [==============================] - 97s 3ms/step - loss: 0.5681 - accuracy: 0.7025 - val_loss: 0.5576 - val_accuracy: 0.7099 Epoch 3/5 28125/28125 [==============================] - 101s 4ms/step - loss: 0.5557 - accuracy: 0.7126 - val_loss: 0.5477 - val_accuracy: 0.7192 Epoch 4/5 28125/28125 [==============================] - 94s 3ms/step - loss: 0.5476 - accuracy: 0.7187 - val_loss: 0.5499 - val_accuracy: 0.7190 Epoch 5/5 28125/28125 [==============================] - 94s 3ms/step - loss: 0.5419 - accuracy: 0.7226 - val_loss: 0.5359 - val_accuracy: 0.7255 . higgs_model_1_1M.evaluate(X_test_2, y_test_2) . 3125/3125 [==============================] - 8s 3ms/step - loss: 0.5359 - accuracy: 0.7255 . [0.5359262228012085, 0.7254999876022339] . plot_loss_curves(history_1_1M) . tf.keras.models.save_model(higgs_model_1_1M,filepath = &quot;/content/drive/MyDrive/higgs_model_1M&quot; ) . INFO:tensorflow:Assets written to: /content/drive/MyDrive/higgs_model_1M/assets . from sklearn.metrics import confusion_matrix . import numpy as np y_pred = higgs_model_1_1M.predict(X_test_2) y_pred_rev = y_pred &gt; 0.7 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . y_test_2.shape . (100000,) . from helper_functions import make_confusion_matrix . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_rev, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False) . y_pred = higgs_model_1_1M.predict(X_test_2) y_pred_rev = y_pred &gt; 0.85 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_revised, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=True) . y_pred = higgs_model_1_1M.predict(X_test_2) y_pred_rev = y_pred &gt; 0.9 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_revised, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=True) . !tensorboard dev upload --logdir ./higgs_noise_detection_model --name &quot;Higgs Dataset Background Noise detection model&quot; --description &quot; A background noise detection model for higgs data set containing signals from LHC runs&quot; --one_shot # Exits the uploader once its finished uploading . ***** TensorBoard Uploader ***** This will upload your TensorBoard logs to https://tensorboard.dev/ from the following directory: ./higgs_noise_detection_model This TensorBoard will be visible to everyone. Do not upload sensitive data. Your use of this service is subject to Google&#39;s Terms of Service &lt;https://policies.google.com/terms&gt; and Privacy Policy &lt;https://policies.google.com/privacy&gt;, and TensorBoard.dev&#39;s Terms of Service &lt;https://tensorboard.dev/policy/terms/&gt;. This notice will not be shown again while you are logged into the uploader. To log out, run `tensorboard dev auth revoke`. Continue? (yes/NO) YES Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&amp;client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&amp;state=UgvVITZ1H6juMnkCviqqbZmyz25Apz&amp;prompt=consent&amp;access_type=offline Enter the authorization code: 4/1AX4XfWjgf8D7vWxv4cFdJxA4LaD9cTclXk6nCecLKasVZOfN_jszCthQiic New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/OduPZdPhR4q9wZwnrr2xqg/ [2022-03-21T14:18:30] Started scanning logdir. [2022-03-21T14:18:34] Total uploaded: 96 scalars, 0 tensors, 4 binary objects (162.1 kB) [2022-03-21T14:18:34] Done scanning logdir. Done. View your TensorBoard at https://tensorboard.dev/experiment/OduPZdPhR4q9wZwnrr2xqg/ . . MODEL 4: Dataset with 1 Million Examples using Normalized Data . import pandas as pd import numpy as np . higgs_df_subset_1M = pd.read_csv(&quot;/content/drive/MyDrive/Higgs_Dataset_Subset/higgs_dataset_subset_1M.csv&quot;) . higgs_df_subset_1M . labels lepton pT lepton eta lepton phi missing energy magnitude missing energy phi jet 1 pt jet 1 eta jet 1 phi jet 1 b-tag ... jet 4 eta jet 4 phi jet 4 b-tag m_jj m_jjj m_lv m_jlv m_bb m_wbb m_wwbb . 0 0.0 | 1.138683 | -0.726635 | -0.005790 | 0.204118 | 0.153842 | 1.585904 | -0.045576 | -1.448527 | 1.086538 | ... | -2.439800 | 0.073642 | 0.000000 | 1.790497 | 1.730592 | 0.980587 | 0.743065 | 2.378752 | 1.534863 | 1.227558 | . 1 1.0 | 0.404633 | 1.014821 | -1.050041 | 1.136441 | -1.403536 | 3.218436 | -1.944837 | 0.801788 | 0.000000 | ... | -1.174742 | -0.912542 | 0.000000 | 1.072789 | 0.649697 | 0.981147 | 2.004577 | 0.521853 | 1.246037 | 1.461494 | . 2 0.0 | 1.137585 | 0.325251 | 1.453598 | 0.804114 | 0.893516 | 0.418095 | -1.164536 | -0.585919 | 0.000000 | ... | 0.280201 | -0.982461 | 3.101961 | 0.816500 | 0.933867 | 0.988956 | 0.852772 | 0.415455 | 0.737194 | 0.691437 | . 3 0.0 | 1.380438 | -0.595149 | -0.727112 | 0.465392 | -0.057453 | 0.399224 | -0.076273 | 1.080084 | 2.173076 | ... | 1.261267 | 1.129085 | 0.000000 | 0.563342 | 0.857068 | 0.992465 | 0.875139 | 0.512037 | 0.686362 | 0.887047 | . 4 1.0 | 0.962628 | 1.191110 | -1.161568 | 1.541759 | 0.569159 | 1.337374 | 0.810973 | 0.458075 | 1.086538 | ... | 0.413452 | 1.309431 | 3.101961 | 1.596246 | 1.146864 | 1.687726 | 1.178736 | 0.925320 | 1.094308 | 0.991339 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 999995 1.0 | 0.721422 | 1.623552 | 1.477457 | 0.686271 | -0.281346 | 1.012260 | 1.695248 | -0.722296 | 0.000000 | ... | 0.872338 | -0.759387 | 0.000000 | 0.503514 | 1.064794 | 0.986955 | 1.028497 | 1.186128 | 1.021383 | 0.876147 | . 999996 0.0 | 0.331612 | -0.221145 | -0.049624 | 1.204492 | -0.422887 | 0.779944 | -0.436717 | 0.490783 | 2.173076 | ... | -1.564503 | -0.700011 | 0.000000 | 0.783572 | 1.037146 | 0.988166 | 1.016495 | 1.074260 | 0.905516 | 0.832388 | . 999997 1.0 | 0.400790 | 0.846325 | 0.566930 | 1.019508 | -0.907278 | 0.905446 | -0.064390 | 1.379447 | 0.000000 | ... | 0.252718 | -0.656728 | 3.101961 | 0.675152 | 0.846188 | 0.982969 | 0.755627 | 0.690340 | 0.628058 | 0.613142 | . 999998 1.0 | 1.094943 | 0.214219 | -0.193333 | 1.558868 | -0.447798 | 2.356229 | -0.188169 | 1.575696 | 0.000000 | ... | 0.600005 | -0.633976 | 0.000000 | 1.553325 | 1.625376 | 1.000506 | 0.909016 | 0.317308 | 1.170940 | 1.174410 | . 999999 1.0 | 0.814939 | 0.368106 | -1.724198 | 0.787842 | 0.240327 | 1.134739 | 0.088105 | 1.269126 | 2.173076 | ... | -1.293002 | 0.221803 | 3.101961 | 0.774443 | 0.656949 | 1.087967 | 0.849058 | 0.885539 | 0.934052 | 0.817784 | . 1000000 rows × 29 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X_2 = higgs_df_subset_1M.drop(columns = &quot;labels&quot;) y_2 = higgs_df_subset_1M[&quot;labels&quot;] import sklearn from sklearn.model_selection import train_test_split X_train_2,X_test_2, y_train_2, y_test_2 = sklearn.model_selection.train_test_split(X_2,y_2, test_size=0.1, random_state=42, shuffle=True) . X_train_normalize = sklearn.preprocessing.normalize(X_train_2, norm=&#39;l2&#39;) X_train_normalize X_test_normalize = sklearn.preprocessing.normalize(X_test_2, norm=&#39;l2&#39;) X_test_normalize . array([[ 0.07698427, -0.2064811 , 0.14270308, ..., 0.17144011, 0.30802015, 0.29493686], [ 0.17588701, -0.08943365, 0.0814417 , ..., 0.15250059, 0.18445288, 0.16348773], [ 0.28982145, -0.03753603, -0.14026131, ..., 0.17654363, 0.18631549, 0.18275347], ..., [ 0.19341948, 0.15594531, -0.2644997 , ..., 0.04310668, 0.13831779, 0.18432919], [ 0.12775341, -0.29667257, -0.21784223, ..., 0.16247403, 0.12411202, 0.1180993 ], [ 0.05182394, -0.07203228, -0.2407318 , ..., 0.11954834, 0.13920714, 0.14846587]]) . X_train_normalize.shape . (900000, 28) . import tensorflow as tf tf.random.set_seed(42) higgs_model_1_normalize_1M = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(10, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_1_normalize_1M.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&#39;accuracy&#39;]) history_1_normalize_1M = higgs_model_1_normalize_1M.fit(X_train_normalize, y_train_2, epochs = 5, validation_data = (X_test_normalize,y_test_2)) . Epoch 1/5 28125/28125 [==============================] - 113s 4ms/step - loss: 0.6003 - accuracy: 0.6728 - val_loss: 0.5795 - val_accuracy: 0.6932 Epoch 2/5 28125/28125 [==============================] - 92s 3ms/step - loss: 0.5726 - accuracy: 0.6991 - val_loss: 0.5668 - val_accuracy: 0.7039 Epoch 3/5 28125/28125 [==============================] - 86s 3ms/step - loss: 0.5633 - accuracy: 0.7068 - val_loss: 0.5569 - val_accuracy: 0.7121 Epoch 4/5 28125/28125 [==============================] - 89s 3ms/step - loss: 0.5568 - accuracy: 0.7115 - val_loss: 0.5622 - val_accuracy: 0.7081 Epoch 5/5 28125/28125 [==============================] - 84s 3ms/step - loss: 0.5517 - accuracy: 0.7153 - val_loss: 0.5488 - val_accuracy: 0.7160 . y_pred = higgs_model_1_normalize_1M.predict(X_test_normalize) y_pred_rev = y_pred &gt; 0.9 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . y_pred = higgs_model_1_normalize_1M.predict(X_test_2) y_pred_rev = y_pred &gt; 0.9 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py from helper_functions import make_confusion_matrix . --2022-03-22 11:54:47-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2022-03-22 11:54:47 (85.5 MB/s) - ‘helper_functions.py’ saved [10246/10246] . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_revised, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=True) . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_revised, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=True) . from helper_functions import plot_loss_curves plot_loss_curves(history_1_normalize_1M) . import tensorflow as tf tf.random.set_seed(42) higgs_model_2_normalize_1M = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(10, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_2_normalize_1M.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(learning_rate = 0.1), metrics = [&#39;accuracy&#39;]) history_2_normalize_1M = higgs_model_1_normalize_1M.fit(X_train_normalize, y_train_2, epochs = 5, validation_data = (X_test_normalize,y_test_2)) . Epoch 1/5 28125/28125 [==============================] - 93s 3ms/step - loss: 0.5478 - accuracy: 0.7180 - val_loss: 0.5450 - val_accuracy: 0.7208 Epoch 2/5 28125/28125 [==============================] - 95s 3ms/step - loss: 0.5449 - accuracy: 0.7204 - val_loss: 0.5455 - val_accuracy: 0.7201 Epoch 3/5 28125/28125 [==============================] - 94s 3ms/step - loss: 0.5423 - accuracy: 0.7224 - val_loss: 0.5380 - val_accuracy: 0.7249 Epoch 4/5 28125/28125 [==============================] - 99s 4ms/step - loss: 0.5401 - accuracy: 0.7239 - val_loss: 0.5449 - val_accuracy: 0.7200 Epoch 5/5 28125/28125 [==============================] - 104s 4ms/step - loss: 0.5381 - accuracy: 0.7253 - val_loss: 0.5365 - val_accuracy: 0.7258 . y_pred = higgs_model_2_normalize_1M.predict(X_test_2) y_pred_rev = y_pred &gt; 0.6 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_revised, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=True) . tf.keras.models.save_model(higgs_model_2_normalize_1M,filepath = &quot;/content/drive/MyDrive/higgs_model_2_normalize_1M&quot; ) . INFO:tensorflow:Assets written to: /content/drive/MyDrive/higgs_model_2_normalize_1M/assets . from helper_functions import create_tensorboard_callback . import tensorflow as tf tf.random.set_seed(42) higgs_model_3_normalize_1M = tf.keras.Sequential([ tf.keras.layers.Dense(300, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_3_normalize_1M.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(learning_rate = 0.05), metrics = [&#39;accuracy&#39;]) history_3_normalize_1M = higgs_model_3_normalize_1M.fit(X_train_normalize, y_train_2, epochs = 5, validation_data = (X_test_normalize,y_test_2),callbacks =[create_tensorboard_callback(dir_name = &quot;higgs_noise_detection_model_with_300_units&quot;, experiment_name=&#39;HIGGS_MODEL_NOISE_DETECTION_WITH_1M_300HIDDENUNITS&#39;)]) . Saving TensorBoard log files to: higgs_noise_detection_model_with_300_units/HIGGS_MODEL_NOISE_DETECTION_WITH_1M_300HIDDENUNITS/20220322-125120 Epoch 1/5 28125/28125 [==============================] - 98s 3ms/step - loss: 0.6236 - accuracy: 0.6521 - val_loss: 0.6204 - val_accuracy: 0.6639 Epoch 2/5 28125/28125 [==============================] - 100s 4ms/step - loss: 0.6160 - accuracy: 0.6635 - val_loss: 0.6197 - val_accuracy: 0.6569 Epoch 3/5 28125/28125 [==============================] - 104s 4ms/step - loss: 0.6152 - accuracy: 0.6652 - val_loss: 0.6173 - val_accuracy: 0.6715 Epoch 4/5 28125/28125 [==============================] - 99s 4ms/step - loss: 0.6150 - accuracy: 0.6651 - val_loss: 0.6198 - val_accuracy: 0.6669 Epoch 5/5 28125/28125 [==============================] - 99s 4ms/step - loss: 0.6144 - accuracy: 0.6656 - val_loss: 0.6094 - val_accuracy: 0.6725 . y_pred = higgs_model_3_normalize_1M.predict(X_test_normalize) y_pred_rev = y_pred &gt; 0.9 y_pred_revised = np.array(y_pred_rev) y_pred_revised = tf.squeeze(y_pred_revised) y_pred_revised.shape . TensorShape([100000]) . make_confusion_matrix(y_true= y_test_2, y_pred=y_pred_revised, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=True) . higgs_model_2.evaluate(X_test, y_test) . 313/313 [==============================] - 1s 3ms/step - loss: 2.7336 - accuracy: 0.5635 . [2.7335827350616455, 0.5634999871253967] . plot_loss_curves(history_2) . tf.random.set_seed(42) higgs_model_3 = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(100, activation = &#39;relu&#39;), tf.keras.layers.Dense(10, activation = &#39;relu&#39;), tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;) ]) higgs_model_3.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&#39;accuracy&#39;]) history_3 = higgs_model_3.fit(X_train_normalize, y_train, epochs = 5, validation_data = (X_test,y_test)) . Epoch 1/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.6455 - accuracy: 0.6213 - val_loss: 1.1575 - val_accuracy: 0.6536 Epoch 2/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.6087 - accuracy: 0.6647 - val_loss: 1.2744 - val_accuracy: 0.6539 Epoch 3/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.5959 - accuracy: 0.6773 - val_loss: 1.4428 - val_accuracy: 0.6607 Epoch 4/5 2813/2813 [==============================] - 11s 4ms/step - loss: 0.5887 - accuracy: 0.6850 - val_loss: 1.4042 - val_accuracy: 0.6596 Epoch 5/5 2813/2813 [==============================] - 12s 4ms/step - loss: 0.5825 - accuracy: 0.6907 - val_loss: 2.2254 - val_accuracy: 0.5800 . higgs_model_3.evaluate(X_test, y_test) . 313/313 [==============================] - 1s 3ms/step - loss: 2.2254 - accuracy: 0.5800 . [2.225376605987549, 0.5799999833106995] . plot_loss_curves(history_3) . References: . Higgs Dataset UCI Machine learning Repository | Tensorflow documentation | Scikit Learn Preprocessing Documentation | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deeplearning/neuralnetworks/tensorflow/classification/2022/03/25/Higgs-Dataset-Machine-learning-Model.html",
            "relUrl": "/deeplearning/neuralnetworks/tensorflow/classification/2022/03/25/Higgs-Dataset-Machine-learning-Model.html",
            "date": " • Mar 25, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": " Milestone Project: Food Vision Big ",
            "content": "Milestone Project: Food Vision Big . This Notebook is an account of my working for the Udemy course :TensorFlow Developer Certificate in 2022: Zero to Mastery. This Notebook covers: . Using TensorFlow Datasets to download and explore data(all of Food101) | Creating a preprocessing function for our data | Batching and preparing datasets for modelling(making them run fast) | Setting up mixed precision training(faster model training) | . As a part of the project: . Building and training a feature extraction model | Fine-tuning feature extraction model to beat the DeepFoodpaper | Evaluating model results on TensorBoard | Evaluating model results by making and plotting predictions. | . Check GPU . Google colab offers GPUs, however not all of them are compatible with mixed prcision training. | . Google Colab offers: . K80 (not compatible) | P100 (not compatible) | Tesla T4 (compatible) | . Knowing this, in order to use mixed precision training we need access to a Tesla T4 (from within goole colab) or if we&#39;re using our own hardware our GPU needs a score of 7.0+ . # re-running this cell !nvidia-smi -L . GPU 0: Tesla K80 (UUID: GPU-d5f295c5-6906-37cd-28fd-7ccfe8b8e738) . Get Helper functions . Rather than writing all the functions we need. We can reuse the helper functions we have created before and dowload them in the colab . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2022-02-22 06:33:05-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2022-02-22 06:33:05 (90.6 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys . Using TensorFlow Datasets . TensorFlow Datasets is a place for prepared and ready-to-use machine learning datasets. Using TensorFlow datasets we can download some famous datasets to work on via the API. . Why use TensorFlow Datasets? . Load data already in Tensor format | Practice on well established datasets(for many different problem types) | Experiment with different modelling techniques on a consisten dataset. | . Why not use TensorFlow Datasets? . Datasets are static (do not change like real-world datasets) | . import tensorflow_datasets as tfds . datasets_list = tfds.list_builders()# Get all available datasets in TFDS print(&quot;food101&quot; in datasets_list) # is our target dataset in the list of TFDS datasets . True . (train_data, test_data), ds_info = tfds.load(name = &quot;food101&quot;, split = [&quot;train&quot;, &quot;validation&quot;], shuffle_files = True, as_supervised = True, # data returned in tuple format(data,label) with_info = True) . Downloading and preparing dataset food101/2.0.0 (download: 4.65 GiB, generated: Unknown size, total: 4.65 GiB) to /root/tensorflow_datasets/food101/2.0.0... Shuffling and writing examples to /root/tensorflow_datasets/food101/2.0.0.incomplete31YQ3Y/food101-train.tfrecord Shuffling and writing examples to /root/tensorflow_datasets/food101/2.0.0.incomplete31YQ3Y/food101-validation.tfrecord Dataset food101 downloaded and prepared to /root/tensorflow_datasets/food101/2.0.0. Subsequent calls will reuse this data. . Exploring the Food 10 data from TensorFlow Datasets . we want to find: . Class names | The shape of our input data(image tensors) | The datatype of our input data | What the labels look like(e.g. are they one-hot encoded or are they label encoded) | Do the labels match up with class names | . ds_info.features . FeaturesDict({ &#39;image&#39;: Image(shape=(None, None, 3), dtype=tf.uint8), &#39;label&#39;: ClassLabel(shape=(), dtype=tf.int64, num_classes=101), }) . classnames = ds_info.features[&quot;label&quot;].names classnames[:10] . [&#39;apple_pie&#39;, &#39;baby_back_ribs&#39;, &#39;baklava&#39;, &#39;beef_carpaccio&#39;, &#39;beef_tartare&#39;, &#39;beet_salad&#39;, &#39;beignets&#39;, &#39;bibimbap&#39;, &#39;bread_pudding&#39;, &#39;breakfast_burrito&#39;] . train_one_sample = train_data.take(1) # sampls are in format (image_tensor, label) train_one_sample . &lt;TakeDataset element_spec=(TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))&gt; . for image, label in train_one_sample: print(f&quot;&quot;&quot; Image shape; {image.shape}, Image datatype : {image.dtype}, Target class from Food 101 (tensor form): {label}, Class name (str form): {classnames[label.numpy()]} &quot;&quot;&quot;) . Image shape; (512, 512, 3), Image datatype : &lt;dtype: &#39;uint8&#39;&gt;, Target class from Food 101 (tensor form): 64, Class name (str form): miso_soup . # What does our image tensor look like from TFDS&#39;s Food101 look like? image . &lt;tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy= array([[[ 43, 89, 125], [ 52, 96, 131], [ 85, 128, 162], ..., [251, 254, 223], [250, 253, 222], [250, 253, 222]], [[ 42, 88, 124], [ 53, 97, 132], [ 92, 135, 169], ..., [251, 254, 223], [250, 253, 222], [250, 253, 222]], [[ 45, 89, 124], [ 52, 96, 131], [ 92, 135, 169], ..., [251, 254, 223], [250, 253, 222], [250, 253, 222]], ..., [[ 91, 99, 86], [ 89, 97, 84], [ 88, 94, 82], ..., [ 37, 44, 50], [ 34, 41, 47], [ 31, 38, 44]], [[ 91, 99, 86], [ 90, 98, 85], [ 88, 96, 83], ..., [ 38, 43, 47], [ 35, 40, 44], [ 33, 38, 42]], [[ 93, 101, 88], [ 93, 101, 88], [ 89, 97, 84], ..., [ 37, 42, 46], [ 35, 40, 44], [ 35, 40, 44]]], dtype=uint8)&gt; . . import tensorflow as tf tf.reduce_min(image), tf.reduce_max(image) . (&lt;tf.Tensor: shape=(), dtype=uint8, numpy=0&gt;, &lt;tf.Tensor: shape=(), dtype=uint8, numpy=255&gt;) . Plot an image from TensorFlow Datasets . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) plt.imshow(image) plt.title(classnames[label.numpy()]) # Add title to image to verify plt.axis(False) . (-0.5, 511.5, 511.5, -0.5) . Preprocessing our data . Neural Networks performs the best when data is in a certain way (e.g. batched, normalized etc.) So in order to get it ready for a neural network, you&#39;ll often have to write preprocessing functions and map it to your data. What we know about our data: . In uint8 datatype. | Comprise of all different size tensors(different sized images) | Not scaled (the pixel values are between 0 &amp; 255 ) | . What we know models like: . Data in float32 dtype (or for mixed precision float16 and float32) | For batches tensorflow likes all of the tensors within a batch to be of same size | Scaled (values between 0 &amp;1 ) also called normalized tensors generally perform better. Since, we&#39;re going to be using an EfficientNetBX pretrained model from tf.keras.applications we don&#39;t need to rescale our data(these architectures have rescaling built-in) | . This means our functions need to: . Reshape our images to all the same size | Conver the dtype of our image tensors from uint8 to float32 | def preprocess_img(image, label, img_shape = 224): &quot;&quot;&quot; Converts image datatype from &#39;uint8&#39; -&gt; &#39;float32&#39; and reshapes image to [img_shape, img_shape, colour_channels] &quot;&quot;&quot; image = tf.image.resize(image, [img_shape, img_shape] ) # Reshape target image #image = image/255. # Scaling images (not required for EfficientNetBX models) return tf.cast(image, tf.float32), label # returns (float32_image, label) tuple . # Preprocess a single sample of image and check the outputs preprocessed_img = preprocess_img(image, label)[0] print(f&quot;Image before preprocessing : n {image[:2]}..., n Shape: {image.shape}, n Datatype:{image.dtype}&quot;) print(f&quot;Image after preprocessing : n {preprocessed_img[:2]}..., n Shape: {preprocessed_img.shape} , n Datatype:{preprocessed_img.dtype}&quot;) . Image before preprocessing : [[[ 43 89 125] [ 52 96 131] [ 85 128 162] ... [251 254 223] [250 253 222] [250 253 222]] [[ 42 88 124] [ 53 97 132] [ 92 135 169] ... [251 254 223] [250 253 222] [250 253 222]]]..., Shape: (512, 512, 3), Datatype:&lt;dtype: &#39;uint8&#39;&gt; Image after preprocessing : [[[ 48.969387 93.68367 129.04082 ] [124.78572 164.07144 195.28572 ] [125.372444 158.94388 183.5153 ] ... [251.78574 254.78574 223.78574 ] [251. 254. 223. ] [250. 253. 222. ]] [[ 65.28572 108.688774 143.09183 ] [129.93878 169.09184 200.17348 ] [ 79.61224 115.04081 140.88266 ] ... [251.78574 254.78574 223.78574 ] [251. 254. 223. ] [250. 253. 222. ]]]..., Shape: (224, 224, 3) , Datatype:&lt;dtype: &#39;float32&#39;&gt; . . Batch &amp; Prepare datasets . # Map preprocessing function to training data ( and parallelize it) train_data = train_data.map(map_func = preprocess_img, num_parallel_calls = tf.data.AUTOTUNE) # Shuffle train data and turn it into data and prefetch it (load it faster) train_data = train_data.shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE) # Map preprocessing function to test data #test_data = test_data.map(map_func = preprocess_img, num_parallel_class= tf.data.AUTOTUNE) test_data = test_data.shuffle(buffer_size = 1000).batch(batch_size=32).prefetch(buffer_size = tf.data.AUTOTUNE) . train_data, test_data . (&lt;PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;, &lt;PrefetchDataset element_spec=(TensorSpec(shape=(None, None, None, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;) . Tensorflow maps this preprocessing function(preprocess_img) across our training dataset, then shuffle a number of elements and then batch them together and finally make sure you prepare new batches(prefetch) whilst the model is looking through (finding patterns) in the current batch. . What happens when you use prefetching (faster) versus what happens when you don&#39;t use prefetching (slower). Source: Page 422 of Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow Book by Aurélien Géron. . Create modelling callbacks . We&#39;re going to create a couple of callbacks to help us while our model trains: . TensorBoard Callback to log training results ( so we can visualize them later) | ModelCheckpoint callback to save our model&#39;s progress after feature extraction | . from helper_functions import create_tensorboard_callback # Create a Modelcheckpoint callback to save a model&#39;s progress during training checkpoint_path= &quot;model_checkpoints/cp.ckpt&quot; model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = &quot;val_acc&quot;, save_best_only = True, save_weights_only = True, verbose = 0) # Dont print whether or not model is being saved . Setup Mixed precision training . For Deep understanding of mixed precision training, check out the TensorFlow guide: https://www.tensorflow.org/guide/mixed_precision . Mixed precision utilizes a combination of float32 and float16 datatypes to speed up model performance . from tensorflow.keras import mixed_precision mixed_precision.set_global_policy(&quot;mixed_float16&quot;) # set global data policy to mixed precision . WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU: Tesla K80, compute capability 3.7 See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities. If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once . WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU: Tesla K80, compute capability 3.7 See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities. If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once . !nvidia-smi . Tue Feb 22 07:56:39 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 73C P0 79W / 149W | 159MiB / 11441MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ . mixed_precision.global_policy() . &lt;Policy &#34;mixed_float16&#34;&gt; . Build feature extraction model . from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing # Create a base model input_shape = (224,224,3) base_model = tf.keras.applications.EfficientNetB0(include_top=False) base_model.trainable = False # Create a functional model inputs = layers.Input(shape = input_shape, name = &quot;input_layer&quot;) # Note: EfficientNetBX models have rescaling built-in but if your model doesn&#39;t you need to have # x = preprocessing.Rescaling(1/255.)(x) x = base_model(inputs, training = False) x = layers.GlobalAveragePooling2D()(x) x = layers.Dense(len(classnames))(x) outputs = layers.Activation(&quot;softmax&quot;, dtype = tf.float32, name = &quot;softmax_float32&quot;)(x) model = tf.keras.Model(inputs,outputs) # Compile the model model.compile(loss = &quot;sparse_categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) . model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (InputLayer) [(None, 224, 224, 3)] 0 efficientnetb0 (Functional) (None, None, None, 1280) 4049571 global_average_pooling2d_2 (None, 1280) 0 (GlobalAveragePooling2D) dense_1 (Dense) (None, 101) 129381 softmax_float32 (Activation (None, 101) 0 ) ================================================================= Total params: 4,178,952 Trainable params: 129,381 Non-trainable params: 4,049,571 _________________________________________________________________ . Checking layes datatype policies(verifying mixed precision) . for layer in model.layers: print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) . input_layer True float32 &lt;Policy &#34;float32&#34;&gt; efficientnetb0 False float32 &lt;Policy &#34;mixed_float16&#34;&gt; global_average_pooling2d_2 True float32 &lt;Policy &#34;mixed_float16&#34;&gt; dense_1 True float32 &lt;Policy &#34;mixed_float16&#34;&gt; softmax_float32 True float32 &lt;Policy &#34;float32&#34;&gt; . Going through the above output we notice: . layer.name : the human readable name of a particular layer | layer.trainable : is the layer trainable or not? | layer.dtype : the data type of layer stores its variables in | layer.dtype_policy : the data type policy a layers computes on its variables with | . # Check the base model layers dtypes for layer in model.layers[1].layers: print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) . input_4 False float32 &lt;Policy &#34;float32&#34;&gt; rescaling_3 False float32 &lt;Policy &#34;mixed_float16&#34;&gt; normalization_3 False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_conv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; top_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; top_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; top_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; . . Fit the feature extraction Model . If our goal is to fine-tune a pretrained model, the general order of doing things is: . Build a feature extraction model(train a couple output layers with base layers frozen) | Fine-tune some of the frozen layers | # Fit the model with callbacks history_101_food_classes_feature_extract = model.fit(train_data, epochs=3, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=int(0.15 * len(test_data)), callbacks=[create_tensorboard_callback(&quot;training_logs&quot;, &quot;efficientnetb0_101_classes_all_data_feature_extract&quot;), model_checkpoint]) . Saving TensorBoard log files to: training_logs/efficientnetb0_101_classes_all_data_feature_extract/20220222-094838 Epoch 1/3 2368/2368 [==============================] - 309s 119ms/step - loss: 1.8179 - accuracy: 0.5596 - val_loss: 1.2317 - val_accuracy: 0.6756 Epoch 2/3 2368/2368 [==============================] - 264s 111ms/step - loss: 1.2942 - accuracy: 0.6668 - val_loss: 1.1215 - val_accuracy: 0.7050 Epoch 3/3 2368/2368 [==============================] - 272s 114ms/step - loss: 1.1436 - accuracy: 0.7034 - val_loss: 1.0969 - val_accuracy: 0.7050 . Nice, looks like our feature extraction model is performing pretty well. How about we evaluate it on the whole test dataset? . results_feature_extract_model = model.evaluate(test_data) results_feature_extract_model . 790/790 [==============================] - 90s 113ms/step - loss: 1.0927 - accuracy: 0.7059 . [1.092657208442688, 0.7059009671211243] . !mkdir -p saved_model . INFO:tensorflow:Assets written to: saved_model/food_vision_big/assets . INFO:tensorflow:Assets written to: saved_model/food_vision_big/assets . loaded_model = tf.keras.models.load_model(&#39;saved_model/food_vision_big&#39;) . # Check the layers in the base model and see what dtype policy they&#39;re using for layer in base_model.layers: print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) . input_1 False float32 &lt;Policy &#34;float32&#34;&gt; rescaling False float32 &lt;Policy &#34;mixed_float16&#34;&gt; normalization False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_conv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; stem_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block1a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block2b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block3b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block4c_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block5c_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_dwconv_pad False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6b_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6c_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_drop False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block6d_add False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_expand_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_expand_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_expand_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_dwconv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_squeeze False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_reshape False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_reduce False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_expand False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_se_excite False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_project_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; block7a_project_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; top_conv False float32 &lt;Policy &#34;mixed_float16&#34;&gt; top_bn False float32 &lt;Policy &#34;mixed_float16&#34;&gt; top_activation False float32 &lt;Policy &#34;mixed_float16&#34;&gt; . . results_loaded_model = loaded_model.evaluate(test_data) . 790/790 [==============================] - 99s 125ms/step - loss: 1.0927 - accuracy: 0.7059 . # Note: this will only work if you&#39;ve instatiated results variables import numpy as np assert np.isclose(results_feature_extract_model, results_loaded_model).all() . Milestone Project 1: FOODVISION model . TODO: . Fine-tuning our feature extraction model to beat the DeepFoodpaper | Evaluating our model results on TensorBoard | Evaluating our model results by making and plotting predictions | . TODO: Preparing our model&#39;s layers for fine-tuning . Next: Fine-tune the feature extraction model to beat the DeepFood paper. . Like all good cooking shows, I&#39;ve saved a model I prepared earlier (the feature extraction model from above) to Google Storage. . You can download it to make sure you&#39;re using the same model as originall trained going forward. . !wget https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip . --2022-02-22 10:18:04-- https://storage.googleapis.com/ztm_tf_course/food_vision/07_efficientnetb0_feature_extract_model_mixed_precision.zip Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 173.194.193.128, 173.194.194.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 16976857 (16M) [application/zip] Saving to: ‘07_efficientnetb0_feature_extract_model_mixed_precision.zip’ 07_efficientnetb0_f 100%[===================&gt;] 16.19M 65.1MB/s in 0.2s 2022-02-22 10:18:04 (65.1 MB/s) - ‘07_efficientnetb0_feature_extract_model_mixed_precision.zip’ saved [16976857/16976857] . !mkdir downloaded_gs_model # create new dir to store downloaded feature extraction model !unzip 07_efficientnetb0_feature_extract_model_mixed_precision.zip -d downloaded_gs_model . Archive: 07_efficientnetb0_feature_extract_model_mixed_precision.zip creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/ creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/ inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/variables.data-00000-of-00001 inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/variables/variables.index inflating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/saved_model.pb creating: downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision/assets/ . # Load and evaluate downloaded GS model tf.get_logger().setLevel(&#39;INFO&#39;) # hide warning logs loaded_gs_model = tf.keras.models.load_model(&quot;downloaded_gs_model/07_efficientnetb0_feature_extract_model_mixed_precision&quot;) . WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named &#34;keras_metadata.pb&#34; in the SavedModel directory. . WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named &#34;keras_metadata.pb&#34; in the SavedModel directory. WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_158253) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_191539) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_196076) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_195780) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_196153) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_180010) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_191136) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_160354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_195703) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_159392) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_191213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_193678) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_194051) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_158768) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_191907) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_162720) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_194708) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_196195) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_194258) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_188022) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_161995) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_183149) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_158824) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_159787) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_158482) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_158588) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_195449) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_194377) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_162615) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_192238) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_160121) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_192860) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_191865) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_160016) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_194750) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_169029) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_170771) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_159448) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_194631) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_192979) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_193263) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_160977) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_162953) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_159836) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_191581) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_158539) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_162382) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_196449) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_163238) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_162277) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_192487) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_191255) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_163009) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_194335) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_193559) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_159730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_161759) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_192161) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_193305) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_160748) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_161371) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_192937) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_196568) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_191788) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_159106) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_159497) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_161315) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_184891) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_model_layer_call_and_return_conditional_losses_178256) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_161710) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_161653) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_159212) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_158197) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_189764) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_192606) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_195081) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_162333) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_160797) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_194009) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_195822) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_161033) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_195330) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_159163) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_160459) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_195407) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_163058) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_192280) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_162671) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference__wrapped_model_152628) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_162044) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_158873) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_160410) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_195004) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_192564) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_161082) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_161420) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_193636) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_196775) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_160072) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_161939) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_193932) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_193186) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_158302) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_195123) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_191462) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_196526) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_160692) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. . . loaded_gs_model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (InputLayer) [(None, 224, 224, 3)] 0 efficientnetb0 (Functional) (None, None, None, 1280) 4049571 pooling_layer (GlobalAverag (None, 1280) 0 ePooling2D) dense (Dense) (None, 101) 129381 softmax_float32 (Activation (None, 101) 0 ) ================================================================= Total params: 4,178,952 Trainable params: 129,381 Non-trainable params: 4,049,571 _________________________________________________________________ . results_loaded_gs_model = loaded_gs_model.evaluate(test_data) results_loaded_gs_model . 790/790 [==============================] - 89s 112ms/step - loss: 1.0881 - accuracy: 0.7065 . [1.0880852937698364, 0.7065346240997314] . for layer in loaded_gs_model.layers: layer.trainable = True # set all layers to trainable print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) . input_layer True float32 &lt;Policy &#34;float32&#34;&gt; efficientnetb0 True float32 &lt;Policy &#34;mixed_float16&#34;&gt; pooling_layer True float32 &lt;Policy &#34;mixed_float16&#34;&gt; dense True float32 &lt;Policy &#34;mixed_float16&#34;&gt; softmax_float32 True float32 &lt;Policy &#34;float32&#34;&gt; . for layer in loaded_gs_model.layers: print(layer.name, layer.trainable, layer.dtype, layer.dtype_policy) . input_layer True float32 &lt;Policy &#34;float32&#34;&gt; efficientnetb0 True float32 &lt;Policy &#34;mixed_float16&#34;&gt; pooling_layer True float32 &lt;Policy &#34;mixed_float16&#34;&gt; dense True float32 &lt;Policy &#34;mixed_float16&#34;&gt; softmax_float32 True float32 &lt;Policy &#34;float32&#34;&gt; . So, Now we have all our layers trainable. We are going to train our model on the dataset with all the layers unfrozen and try to beat the results of DeepFood Paper (which has accuracy close to 77%) . # Monitor the val_loss and stop training if it doesn&#39;t improve for 3 epochs EarlyStopping_callback = tf.keras.callbacks.EarlyStopping(monitor = &quot;val_loss&quot;, patience = 3, verbose = 0) # Create ModelCheckpoint callback to save best model during fine-tuning # Save the best model only # Monitor val_loss while training and save the best model (lowest val_loss) checkpoint_path = &quot;fine_tune_checkpoints/&quot; model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor=&quot;val_loss&quot;) # Creating learning rate reduction callback reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=&quot;val_loss&quot;, factor=0.2, # multiply the learning rate by 0.2 (reduce by 5x) patience=2, verbose=1, # print out when learning rate goes down min_lr=1e-7) . # Use the Adam optimizer with a 10x lower than default learning rate loaded_gs_model.compile(loss = &quot;sparse_categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) . # Use 100 epochs as the default # Validate on 15% of the test_data # Use the create_tensorboard_callback, ModelCheckpoint and EarlyStopping callbacks you created eaelier history_101_food_classes_all_data_fine_tune = loaded_gs_model.fit(train_data, epochs = 100, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = int(0.15*len(test_data)), callbacks = [create_tensorboard_callback(&quot;training_logs&quot;, &quot;efficientb0_101_classes_all_data_fine_tuning&quot;), model_checkpoint, EarlyStopping_callback, reduce_lr]) . Saving TensorBoard log files to: training_logs/efficientb0_101_classes_all_data_fine_tuning/20220222-114347 Epoch 1/100 2367/2368 [============================&gt;.] - ETA: 0s - loss: 0.9892 - accuracy: 0.7442INFO:tensorflow:Assets written to: fine_tune_checkpoints/assets . INFO:tensorflow:Assets written to: fine_tune_checkpoints/assets . 2368/2368 [==============================] - 341s 142ms/step - loss: 0.9892 - accuracy: 0.7443 - val_loss: 1.0841 - val_accuracy: 0.7074 - lr: 1.0000e-04 Epoch 2/100 2368/2368 [==============================] - 272s 114ms/step - loss: 0.9892 - accuracy: 0.7443 - val_loss: 1.0922 - val_accuracy: 0.7052 - lr: 1.0000e-04 Epoch 3/100 2367/2368 [============================&gt;.] - ETA: 0s - loss: 0.9892 - accuracy: 0.7443 Epoch 3: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05. 2368/2368 [==============================] - 271s 113ms/step - loss: 0.9892 - accuracy: 0.7443 - val_loss: 1.0863 - val_accuracy: 0.7071 - lr: 1.0000e-04 Epoch 4/100 2368/2368 [==============================] - 270s 113ms/step - loss: 0.9892 - accuracy: 0.7443 - val_loss: 1.0884 - val_accuracy: 0.7063 - lr: 2.0000e-05 . TODO: View training results on TensorBoard . Upload and view your model&#39;s training results to TensorBoard.dev and view them. . !tensorboard dev upload --logdir ./training_logs --name &quot;Fine-tuning EfficientNetB0 on all Food101 Data&quot; --description &quot;Training results for fine-tuning EfficientNetB0 on Food101 Data with learning rate 0.0001&quot; . TODO: Evaluate your trained model . Some ideas you might want to go through: . Find the precision, recall and f1 scores for each class (all 101). | Build a confusion matrix for each of the classes. | Find your model&#39;s most wrong predictions (those with the highest prediction probability but the wrong prediction). | pred_probs = loaded_gs_model.predict(test_data, verbose=1) # set verbosity to see how long it will take . 790/790 [==============================] - 80s 98ms/step . len(pred_probs) . 25250 . pred_probs.shape . (25250, 101) . # How do they look? pred_probs[:10] . print(f&quot;Number of prediction probabilities for sample 0: {len(pred_probs[0])}&quot;) print(f&quot;What prediction probability sample 0 looks like: n {pred_probs[0]}&quot;) print(f&quot;The class with the highest predicted probability by the model for sample 0: {pred_probs[0].argmax()}&quot;) . Number of prediction probabilities for sample 0: 101 What prediction probability sample 0 looks like: [1.4401069e-02 8.7248621e-04 6.5537915e-04 1.9259790e-04 2.6844227e-04 1.4441166e-03 2.2685886e-03 1.0428986e-05 1.0808602e-01 3.4739010e-04 2.4208028e-03 8.9741443e-05 4.9706534e-03 1.7937253e-03 3.0230531e-03 3.9168997e-03 2.1527853e-04 1.1786536e-03 2.0797420e-03 1.7351459e-03 8.2122732e-04 1.9372971e-04 2.7859115e-04 2.6635322e-04 2.4794212e-03 2.4826851e-04 3.0876575e-02 2.7369595e-05 4.1981181e-03 7.9650054e-06 2.6739572e-04 4.9948638e-05 1.4542394e-02 1.4254733e-05 2.2348419e-02 1.3251959e-03 6.5145308e-05 6.4395950e-04 3.6477347e-04 2.8687369e-04 8.4456497e-06 4.3317920e-04 1.9041091e-02 1.1983844e-03 6.6171189e-05 1.5174084e-05 4.9374090e-04 8.0134459e-03 3.7708841e-04 1.0130905e-03 7.9440768e-04 7.9650054e-06 7.5740227e-03 2.9540254e-04 9.9140896e-05 9.2590140e-05 1.7641925e-03 1.1146712e-04 2.1067130e-05 4.4509084e-03 9.9738396e-04 1.2657462e-03 1.3471788e-04 1.5055999e-05 1.0029457e-05 5.9208577e-04 3.2054773e-03 5.1743595e-04 5.0249667e-04 5.2376992e-01 7.4689458e-05 6.7354698e-04 4.6839113e-03 2.8519772e-04 8.5477560e-04 4.5383235e-06 5.0249667e-04 6.2171364e-04 1.4843705e-02 6.2904228e-04 6.9618232e-05 1.5533926e-05 5.1356256e-02 2.0260060e-05 2.4757916e-03 1.6236639e-03 3.1749285e-05 9.9349557e-04 1.8844793e-06 9.6260263e-03 1.9032566e-05 1.4824790e-04 8.1074642e-05 1.2244096e-03 4.0932561e-04 3.2472243e-03 2.1824200e-04 1.2136953e-03 1.1524751e-03 2.3096017e-04 9.6509956e-02] The class with the highest predicted probability by the model for sample 0: 69 . pred_classes = pred_probs.argmax(axis=1) # How do they look? pred_classes.shape . array([ 69, 57, 46, 69, 100, 57, 10, 17, 16, 22]) . y_labels = [] for images, labels in test_data.unbatch(): # unbatch the test data and get images and labels y_labels.append(labels.numpy().argmax()) # append the index which has the largest value (labels are one-hot) y_labels[:10] # check what they look like (unshuffled) . [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] . len(y_labels) . 25250 . from sklearn.metrics import accuracy_score sklearn_accuracy = accuracy_score(y_labels, pred_classes) sklearn_accuracy . 0.0057425742574257425 . (train_data, test_data), ds_info = tfds.load(name=&quot;food101&quot;, # target dataset to get from TFDS split=[&quot;train&quot;, &quot;validation&quot;], # what splits of data should we get? note: not all datasets have train, valid, test shuffle_files=True, # shuffle files on download? as_supervised=True, # download data in tuple format (sample, label), e.g. (image, label) with_info=True) # include dataset metadata? if so, tfds.load() returns tuple (data, ds_info) . class_names = ds_info.features[&quot;label&quot;].names class_names[:10] . [&#39;apple_pie&#39;, &#39;baby_back_ribs&#39;, &#39;baklava&#39;, &#39;beef_carpaccio&#39;, &#39;beef_tartare&#39;, &#39;beet_salad&#39;, &#39;beignets&#39;, &#39;bibimbap&#39;, &#39;bread_pudding&#39;, &#39;breakfast_burrito&#39;] . References: . TensorFlow Developer Certificate in 2022: Zero to Mastery | Metrics and scoring: quantifying the quality of predictions | TensorFlow Datasets Food101 | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/tensorflow/transfer-learning/large_scale_ml/2022/02/24/Food-Vision-Milestone-Project.html",
            "relUrl": "/deep%20learning/neuralnetworks/tensorflow/transfer-learning/large_scale_ml/2022/02/24/Food-Vision-Milestone-Project.html",
            "date": " • Feb 24, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Transfer Learning with TensorFlow : Scaling Up",
            "content": "Transfer Learning in TensorFlow : Scaling Up . This Notebook is an account of my working for the Udemy course :TensorFlow Developer Certificate in 2022: Zero to Mastery. . Concepts covered in this Notebook: . Dowloading and preparing 10% of all Food101 Classes (7500+ training images) | Training a transfer learning feature extraction model. | Fine-tuning feature extraction model to beat the original Food101 with only 10% of data. | Evaluating Food Vison Mini&#39;s predictions Find the most wrong predictions(on test dataset) | . | Making predictions with Food Vision mini on our own custom images | . In this Notebook, we are scaling up our models for even larger dataset and with more Classes of data -- For all classes of the FOOD Vision Dataset(i.e. 101 classes of Food) . Creating helper functions . We can download the helper functions file that has the entire collection of custom built functions we have created in the previous notebooks. . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2022-02-20 18:37:18-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2022-02-20 18:37:18 (49.1 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir . 101 Food Classes : working with less data . Our goal is to beat the original Food101 paper with 10% of the training data, so let&#39;s download it. . The data we&#39;re downloading comes from the original Food101 Dataset but has been preprocessed using image_modification.ipynb Notebook. . !wget https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip unzip_data(&quot;101_food_classes_10_percent.zip&quot;) train_dir = &quot;101_food_classes_10_percent/train/&quot; test_dir = &quot;101_food_classes_10_percent/test/&quot; . --2022-02-20 14:26:13-- https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.193.128, 173.194.194.128, 173.194.195.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.193.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1625420029 (1.5G) [application/zip] Saving to: ‘101_food_classes_10_percent.zip’ 101_food_classes_10 100%[===================&gt;] 1.51G 216MB/s in 7.3s 2022-02-20 14:26:20 (213 MB/s) - ‘101_food_classes_10_percent.zip’ saved [1625420029/1625420029] . # how many image classes are there walk_through_dir(&quot;101_food_classes_10_percent&quot;) . There are 2 directories and 0 images in &#39;101_food_classes_10_percent&#39;. There are 101 directories and 0 images in &#39;101_food_classes_10_percent/train&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/prime_rib&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/strawberry_shortcake&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/takoyaki&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/pizza&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/filet_mignon&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/paella&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/mussels&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/eggs_benedict&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/ravioli&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/apple_pie&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/peking_duck&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/baklava&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/pork_chop&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/spaghetti_bolognese&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/huevos_rancheros&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/pad_thai&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/breakfast_burrito&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/cannoli&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/fried_rice&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/chicken_quesadilla&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/french_toast&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/garlic_bread&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/chocolate_cake&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/fried_calamari&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/macarons&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/ceviche&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/crab_cakes&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/club_sandwich&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/dumplings&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/spring_rolls&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/bread_pudding&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/sashimi&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/risotto&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/pho&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/foie_gras&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/frozen_yogurt&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/grilled_salmon&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/cheese_plate&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/poutine&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/bruschetta&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/cheesecake&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/caesar_salad&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/beet_salad&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/lasagna&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/carrot_cake&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/tuna_tartare&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/beef_carpaccio&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/falafel&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/greek_salad&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/guacamole&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/hummus&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/hamburger&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/beef_tartare&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/beignets&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/waffles&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/gnocchi&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/baby_back_ribs&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/grilled_cheese_sandwich&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/steak&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/edamame&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/spaghetti_carbonara&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/sushi&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/macaroni_and_cheese&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/churros&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/cup_cakes&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/chicken_wings&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/clam_chowder&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/scallops&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/hot_dog&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/french_onion_soup&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/ramen&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/pancakes&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/french_fries&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/panna_cotta&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/tiramisu&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/hot_and_sour_soup&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/pulled_pork_sandwich&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/escargots&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/gyoza&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/seaweed_salad&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/donuts&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/miso_soup&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/chocolate_mousse&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/shrimp_and_grits&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/red_velvet_cake&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/ice_cream&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/bibimbap&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/lobster_roll_sandwich&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/nachos&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/samosa&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/fish_and_chips&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/caprese_salad&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/croque_madame&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/tacos&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/chicken_curry&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/omelette&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/deviled_eggs&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/oysters&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/creme_brulee&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/lobster_bisque&#39;. There are 0 directories and 75 images in &#39;101_food_classes_10_percent/train/onion_rings&#39;. There are 101 directories and 0 images in &#39;101_food_classes_10_percent/test&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/prime_rib&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/strawberry_shortcake&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/takoyaki&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/pizza&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/filet_mignon&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/paella&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/mussels&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/eggs_benedict&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/ravioli&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/apple_pie&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/peking_duck&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/baklava&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/pork_chop&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/spaghetti_bolognese&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/huevos_rancheros&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/pad_thai&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/breakfast_burrito&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/cannoli&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/chicken_quesadilla&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/french_toast&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/garlic_bread&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/chocolate_cake&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/fried_calamari&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/macarons&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/ceviche&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/crab_cakes&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/club_sandwich&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/dumplings&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/spring_rolls&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/bread_pudding&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/sashimi&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/risotto&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/pho&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/foie_gras&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/frozen_yogurt&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/cheese_plate&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/poutine&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/bruschetta&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/cheesecake&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/caesar_salad&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/beet_salad&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/lasagna&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/carrot_cake&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/tuna_tartare&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/beef_carpaccio&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/falafel&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/greek_salad&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/guacamole&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/hummus&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/hamburger&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/beef_tartare&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/beignets&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/waffles&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/gnocchi&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/baby_back_ribs&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/grilled_cheese_sandwich&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/steak&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/edamame&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/spaghetti_carbonara&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/sushi&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/macaroni_and_cheese&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/churros&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/cup_cakes&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/chicken_wings&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/clam_chowder&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/scallops&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/hot_dog&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/french_onion_soup&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/ramen&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/pancakes&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/french_fries&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/panna_cotta&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/tiramisu&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/hot_and_sour_soup&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/pulled_pork_sandwich&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/escargots&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/gyoza&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/seaweed_salad&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/donuts&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/miso_soup&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/chocolate_mousse&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/shrimp_and_grits&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/red_velvet_cake&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/ice_cream&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/bibimbap&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/lobster_roll_sandwich&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/nachos&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/samosa&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/fish_and_chips&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/caprese_salad&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/croque_madame&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/tacos&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/chicken_curry&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/omelette&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/deviled_eggs&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/oysters&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/creme_brulee&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/lobster_bisque&#39;. There are 0 directories and 250 images in &#39;101_food_classes_10_percent/test/onion_rings&#39;. . . import tensorflow as tf IMG_SIZE = (224,224) train_data_all_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE) test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE, shuffle = False) # don&#39;t shuffle test data for prediction analysis . Found 7575 files belonging to 101 classes. Found 25250 files belonging to 101 classes. . Train a large Model with transfer learning on 10% of 101 food classes . Here are the steps we&#39;re going to take : . Create a ModelCheckpoint callback | Create a data augmentation layer to build data augmentation right into the layer | Build a headless(no top layers) Functional EfficientNetB0 backboned-model | Compile our model | Feature extract for 5 full passes(5 epochs on the traindataset and validate on 15% of the test data, to save epoch time) | . checkpoint_path = &quot;101_classes_10_percent_data_model_checkpoint&quot; checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only = True, monitor = &quot;val_accuracy&quot;, save_best_only = True) . from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing from tensorflow.keras.models import Sequential # Setup data augmentation data_augmentation = Sequential([ preprocessing.RandomFlip(&quot;horizontal&quot;), preprocessing.RandomRotation(0.2), preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), preprocessing.RandomZoom(0.2) # preprocessing.Rescaling(1/255.) # rescale inputs of images to between 0 and 1 required for ResNet50 like models ], name = &quot;data_augmentation&quot;) . base_model = tf.keras.applications.EfficientNetB0(include_top=False) base_model.trainable = False # Setup model architecture with trainable top layers inputs = layers.Input(shape = (224,224,3), name = &quot;input_layer&quot;) x = data_augmentation(inputs), # augment images (only happens during training phase) x = base_model(x, training = False) # Put the base model in inference mode so weights don&#39;t get updated x = layers.GlobalAveragePooling2D(name = &quot;global_avg_pool_layer&quot;)(x) outputs = layers.Dense(len(train_data_all_10_percent.class_names), activation = &quot;softmax&quot;, name = &quot;output_layer&quot;)(x) model = tf.keras.Model(inputs,outputs) . WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (InputLayer) [(None, 224, 224, 3)] 0 data_augmentation (Sequenti (None, 224, 224) 0 al) efficientnetb0 (Functional) (None, None, None, 1280) 4049571 global_avg_pool_layer (Glob (None, 1280) 0 alAveragePooling2D) output_layer (Dense) (None, 101) 129381 ================================================================= Total params: 4,178,952 Trainable params: 129,381 Non-trainable params: 4,049,571 _________________________________________________________________ . model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit history_all_classes_10_percent = model.fit(train_data_all_10_percent, epochs =5, validation_data =test_data, validation_steps = int(0.25*len(test_data)), callbacks = [checkpoint_callback]) . Epoch 1/5 WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). 237/237 [==============================] - ETA: 0s - loss: 3.4688 - accuracy: 0.2467WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). 237/237 [==============================] - 147s 541ms/step - loss: 3.4688 - accuracy: 0.2467 - val_loss: 2.6962 - val_accuracy: 0.3731 Epoch 2/5 237/237 [==============================] - 106s 447ms/step - loss: 2.3543 - accuracy: 0.4524 - val_loss: 2.2650 - val_accuracy: 0.4410 Epoch 3/5 237/237 [==============================] - 97s 409ms/step - loss: 1.9770 - accuracy: 0.5350 - val_loss: 2.0941 - val_accuracy: 0.4676 Epoch 4/5 237/237 [==============================] - 93s 391ms/step - loss: 1.7549 - accuracy: 0.5781 - val_loss: 2.0071 - val_accuracy: 0.4789 Epoch 5/5 237/237 [==============================] - 84s 354ms/step - loss: 1.6097 - accuracy: 0.6057 - val_loss: 1.9830 - val_accuracy: 0.4765 . feature_extraction_results= model.evaluate(test_data) feature_extraction_results . 790/790 [==============================] - 106s 133ms/step - loss: 1.6050 - accuracy: 0.5757 . [1.6050441265106201, 0.5757227540016174] . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) plot_loss_curves(history_all_classes_10_percent) . Ideally, the two curves should be very close to each other, but if they are not close to each other it means, our model maybe overfitting(Performing too well on the training data and not generalizing to unseen data) . Fine-tuning . base_model.trainable = True # Refreeze every layer except the last 5 for layer in base_model.layers[:-5]: layer.trainable = False . model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.0001), metrics = [&quot;accuracy&quot;]) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . # Check the trainable layers for layer in model.layers: print(layer.name, layer.trainable) . input_layer True data_augmentation True efficientnetb0 True global_avg_pool_layer True output_layer True . . # Check which layers are trainable in our base model for layer_number , layer in enumerate(model.layers[2].layers): print(layer_number, layer.name, layer.trainable) . 0 input_4 False 1 rescaling_3 False 2 normalization_3 False 3 stem_conv_pad False 4 stem_conv False 5 stem_bn False 6 stem_activation False 7 block1a_dwconv False 8 block1a_bn False 9 block1a_activation False 10 block1a_se_squeeze False 11 block1a_se_reshape False 12 block1a_se_reduce False 13 block1a_se_expand False 14 block1a_se_excite False 15 block1a_project_conv False 16 block1a_project_bn False 17 block2a_expand_conv False 18 block2a_expand_bn False 19 block2a_expand_activation False 20 block2a_dwconv_pad False 21 block2a_dwconv False 22 block2a_bn False 23 block2a_activation False 24 block2a_se_squeeze False 25 block2a_se_reshape False 26 block2a_se_reduce False 27 block2a_se_expand False 28 block2a_se_excite False 29 block2a_project_conv False 30 block2a_project_bn False 31 block2b_expand_conv False 32 block2b_expand_bn False 33 block2b_expand_activation False 34 block2b_dwconv False 35 block2b_bn False 36 block2b_activation False 37 block2b_se_squeeze False 38 block2b_se_reshape False 39 block2b_se_reduce False 40 block2b_se_expand False 41 block2b_se_excite False 42 block2b_project_conv False 43 block2b_project_bn False 44 block2b_drop False 45 block2b_add False 46 block3a_expand_conv False 47 block3a_expand_bn False 48 block3a_expand_activation False 49 block3a_dwconv_pad False 50 block3a_dwconv False 51 block3a_bn False 52 block3a_activation False 53 block3a_se_squeeze False 54 block3a_se_reshape False 55 block3a_se_reduce False 56 block3a_se_expand False 57 block3a_se_excite False 58 block3a_project_conv False 59 block3a_project_bn False 60 block3b_expand_conv False 61 block3b_expand_bn False 62 block3b_expand_activation False 63 block3b_dwconv False 64 block3b_bn False 65 block3b_activation False 66 block3b_se_squeeze False 67 block3b_se_reshape False 68 block3b_se_reduce False 69 block3b_se_expand False 70 block3b_se_excite False 71 block3b_project_conv False 72 block3b_project_bn False 73 block3b_drop False 74 block3b_add False 75 block4a_expand_conv False 76 block4a_expand_bn False 77 block4a_expand_activation False 78 block4a_dwconv_pad False 79 block4a_dwconv False 80 block4a_bn False 81 block4a_activation False 82 block4a_se_squeeze False 83 block4a_se_reshape False 84 block4a_se_reduce False 85 block4a_se_expand False 86 block4a_se_excite False 87 block4a_project_conv False 88 block4a_project_bn False 89 block4b_expand_conv False 90 block4b_expand_bn False 91 block4b_expand_activation False 92 block4b_dwconv False 93 block4b_bn False 94 block4b_activation False 95 block4b_se_squeeze False 96 block4b_se_reshape False 97 block4b_se_reduce False 98 block4b_se_expand False 99 block4b_se_excite False 100 block4b_project_conv False 101 block4b_project_bn False 102 block4b_drop False 103 block4b_add False 104 block4c_expand_conv False 105 block4c_expand_bn False 106 block4c_expand_activation False 107 block4c_dwconv False 108 block4c_bn False 109 block4c_activation False 110 block4c_se_squeeze False 111 block4c_se_reshape False 112 block4c_se_reduce False 113 block4c_se_expand False 114 block4c_se_excite False 115 block4c_project_conv False 116 block4c_project_bn False 117 block4c_drop False 118 block4c_add False 119 block5a_expand_conv False 120 block5a_expand_bn False 121 block5a_expand_activation False 122 block5a_dwconv False 123 block5a_bn False 124 block5a_activation False 125 block5a_se_squeeze False 126 block5a_se_reshape False 127 block5a_se_reduce False 128 block5a_se_expand False 129 block5a_se_excite False 130 block5a_project_conv False 131 block5a_project_bn False 132 block5b_expand_conv False 133 block5b_expand_bn False 134 block5b_expand_activation False 135 block5b_dwconv False 136 block5b_bn False 137 block5b_activation False 138 block5b_se_squeeze False 139 block5b_se_reshape False 140 block5b_se_reduce False 141 block5b_se_expand False 142 block5b_se_excite False 143 block5b_project_conv False 144 block5b_project_bn False 145 block5b_drop False 146 block5b_add False 147 block5c_expand_conv False 148 block5c_expand_bn False 149 block5c_expand_activation False 150 block5c_dwconv False 151 block5c_bn False 152 block5c_activation False 153 block5c_se_squeeze False 154 block5c_se_reshape False 155 block5c_se_reduce False 156 block5c_se_expand False 157 block5c_se_excite False 158 block5c_project_conv False 159 block5c_project_bn False 160 block5c_drop False 161 block5c_add False 162 block6a_expand_conv False 163 block6a_expand_bn False 164 block6a_expand_activation False 165 block6a_dwconv_pad False 166 block6a_dwconv False 167 block6a_bn False 168 block6a_activation False 169 block6a_se_squeeze False 170 block6a_se_reshape False 171 block6a_se_reduce False 172 block6a_se_expand False 173 block6a_se_excite False 174 block6a_project_conv False 175 block6a_project_bn False 176 block6b_expand_conv False 177 block6b_expand_bn False 178 block6b_expand_activation False 179 block6b_dwconv False 180 block6b_bn False 181 block6b_activation False 182 block6b_se_squeeze False 183 block6b_se_reshape False 184 block6b_se_reduce False 185 block6b_se_expand False 186 block6b_se_excite False 187 block6b_project_conv False 188 block6b_project_bn False 189 block6b_drop False 190 block6b_add False 191 block6c_expand_conv False 192 block6c_expand_bn False 193 block6c_expand_activation False 194 block6c_dwconv False 195 block6c_bn False 196 block6c_activation False 197 block6c_se_squeeze False 198 block6c_se_reshape False 199 block6c_se_reduce False 200 block6c_se_expand False 201 block6c_se_excite False 202 block6c_project_conv False 203 block6c_project_bn False 204 block6c_drop False 205 block6c_add False 206 block6d_expand_conv False 207 block6d_expand_bn False 208 block6d_expand_activation False 209 block6d_dwconv False 210 block6d_bn False 211 block6d_activation False 212 block6d_se_squeeze False 213 block6d_se_reshape False 214 block6d_se_reduce False 215 block6d_se_expand False 216 block6d_se_excite False 217 block6d_project_conv False 218 block6d_project_bn False 219 block6d_drop False 220 block6d_add False 221 block7a_expand_conv False 222 block7a_expand_bn False 223 block7a_expand_activation False 224 block7a_dwconv False 225 block7a_bn False 226 block7a_activation False 227 block7a_se_squeeze False 228 block7a_se_reshape False 229 block7a_se_reduce False 230 block7a_se_expand False 231 block7a_se_excite False 232 block7a_project_conv True 233 block7a_project_bn True 234 top_conv True 235 top_bn True 236 top_activation True . . fine_tune_epochs = 10 # model has already done 5 epochs(feature extraction) # This is the total no of epochs 5 for feature extraction and 5 for fine-tuning # Fine-tune model history_all_classes_10_percentp_fine_tune = model.fit(train_data_all_10_percent, epochs = fine_tune_epochs, validation_data = test_data, validation_steps =int(0.25*len(test_data)), initial_epoch = history_all_classes_10_percent.epoch[-1]) . Epoch 5/10 WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). 237/237 [==============================] - ETA: 0s - loss: 1.3739 - accuracy: 0.6396WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). 237/237 [==============================] - 100s 382ms/step - loss: 1.3739 - accuracy: 0.6396 - val_loss: 1.9134 - val_accuracy: 0.4916 Epoch 6/10 237/237 [==============================] - 86s 361ms/step - loss: 1.2250 - accuracy: 0.6817 - val_loss: 1.9096 - val_accuracy: 0.4914 Epoch 7/10 237/237 [==============================] - 80s 334ms/step - loss: 1.1593 - accuracy: 0.6944 - val_loss: 1.8845 - val_accuracy: 0.5040 Epoch 8/10 237/237 [==============================] - 81s 340ms/step - loss: 1.0929 - accuracy: 0.7067 - val_loss: 1.9180 - val_accuracy: 0.4975 Epoch 9/10 237/237 [==============================] - 76s 320ms/step - loss: 1.0192 - accuracy: 0.7291 - val_loss: 1.8902 - val_accuracy: 0.5030 Epoch 10/10 237/237 [==============================] - 77s 323ms/step - loss: 0.9608 - accuracy: 0.7430 - val_loss: 1.9129 - val_accuracy: 0.4992 . all_classes_10_percent_fine_tune_results = model.evaluate(test_data) all_classes_10_percent_fine_tune_results . 790/790 [==============================] - 106s 134ms/step - loss: 1.6050 - accuracy: 0.5757 . [1.6050441265106201, 0.5757227540016174] . compare_historys(original_history = history_all_classes_10_percent, new_history = history_all_classes_10_percentp_fine_tune, initial_epochs = 5) . Saving and loading our Model . To use our model in an external application we need to save it and export it somewhere . model.save(&quot;/content/drive/MyDrive/tensorflowcourseudemy/101_food_classes_10_percent_fine_tuned_model&quot;) . WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). INFO:tensorflow:Assets written to: /content/drive/MyDrive/tensorflowcourseudemy/101_food_classes_10_percent_fine_tuned_model/assets . # Load and evaluate the saved model loaded_model = tf.keras.models.load_model(&quot;/content/drive/MyDrive/tensorflowcourseudemy/101_food_classes_10_percent_fine_tuned_model&quot;) . WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). . . loaded_model_results = loaded_model.evaluate(test_data) loaded_model_results . WARNING:tensorflow:Model was constructed with shape (None, 224, 224) for input KerasTensor(type_spec=TensorSpec(shape=(None, 224, 224), dtype=tf.float32, name=&#39;random_flip_2_input&#39;), name=&#39;random_flip_2_input&#39;, description=&#34;created by layer &#39;random_flip_2_input&#39;&#34;), but it was called on an input with incompatible shape (None, 224, 224, 3). 790/790 [==============================] - 108s 134ms/step - loss: 1.6050 - accuracy: 0.5757 . [1.6050441265106201, 0.5757227540016174] . all_classes_10_percent_fine_tune_results . [1.6050441265106201, 0.5757227540016174] . Our loaded model gives the exact same results as the model we performed on this Notebook. That means we get the same predicts if we use the saved model in any application. . Evaluating the performance of our model across all different classes . Let&#39;s make some predictions, visualize them and then later find out which predictions were the &quot;most&quot; wrong. . import tensorflow as tf # Download a pre-trained model !wget https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip . --2022-02-20 16:21:59-- https://storage.googleapis.com/ztm_tf_course/food_vision/06_101_food_class_10_percent_saved_big_dog_model.zip Resolving storage.googleapis.com (storage.googleapis.com)... 209.85.145.128, 209.85.146.128, 142.250.125.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|209.85.145.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 46760742 (45M) [application/zip] Saving to: ‘06_101_food_class_10_percent_saved_big_dog_model.zip’ 06_101_food_class_1 100%[===================&gt;] 44.59M 55.6MB/s in 0.8s 2022-02-20 16:22:00 (55.6 MB/s) - ‘06_101_food_class_10_percent_saved_big_dog_model.zip’ saved [46760742/46760742] . unzip_data(&quot;/content/06_101_food_class_10_percent_saved_big_dog_model.zip&quot;) . # Load in saved model model = tf.keras.models.load_model(&quot;/content/06_101_food_class_10_percent_saved_big_dog_model&quot;) . WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named &#34;keras_metadata.pb&#34; in the SavedModel directory. . WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_419470) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_446460) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_450449) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_415747) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_416083) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_activation_layer_call_and_return_conditional_losses_450775) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_451847) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_417915) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_451887) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_452467) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_functional_17_layer_call_and_return_conditional_losses_438312) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_expand_activation_layer_call_and_return_conditional_losses_417583) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_418582) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_454031) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_455436) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_415524) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_451474) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_451768) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_441729) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_454357) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_activation_layer_call_and_return_conditional_losses_416695) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_454238) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_functional_17_layer_call_and_return_conditional_losses_436681) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_activation_layer_call_and_return_conditional_losses_415804) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_452919) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_453658) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_448082) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_418915) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_453539) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_452586) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_450163) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_418018) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_455357) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_417639) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_451188) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_activation_layer_call_and_return_conditional_losses_420190) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_415468) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_455476) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_417354) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_se_reduce_layer_call_and_return_conditional_losses_452213) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_452173) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_se_reduce_layer_call_and_return_conditional_losses_415571) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_451514) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_activation_layer_call_and_return_conditional_losses_417971) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_454730) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_se_reduce_layer_call_and_return_conditional_losses_416742) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_450489) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_451148) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_418194) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_se_reduce_layer_call_and_return_conditional_losses_416463) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_429711) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_443351) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_expand_activation_layer_call_and_return_conditional_losses_418526) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_453245) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_activation_layer_call_and_return_conditional_losses_416416) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_efficientnetb0_layer_call_and_return_conditional_losses_428089) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_416027) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_453912) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_activation_layer_call_and_return_conditional_losses_452546) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_se_reduce_layer_call_and_return_conditional_losses_420237) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_se_reduce_layer_call_and_return_conditional_losses_418629) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_416359) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_451395) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_454690) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_419905) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_activation_layer_call_and_return_conditional_losses_419526) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_418297) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_452094) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference__wrapped_model_408990) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5c_activation_layer_call_and_return_conditional_losses_453618) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_454984) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_expand_activation_layer_call_and_return_conditional_losses_450696) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_expand_activation_layer_call_and_return_conditional_losses_418858) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_stem_activation_layer_call_and_return_conditional_losses_450044) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_activation_layer_call_and_return_conditional_losses_418250) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_activation_layer_call_and_return_conditional_losses_453991) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_se_reduce_layer_call_and_return_conditional_losses_453285) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_expand_activation_layer_call_and_return_conditional_losses_416971) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_455683) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_se_reduce_layer_call_and_return_conditional_losses_415851) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5b_expand_activation_layer_call_and_return_conditional_losses_453166) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_top_activation_layer_call_and_return_conditional_losses_420413) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block1a_activation_layer_call_and_return_conditional_losses_450123) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_se_reduce_layer_call_and_return_conditional_losses_417075) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_expand_activation_layer_call_and_return_conditional_losses_452840) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4b_activation_layer_call_and_return_conditional_losses_417307) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_455063) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_expand_activation_layer_call_and_return_conditional_losses_419802) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_activation_layer_call_and_return_conditional_losses_419858) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block5a_se_reduce_layer_call_and_return_conditional_losses_452959) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3a_expand_activation_layer_call_and_return_conditional_losses_451069) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2a_expand_activation_layer_call_and_return_conditional_losses_450370) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_expand_activation_layer_call_and_return_conditional_losses_419138) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_419194) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_se_reduce_layer_call_and_return_conditional_losses_419573) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block7a_expand_activation_layer_call_and_return_conditional_losses_420134) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4a_activation_layer_call_and_return_conditional_losses_417028) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6c_expand_activation_layer_call_and_return_conditional_losses_454611) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block3b_expand_activation_layer_call_and_return_conditional_losses_416639) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block4c_se_reduce_layer_call_and_return_conditional_losses_417686) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. . WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. . WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function. . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-0._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-0._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-1._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-1._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-2._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-2._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-3._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-3._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-4._random_generator._generator._state_var . WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).layer-1.layer-4._random_generator._generator._state_var WARNING:absl:Importing a function (__inference_block4b_expand_activation_layer_call_and_return_conditional_losses_417251) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6d_se_reduce_layer_call_and_return_conditional_losses_455103) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_450815) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block2b_se_reduce_layer_call_and_return_conditional_losses_416130) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_activation_layer_call_and_return_conditional_losses_454317) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6a_se_reduce_layer_call_and_return_conditional_losses_418962) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. WARNING:absl:Importing a function (__inference_block6b_se_reduce_layer_call_and_return_conditional_losses_419241) with ops with unsaved custom gradients. Will likely fail if a gradient is requested. . . results_downloaded_model = model.evaluate(test_data) results_downloaded_model . 790/790 [==============================] - 108s 134ms/step - loss: 1.8027 - accuracy: 0.6078 . [1.8027207851409912, 0.6077623963356018] . Making predictions with our trained model . preds_probs = model.predict(test_data, verbose = 1) . 790/790 [==============================] - 100s 125ms/step . preds_probs . array([[5.9541941e-02, 3.5742332e-06, 4.1376889e-02, ..., 1.4138899e-09, 8.3530460e-05, 3.0897565e-03], [9.6401680e-01, 1.3753089e-09, 8.4779976e-04, ..., 5.4286684e-05, 7.8363253e-12, 9.8467334e-10], [9.5925868e-01, 3.2534019e-05, 1.4867033e-03, ..., 7.1891884e-07, 5.4398350e-07, 4.0276311e-05], ..., [1.5138583e-05, 4.0972975e-04, 8.0249712e-10, ..., 2.1742959e-05, 1.0797195e-05, 5.3789973e-01], [5.9317499e-03, 4.9236189e-03, 9.9823205e-03, ..., 1.1989424e-04, 1.6889933e-05, 4.5217723e-02], [3.1363364e-02, 7.5052544e-03, 4.2974975e-04, ..., 5.0347066e-04, 5.2056580e-06, 6.9062799e-01]], dtype=float32) . len(preds_probs) . 25250 . preds_probs.shape . (25250, 101) . The predictions were done on 25250 images and the shape of our predictions is (25250,101) . preds_probs[:10] . array([[5.9541941e-02, 3.5742332e-06, 4.1376889e-02, ..., 1.4138899e-09, 8.3530460e-05, 3.0897565e-03], [9.6401680e-01, 1.3753089e-09, 8.4779976e-04, ..., 5.4286684e-05, 7.8363253e-12, 9.8467334e-10], [9.5925868e-01, 3.2534019e-05, 1.4867033e-03, ..., 7.1891884e-07, 5.4398350e-07, 4.0276311e-05], ..., [4.7313324e-01, 1.2931301e-07, 1.4805583e-03, ..., 5.9749611e-04, 6.6969820e-05, 2.3469329e-05], [4.4571780e-02, 4.7265351e-07, 1.2258515e-01, ..., 6.3498578e-06, 7.5319103e-06, 3.6778715e-03], [7.2438985e-01, 1.9249777e-09, 5.2310857e-05, ..., 1.2291447e-03, 1.5793171e-09, 9.6395503e-05]], dtype=float32) . preds_probs[0], len(preds_probs[0]), sum(preds_probs[0]) . (array([5.9541941e-02, 3.5742332e-06, 4.1376889e-02, 1.0660903e-09, 8.1613996e-09, 8.6639682e-09, 8.0926134e-07, 8.5652442e-07, 1.9858850e-05, 8.0977554e-07, 3.1727692e-09, 9.8673388e-07, 2.8532100e-04, 7.8049661e-10, 7.4230990e-04, 3.8915794e-05, 6.4740016e-06, 2.4977169e-06, 3.7891397e-05, 2.0678806e-07, 1.5538471e-05, 8.1506892e-07, 2.6230925e-06, 2.0010653e-07, 8.3827712e-07, 5.4215743e-06, 3.7391112e-06, 1.3150788e-08, 2.7761345e-03, 2.8051816e-05, 6.8561651e-10, 2.5574524e-05, 1.6688934e-04, 7.6409645e-10, 4.0452869e-04, 1.3150487e-08, 1.7957433e-06, 1.4448400e-06, 2.3062853e-02, 8.2465459e-07, 8.5366531e-07, 1.7138503e-06, 7.0526130e-06, 1.8402382e-08, 2.8553984e-07, 7.9482870e-06, 2.0682012e-06, 1.8525193e-07, 3.3619781e-08, 3.1522335e-04, 1.0410886e-05, 8.5448306e-07, 8.4741890e-01, 1.0555387e-05, 4.4094719e-07, 3.7404192e-05, 3.5306137e-05, 3.2489079e-05, 6.7313988e-05, 1.2852399e-08, 2.6220215e-10, 1.0318094e-05, 8.5742751e-05, 1.0569768e-06, 2.1293156e-06, 3.7636986e-05, 7.5972878e-08, 2.5340833e-04, 9.2906589e-07, 1.2598188e-04, 6.2621680e-06, 1.2458612e-08, 4.0519622e-05, 6.8728390e-08, 1.2546213e-06, 5.2887103e-08, 7.5424801e-08, 7.5397300e-05, 7.7540310e-05, 6.4025420e-07, 9.9033900e-07, 2.2225931e-05, 1.5013910e-05, 1.4038655e-07, 1.2232513e-05, 1.9044673e-02, 5.0000424e-05, 4.6225618e-06, 1.5388186e-07, 3.3824463e-07, 3.9227444e-09, 1.6563394e-07, 8.1322025e-05, 4.8964989e-06, 2.4068495e-07, 2.3124319e-05, 3.1040644e-04, 3.1380074e-05, 1.4138899e-09, 8.3530460e-05, 3.0897565e-03], dtype=float32), 101, 1.0000000616546507) . All of the prediction probabilites(array of with N number of variables, where N is the number of classes) for any prediction ideally should sum up to 1. But the value we got here is 1.0000000616546507 which is close to one, it&#39;s not because there is something wrong with the model, it is because of the way computers store numbers in memory. To understand better look into the precision of various datatypes. . print(f&quot;Number of prediction probabilites for sample 0: {len(preds_probs)}&quot;) print(f&quot;What prediction probability sample 0 looks like: n {preds_probs[0]}&quot;) print(f&quot;The class with the highest predicted probability by the model for sample 0: {preds_probs[0].argmax()}&quot;) . Number of prediction probabilites for sample 0: 25250 What prediction probability sample 0 looks like: [5.9541941e-02 3.5742332e-06 4.1376889e-02 1.0660903e-09 8.1613996e-09 8.6639682e-09 8.0926134e-07 8.5652442e-07 1.9858850e-05 8.0977554e-07 3.1727692e-09 9.8673388e-07 2.8532100e-04 7.8049661e-10 7.4230990e-04 3.8915794e-05 6.4740016e-06 2.4977169e-06 3.7891397e-05 2.0678806e-07 1.5538471e-05 8.1506892e-07 2.6230925e-06 2.0010653e-07 8.3827712e-07 5.4215743e-06 3.7391112e-06 1.3150788e-08 2.7761345e-03 2.8051816e-05 6.8561651e-10 2.5574524e-05 1.6688934e-04 7.6409645e-10 4.0452869e-04 1.3150487e-08 1.7957433e-06 1.4448400e-06 2.3062853e-02 8.2465459e-07 8.5366531e-07 1.7138503e-06 7.0526130e-06 1.8402382e-08 2.8553984e-07 7.9482870e-06 2.0682012e-06 1.8525193e-07 3.3619781e-08 3.1522335e-04 1.0410886e-05 8.5448306e-07 8.4741890e-01 1.0555387e-05 4.4094719e-07 3.7404192e-05 3.5306137e-05 3.2489079e-05 6.7313988e-05 1.2852399e-08 2.6220215e-10 1.0318094e-05 8.5742751e-05 1.0569768e-06 2.1293156e-06 3.7636986e-05 7.5972878e-08 2.5340833e-04 9.2906589e-07 1.2598188e-04 6.2621680e-06 1.2458612e-08 4.0519622e-05 6.8728390e-08 1.2546213e-06 5.2887103e-08 7.5424801e-08 7.5397300e-05 7.7540310e-05 6.4025420e-07 9.9033900e-07 2.2225931e-05 1.5013910e-05 1.4038655e-07 1.2232513e-05 1.9044673e-02 5.0000424e-05 4.6225618e-06 1.5388186e-07 3.3824463e-07 3.9227444e-09 1.6563394e-07 8.1322025e-05 4.8964989e-06 2.4068495e-07 2.3124319e-05 3.1040644e-04 3.1380074e-05 1.4138899e-09 8.3530460e-05 3.0897565e-03] The class with the highest predicted probability by the model for sample 0: 52 . test_data.class_names[52] . &#39;gyoza&#39; . pred_classes = preds_probs.argmax(axis =1) pred_classes[:10] . array([52, 0, 0, 80, 79, 61, 29, 0, 85, 0]) . len(pred_classes) . 25250 . Now we&#39;ve got a predictions array of all our model&#39;s predictions, to evaluate them, we need to compare them to the ground truth labels. . y_labels = [] for images, labels in test_data.unbatch(): y_labels.append(labels.numpy().argmax()) # currently test labels look like : [0,0,0,1,0...] we want the index value y_labels[:10] # look at the first 10 . [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] . len(y_labels) . 25250 . Evluating our model&#39;s predictions . one way to check that our model&#39;s predictions array is in the same order as our test labels array is to find the accuracy score. . results_downloaded_model . [1.8027207851409912, 0.6077623963356018] . from sklearn.metrics import accuracy_score sklearn_accuracy = accuracy_score(y_true = y_labels, y_pred = pred_classes) sklearn_accuracy . 0.6077623762376237 . import numpy as np np.isclose(results_downloaded_model[1], sklearn_accuracy) . True . Making a Confusion Matrix . from helper_functions import make_confusion_matrix . class_names = test_data.class_names class_names[:10] . [&#39;apple_pie&#39;, &#39;baby_back_ribs&#39;, &#39;baklava&#39;, &#39;beef_carpaccio&#39;, &#39;beef_tartare&#39;, &#39;beet_salad&#39;, &#39;beignets&#39;, &#39;bibimbap&#39;, &#39;bread_pudding&#39;, &#39;breakfast_burrito&#39;] . # plot_confusion_matrix function - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html import itertools import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import confusion_matrix # Our function needs a different name to sklearn&#39;s plot_confusion_matrix def make_confusion_matrix(y_true, y_pred, classes=None, figsize=(10, 10), text_size=15, norm=False, savefig=False): &quot;&quot;&quot;Makes a labelled confusion matrix comparing predictions and ground truth labels. If classes is passed, confusion matrix will be labelled, if not, integer class values will be used. Args: y_true: Array of truth labels (must be same shape as y_pred). y_pred: Array of predicted labels (must be same shape as y_true). classes: Array of class labels (e.g. string form). If `None`, integer labels are used. figsize: Size of output figure (default=(10, 10)). text_size: Size of output figure text (default=15). norm: normalize values or not (default=False). savefig: save confusion matrix to file (default=False). Returns: A labelled confusion matrix plot comparing y_true and y_pred. Example usage: make_confusion_matrix(y_true=test_labels, # ground truth test labels y_pred=y_preds, # predicted labels classes=class_names, # array of class label names figsize=(15, 15), text_size=10) &quot;&quot;&quot; # Create the confustion matrix cm = confusion_matrix(y_true, y_pred) cm_norm = cm.astype(&quot;float&quot;) / cm.sum(axis=1)[:, np.newaxis] # normalize it n_classes = cm.shape[0] # find the number of classes we&#39;re dealing with # Plot the figure and make it pretty fig, ax = plt.subplots(figsize=figsize) cax = ax.matshow(cm, cmap=plt.cm.Blues) # colors will represent how &#39;correct&#39; a class is, darker == better fig.colorbar(cax) # Are there a list of classes? if classes: labels = classes else: labels = np.arange(cm.shape[0]) # Label the axes ax.set(title=&quot;Confusion Matrix&quot;, xlabel=&quot;Predicted label&quot;, ylabel=&quot;True label&quot;, xticks=np.arange(n_classes), # create enough axis slots for each class yticks=np.arange(n_classes), xticklabels=labels, # axes will labeled with class names (if they exist) or ints yticklabels=labels) # Make x-axis labels appear on bottom ax.xaxis.set_label_position(&quot;bottom&quot;) ax.xaxis.tick_bottom() ### Added: Rotate xticks for readability &amp; increase font size (required due to such a large confusion matrix) plt.xticks(rotation=70, fontsize=text_size) plt.yticks(fontsize=text_size) # Set the threshold for different colors threshold = (cm.max() + cm.min()) / 2. # Plot the text on each cell for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): if norm: plt.text(j, i, f&quot;{cm[i, j]} ({cm_norm[i, j]*100:.1f}%)&quot;, horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; threshold else &quot;black&quot;, size=text_size) else: plt.text(j, i, f&quot;{cm[i, j]}&quot;, horizontalalignment=&quot;center&quot;, color=&quot;white&quot; if cm[i, j] &gt; threshold else &quot;black&quot;, size=text_size) # Save the figure to the current working directory if savefig: fig.savefig(&quot;confusion_matrix.png&quot;) . make_confusion_matrix(y_true = y_labels, y_pred = pred_classes, classes = class_names, figsize= (100,100), text_size = 20, savefig = True) . . Making a Classification Report . Sckit_learn has a helpful function for acquiring many different classification metrics per class (e.g. precision, recall and F1 score) called classification_report. . # Make a classification report from sklearn.metrics import classification_report print(classification_report(y_true = y_labels, y_pred = pred_classes)) . precision recall f1-score support 0 0.29 0.20 0.24 250 1 0.51 0.69 0.59 250 2 0.56 0.65 0.60 250 3 0.74 0.53 0.62 250 4 0.73 0.43 0.54 250 5 0.34 0.54 0.42 250 6 0.67 0.79 0.72 250 7 0.82 0.76 0.79 250 8 0.40 0.37 0.39 250 9 0.62 0.44 0.51 250 10 0.62 0.42 0.50 250 11 0.84 0.49 0.62 250 12 0.52 0.74 0.61 250 13 0.56 0.60 0.58 250 14 0.56 0.59 0.57 250 15 0.44 0.32 0.37 250 16 0.45 0.75 0.57 250 17 0.37 0.51 0.43 250 18 0.43 0.60 0.50 250 19 0.68 0.60 0.64 250 20 0.68 0.75 0.71 250 21 0.35 0.64 0.45 250 22 0.30 0.37 0.33 250 23 0.66 0.77 0.71 250 24 0.83 0.72 0.77 250 25 0.76 0.71 0.73 250 26 0.51 0.42 0.46 250 27 0.78 0.72 0.75 250 28 0.70 0.69 0.69 250 29 0.70 0.68 0.69 250 30 0.92 0.63 0.75 250 31 0.78 0.70 0.74 250 32 0.75 0.83 0.79 250 33 0.89 0.98 0.94 250 34 0.68 0.78 0.72 250 35 0.78 0.66 0.72 250 36 0.53 0.56 0.55 250 37 0.30 0.55 0.39 250 38 0.78 0.63 0.69 250 39 0.27 0.33 0.30 250 40 0.72 0.81 0.76 250 41 0.81 0.62 0.70 250 42 0.50 0.58 0.54 250 43 0.75 0.60 0.67 250 44 0.74 0.45 0.56 250 45 0.77 0.85 0.81 250 46 0.81 0.46 0.58 250 47 0.44 0.49 0.46 250 48 0.45 0.81 0.58 250 49 0.50 0.44 0.47 250 50 0.54 0.39 0.46 250 51 0.71 0.86 0.78 250 52 0.51 0.77 0.61 250 53 0.67 0.68 0.68 250 54 0.88 0.75 0.81 250 55 0.86 0.69 0.76 250 56 0.56 0.24 0.34 250 57 0.62 0.45 0.52 250 58 0.68 0.58 0.62 250 59 0.70 0.37 0.49 250 60 0.83 0.59 0.69 250 61 0.54 0.81 0.65 250 62 0.72 0.49 0.58 250 63 0.94 0.86 0.90 250 64 0.78 0.85 0.81 250 65 0.82 0.82 0.82 250 66 0.69 0.32 0.44 250 67 0.41 0.58 0.48 250 68 0.90 0.78 0.83 250 69 0.84 0.82 0.83 250 70 0.62 0.83 0.71 250 71 0.81 0.46 0.59 250 72 0.64 0.65 0.65 250 73 0.51 0.44 0.47 250 74 0.72 0.61 0.66 250 75 0.84 0.90 0.87 250 76 0.78 0.78 0.78 250 77 0.36 0.27 0.31 250 78 0.79 0.74 0.76 250 79 0.44 0.81 0.57 250 80 0.57 0.60 0.59 250 81 0.65 0.70 0.68 250 82 0.38 0.31 0.34 250 83 0.58 0.80 0.67 250 84 0.61 0.38 0.47 250 85 0.44 0.74 0.55 250 86 0.71 0.86 0.78 250 87 0.41 0.39 0.40 250 88 0.83 0.80 0.81 250 89 0.71 0.31 0.43 250 90 0.92 0.69 0.79 250 91 0.83 0.87 0.85 250 92 0.68 0.65 0.67 250 93 0.31 0.38 0.34 250 94 0.61 0.54 0.57 250 95 0.74 0.61 0.67 250 96 0.56 0.29 0.38 250 97 0.45 0.74 0.56 250 98 0.47 0.33 0.39 250 99 0.52 0.27 0.35 250 100 0.59 0.70 0.64 250 accuracy 0.61 25250 macro avg 0.63 0.61 0.61 25250 weighted avg 0.63 0.61 0.61 25250 . . The numbers above give a great class-by-class evaluation of our model&#39;s predictions but with so many classes, they&#39;re quite hard to understand. . # Get a dictionary of the classification report classification_report_dict = classification_report(y_labels, pred_classes, output_dict = True) classification_report_dict . {&#39;0&#39;: {&#39;f1-score&#39;: 0.24056603773584903, &#39;precision&#39;: 0.29310344827586204, &#39;recall&#39;: 0.204, &#39;support&#39;: 250}, &#39;1&#39;: {&#39;f1-score&#39;: 0.5864406779661017, &#39;precision&#39;: 0.5088235294117647, &#39;recall&#39;: 0.692, &#39;support&#39;: 250}, &#39;10&#39;: {&#39;f1-score&#39;: 0.5047619047619047, &#39;precision&#39;: 0.6235294117647059, &#39;recall&#39;: 0.424, &#39;support&#39;: 250}, &#39;100&#39;: {&#39;f1-score&#39;: 0.641025641025641, &#39;precision&#39;: 0.5912162162162162, &#39;recall&#39;: 0.7, &#39;support&#39;: 250}, &#39;11&#39;: {&#39;f1-score&#39;: 0.6161616161616161, &#39;precision&#39;: 0.8356164383561644, &#39;recall&#39;: 0.488, &#39;support&#39;: 250}, &#39;12&#39;: {&#39;f1-score&#39;: 0.6105610561056106, &#39;precision&#39;: 0.5196629213483146, &#39;recall&#39;: 0.74, &#39;support&#39;: 250}, &#39;13&#39;: {&#39;f1-score&#39;: 0.5775193798449612, &#39;precision&#39;: 0.5601503759398496, &#39;recall&#39;: 0.596, &#39;support&#39;: 250}, &#39;14&#39;: {&#39;f1-score&#39;: 0.574757281553398, &#39;precision&#39;: 0.5584905660377358, &#39;recall&#39;: 0.592, &#39;support&#39;: 250}, &#39;15&#39;: {&#39;f1-score&#39;: 0.36744186046511623, &#39;precision&#39;: 0.4388888888888889, &#39;recall&#39;: 0.316, &#39;support&#39;: 250}, &#39;16&#39;: {&#39;f1-score&#39;: 0.5654135338345864, &#39;precision&#39;: 0.4530120481927711, &#39;recall&#39;: 0.752, &#39;support&#39;: 250}, &#39;17&#39;: {&#39;f1-score&#39;: 0.42546063651591287, &#39;precision&#39;: 0.3659942363112392, &#39;recall&#39;: 0.508, &#39;support&#39;: 250}, &#39;18&#39;: {&#39;f1-score&#39;: 0.5008403361344538, &#39;precision&#39;: 0.4318840579710145, &#39;recall&#39;: 0.596, &#39;support&#39;: 250}, &#39;19&#39;: {&#39;f1-score&#39;: 0.6411889596602972, &#39;precision&#39;: 0.6832579185520362, &#39;recall&#39;: 0.604, &#39;support&#39;: 250}, &#39;2&#39;: {&#39;f1-score&#39;: 0.6022304832713754, &#39;precision&#39;: 0.5625, &#39;recall&#39;: 0.648, &#39;support&#39;: 250}, &#39;20&#39;: {&#39;f1-score&#39;: 0.7123809523809523, &#39;precision&#39;: 0.68, &#39;recall&#39;: 0.748, &#39;support&#39;: 250}, &#39;21&#39;: {&#39;f1-score&#39;: 0.45261669024045265, &#39;precision&#39;: 0.350109409190372, &#39;recall&#39;: 0.64, &#39;support&#39;: 250}, &#39;22&#39;: {&#39;f1-score&#39;: 0.3291592128801431, &#39;precision&#39;: 0.2977346278317152, &#39;recall&#39;: 0.368, &#39;support&#39;: 250}, &#39;23&#39;: {&#39;f1-score&#39;: 0.7134935304990757, &#39;precision&#39;: 0.6632302405498282, &#39;recall&#39;: 0.772, &#39;support&#39;: 250}, &#39;24&#39;: {&#39;f1-score&#39;: 0.7708779443254817, &#39;precision&#39;: 0.8294930875576036, &#39;recall&#39;: 0.72, &#39;support&#39;: 250}, &#39;25&#39;: {&#39;f1-score&#39;: 0.734020618556701, &#39;precision&#39;: 0.7574468085106383, &#39;recall&#39;: 0.712, &#39;support&#39;: 250}, &#39;26&#39;: {&#39;f1-score&#39;: 0.4625550660792952, &#39;precision&#39;: 0.5147058823529411, &#39;recall&#39;: 0.42, &#39;support&#39;: 250}, &#39;27&#39;: {&#39;f1-score&#39;: 0.7494824016563146, &#39;precision&#39;: 0.776824034334764, &#39;recall&#39;: 0.724, &#39;support&#39;: 250}, &#39;28&#39;: {&#39;f1-score&#39;: 0.6935483870967742, &#39;precision&#39;: 0.6991869918699187, &#39;recall&#39;: 0.688, &#39;support&#39;: 250}, &#39;29&#39;: {&#39;f1-score&#39;: 0.6910569105691057, &#39;precision&#39;: 0.7024793388429752, &#39;recall&#39;: 0.68, &#39;support&#39;: 250}, &#39;3&#39;: {&#39;f1-score&#39;: 0.616822429906542, &#39;precision&#39;: 0.7415730337078652, &#39;recall&#39;: 0.528, &#39;support&#39;: 250}, &#39;30&#39;: {&#39;f1-score&#39;: 0.7476190476190476, &#39;precision&#39;: 0.9235294117647059, &#39;recall&#39;: 0.628, &#39;support&#39;: 250}, &#39;31&#39;: {&#39;f1-score&#39;: 0.7357293868921776, &#39;precision&#39;: 0.7802690582959642, &#39;recall&#39;: 0.696, &#39;support&#39;: 250}, &#39;32&#39;: {&#39;f1-score&#39;: 0.7855787476280836, &#39;precision&#39;: 0.7472924187725631, &#39;recall&#39;: 0.828, &#39;support&#39;: 250}, &#39;33&#39;: {&#39;f1-score&#39;: 0.9371428571428572, &#39;precision&#39;: 0.8945454545454545, &#39;recall&#39;: 0.984, &#39;support&#39;: 250}, &#39;34&#39;: {&#39;f1-score&#39;: 0.7238805970149255, &#39;precision&#39;: 0.6783216783216783, &#39;recall&#39;: 0.776, &#39;support&#39;: 250}, &#39;35&#39;: {&#39;f1-score&#39;: 0.715835140997831, &#39;precision&#39;: 0.7819905213270142, &#39;recall&#39;: 0.66, &#39;support&#39;: 250}, &#39;36&#39;: {&#39;f1-score&#39;: 0.5475728155339805, &#39;precision&#39;: 0.5320754716981132, &#39;recall&#39;: 0.564, &#39;support&#39;: 250}, &#39;37&#39;: {&#39;f1-score&#39;: 0.3870056497175141, &#39;precision&#39;: 0.29912663755458513, &#39;recall&#39;: 0.548, &#39;support&#39;: 250}, &#39;38&#39;: {&#39;f1-score&#39;: 0.6946902654867257, &#39;precision&#39;: 0.7772277227722773, &#39;recall&#39;: 0.628, &#39;support&#39;: 250}, &#39;39&#39;: {&#39;f1-score&#39;: 0.29749103942652333, &#39;precision&#39;: 0.2694805194805195, &#39;recall&#39;: 0.332, &#39;support&#39;: 250}, &#39;4&#39;: {&#39;f1-score&#39;: 0.544080604534005, &#39;precision&#39;: 0.7346938775510204, &#39;recall&#39;: 0.432, &#39;support&#39;: 250}, &#39;40&#39;: {&#39;f1-score&#39;: 0.7622641509433963, &#39;precision&#39;: 0.7214285714285714, &#39;recall&#39;: 0.808, &#39;support&#39;: 250}, &#39;41&#39;: {&#39;f1-score&#39;: 0.7029478458049886, &#39;precision&#39;: 0.8115183246073299, &#39;recall&#39;: 0.62, &#39;support&#39;: 250}, &#39;42&#39;: {&#39;f1-score&#39;: 0.537037037037037, &#39;precision&#39;: 0.5, &#39;recall&#39;: 0.58, &#39;support&#39;: 250}, &#39;43&#39;: {&#39;f1-score&#39;: 0.6651884700665188, &#39;precision&#39;: 0.746268656716418, &#39;recall&#39;: 0.6, &#39;support&#39;: 250}, &#39;44&#39;: {&#39;f1-score&#39;: 0.5586034912718205, &#39;precision&#39;: 0.7417218543046358, &#39;recall&#39;: 0.448, &#39;support&#39;: 250}, &#39;45&#39;: {&#39;f1-score&#39;: 0.8114285714285714, &#39;precision&#39;: 0.7745454545454545, &#39;recall&#39;: 0.852, &#39;support&#39;: 250}, &#39;46&#39;: {&#39;f1-score&#39;: 0.5831202046035805, &#39;precision&#39;: 0.8085106382978723, &#39;recall&#39;: 0.456, &#39;support&#39;: 250}, &#39;47&#39;: {&#39;f1-score&#39;: 0.4641509433962264, &#39;precision&#39;: 0.4392857142857143, &#39;recall&#39;: 0.492, &#39;support&#39;: 250}, &#39;48&#39;: {&#39;f1-score&#39;: 0.577524893314367, &#39;precision&#39;: 0.4481236203090508, &#39;recall&#39;: 0.812, &#39;support&#39;: 250}, &#39;49&#39;: {&#39;f1-score&#39;: 0.47234042553191485, &#39;precision&#39;: 0.5045454545454545, &#39;recall&#39;: 0.444, &#39;support&#39;: 250}, &#39;5&#39;: {&#39;f1-score&#39;: 0.41860465116279066, &#39;precision&#39;: 0.34177215189873417, &#39;recall&#39;: 0.54, &#39;support&#39;: 250}, &#39;50&#39;: {&#39;f1-score&#39;: 0.45581395348837206, &#39;precision&#39;: 0.5444444444444444, &#39;recall&#39;: 0.392, &#39;support&#39;: 250}, &#39;51&#39;: {&#39;f1-score&#39;: 0.7783783783783783, &#39;precision&#39;: 0.7081967213114754, &#39;recall&#39;: 0.864, &#39;support&#39;: 250}, &#39;52&#39;: {&#39;f1-score&#39;: 0.6124401913875598, &#39;precision&#39;: 0.5092838196286472, &#39;recall&#39;: 0.768, &#39;support&#39;: 250}, &#39;53&#39;: {&#39;f1-score&#39;: 0.6759443339960238, &#39;precision&#39;: 0.6719367588932806, &#39;recall&#39;: 0.68, &#39;support&#39;: 250}, &#39;54&#39;: {&#39;f1-score&#39;: 0.8103448275862069, &#39;precision&#39;: 0.8785046728971962, &#39;recall&#39;: 0.752, &#39;support&#39;: 250}, &#39;55&#39;: {&#39;f1-score&#39;: 0.7644444444444444, &#39;precision&#39;: 0.86, &#39;recall&#39;: 0.688, &#39;support&#39;: 250}, &#39;56&#39;: {&#39;f1-score&#39;: 0.3398328690807799, &#39;precision&#39;: 0.5596330275229358, &#39;recall&#39;: 0.244, &#39;support&#39;: 250}, &#39;57&#39;: {&#39;f1-score&#39;: 0.5209302325581396, &#39;precision&#39;: 0.6222222222222222, &#39;recall&#39;: 0.448, &#39;support&#39;: 250}, &#39;58&#39;: {&#39;f1-score&#39;: 0.6233766233766233, &#39;precision&#39;: 0.6792452830188679, &#39;recall&#39;: 0.576, &#39;support&#39;: 250}, &#39;59&#39;: {&#39;f1-score&#39;: 0.486910994764398, &#39;precision&#39;: 0.7045454545454546, &#39;recall&#39;: 0.372, &#39;support&#39;: 250}, &#39;6&#39;: {&#39;f1-score&#39;: 0.7229357798165138, &#39;precision&#39;: 0.6677966101694915, &#39;recall&#39;: 0.788, &#39;support&#39;: 250}, &#39;60&#39;: {&#39;f1-score&#39;: 0.6885245901639344, &#39;precision&#39;: 0.8305084745762712, &#39;recall&#39;: 0.588, &#39;support&#39;: 250}, &#39;61&#39;: {&#39;f1-score&#39;: 0.6495176848874598, &#39;precision&#39;: 0.543010752688172, &#39;recall&#39;: 0.808, &#39;support&#39;: 250}, &#39;62&#39;: {&#39;f1-score&#39;: 0.5823389021479712, &#39;precision&#39;: 0.7218934911242604, &#39;recall&#39;: 0.488, &#39;support&#39;: 250}, &#39;63&#39;: {&#39;f1-score&#39;: 0.895397489539749, &#39;precision&#39;: 0.9385964912280702, &#39;recall&#39;: 0.856, &#39;support&#39;: 250}, &#39;64&#39;: {&#39;f1-score&#39;: 0.8129770992366412, &#39;precision&#39;: 0.7773722627737226, &#39;recall&#39;: 0.852, &#39;support&#39;: 250}, &#39;65&#39;: {&#39;f1-score&#39;: 0.82, &#39;precision&#39;: 0.82, &#39;recall&#39;: 0.82, &#39;support&#39;: 250}, &#39;66&#39;: {&#39;f1-score&#39;: 0.44141689373297005, &#39;precision&#39;: 0.6923076923076923, &#39;recall&#39;: 0.324, &#39;support&#39;: 250}, &#39;67&#39;: {&#39;f1-score&#39;: 0.47840531561461797, &#39;precision&#39;: 0.4090909090909091, &#39;recall&#39;: 0.576, &#39;support&#39;: 250}, &#39;68&#39;: {&#39;f1-score&#39;: 0.832618025751073, &#39;precision&#39;: 0.8981481481481481, &#39;recall&#39;: 0.776, &#39;support&#39;: 250}, &#39;69&#39;: {&#39;f1-score&#39;: 0.8340080971659919, &#39;precision&#39;: 0.8442622950819673, &#39;recall&#39;: 0.824, &#39;support&#39;: 250}, &#39;7&#39;: {&#39;f1-score&#39;: 0.7908902691511386, &#39;precision&#39;: 0.8197424892703863, &#39;recall&#39;: 0.764, &#39;support&#39;: 250}, &#39;70&#39;: {&#39;f1-score&#39;: 0.7101200686106347, &#39;precision&#39;: 0.6216216216216216, &#39;recall&#39;: 0.828, &#39;support&#39;: 250}, &#39;71&#39;: {&#39;f1-score&#39;: 0.5903307888040712, &#39;precision&#39;: 0.8111888111888111, &#39;recall&#39;: 0.464, &#39;support&#39;: 250}, &#39;72&#39;: {&#39;f1-score&#39;: 0.6468253968253969, &#39;precision&#39;: 0.6417322834645669, &#39;recall&#39;: 0.652, &#39;support&#39;: 250}, &#39;73&#39;: {&#39;f1-score&#39;: 0.4743589743589744, &#39;precision&#39;: 0.5091743119266054, &#39;recall&#39;: 0.444, &#39;support&#39;: 250}, &#39;74&#39;: {&#39;f1-score&#39;: 0.658008658008658, &#39;precision&#39;: 0.7169811320754716, &#39;recall&#39;: 0.608, &#39;support&#39;: 250}, &#39;75&#39;: {&#39;f1-score&#39;: 0.8665377176015473, &#39;precision&#39;: 0.8389513108614233, &#39;recall&#39;: 0.896, &#39;support&#39;: 250}, &#39;76&#39;: {&#39;f1-score&#39;: 0.7808764940239045, &#39;precision&#39;: 0.7777777777777778, &#39;recall&#39;: 0.784, &#39;support&#39;: 250}, &#39;77&#39;: {&#39;f1-score&#39;: 0.30875576036866365, &#39;precision&#39;: 0.3641304347826087, &#39;recall&#39;: 0.268, &#39;support&#39;: 250}, &#39;78&#39;: {&#39;f1-score&#39;: 0.7603305785123966, &#39;precision&#39;: 0.7863247863247863, &#39;recall&#39;: 0.736, &#39;support&#39;: 250}, &#39;79&#39;: {&#39;f1-score&#39;: 0.571830985915493, &#39;precision&#39;: 0.44130434782608696, &#39;recall&#39;: 0.812, &#39;support&#39;: 250}, &#39;8&#39;: {&#39;f1-score&#39;: 0.3866943866943867, &#39;precision&#39;: 0.4025974025974026, &#39;recall&#39;: 0.372, &#39;support&#39;: 250}, &#39;80&#39;: {&#39;f1-score&#39;: 0.5870841487279843, &#39;precision&#39;: 0.5747126436781609, &#39;recall&#39;: 0.6, &#39;support&#39;: 250}, &#39;81&#39;: {&#39;f1-score&#39;: 0.6756756756756757, &#39;precision&#39;: 0.6529850746268657, &#39;recall&#39;: 0.7, &#39;support&#39;: 250}, &#39;82&#39;: {&#39;f1-score&#39;: 0.34285714285714286, &#39;precision&#39;: 0.3804878048780488, &#39;recall&#39;: 0.312, &#39;support&#39;: 250}, &#39;83&#39;: {&#39;f1-score&#39;: 0.6711409395973154, &#39;precision&#39;: 0.5780346820809249, &#39;recall&#39;: 0.8, &#39;support&#39;: 250}, &#39;84&#39;: {&#39;f1-score&#39;: 0.4653465346534653, &#39;precision&#39;: 0.6103896103896104, &#39;recall&#39;: 0.376, &#39;support&#39;: 250}, &#39;85&#39;: {&#39;f1-score&#39;: 0.5525525525525525, &#39;precision&#39;: 0.4423076923076923, &#39;recall&#39;: 0.736, &#39;support&#39;: 250}, &#39;86&#39;: {&#39;f1-score&#39;: 0.7783783783783783, &#39;precision&#39;: 0.7081967213114754, &#39;recall&#39;: 0.864, &#39;support&#39;: 250}, &#39;87&#39;: {&#39;f1-score&#39;: 0.3975409836065574, &#39;precision&#39;: 0.40756302521008403, &#39;recall&#39;: 0.388, &#39;support&#39;: 250}, &#39;88&#39;: {&#39;f1-score&#39;: 0.8130081300813008, &#39;precision&#39;: 0.8264462809917356, &#39;recall&#39;: 0.8, &#39;support&#39;: 250}, &#39;89&#39;: {&#39;f1-score&#39;: 0.4301675977653631, &#39;precision&#39;: 0.7129629629629629, &#39;recall&#39;: 0.308, &#39;support&#39;: 250}, &#39;9&#39;: {&#39;f1-score&#39;: 0.5117370892018779, &#39;precision&#39;: 0.6193181818181818, &#39;recall&#39;: 0.436, &#39;support&#39;: 250}, &#39;90&#39;: {&#39;f1-score&#39;: 0.7881548974943051, &#39;precision&#39;: 0.9153439153439153, &#39;recall&#39;: 0.692, &#39;support&#39;: 250}, &#39;91&#39;: {&#39;f1-score&#39;: 0.84765625, &#39;precision&#39;: 0.8282442748091603, &#39;recall&#39;: 0.868, &#39;support&#39;: 250}, &#39;92&#39;: {&#39;f1-score&#39;: 0.6652977412731006, &#39;precision&#39;: 0.6835443037974683, &#39;recall&#39;: 0.648, &#39;support&#39;: 250}, &#39;93&#39;: {&#39;f1-score&#39;: 0.34234234234234234, &#39;precision&#39;: 0.3114754098360656, &#39;recall&#39;: 0.38, &#39;support&#39;: 250}, &#39;94&#39;: {&#39;f1-score&#39;: 0.5714285714285714, &#39;precision&#39;: 0.6118721461187214, &#39;recall&#39;: 0.536, &#39;support&#39;: 250}, &#39;95&#39;: {&#39;f1-score&#39;: 0.6710526315789473, &#39;precision&#39;: 0.7427184466019418, &#39;recall&#39;: 0.612, &#39;support&#39;: 250}, &#39;96&#39;: {&#39;f1-score&#39;: 0.3809523809523809, &#39;precision&#39;: 0.5625, &#39;recall&#39;: 0.288, &#39;support&#39;: 250}, &#39;97&#39;: {&#39;f1-score&#39;: 0.5644916540212443, &#39;precision&#39;: 0.4547677261613692, &#39;recall&#39;: 0.744, &#39;support&#39;: 250}, &#39;98&#39;: {&#39;f1-score&#39;: 0.3858823529411765, &#39;precision&#39;: 0.4685714285714286, &#39;recall&#39;: 0.328, &#39;support&#39;: 250}, &#39;99&#39;: {&#39;f1-score&#39;: 0.35356200527704484, &#39;precision&#39;: 0.5193798449612403, &#39;recall&#39;: 0.268, &#39;support&#39;: 250}, &#39;accuracy&#39;: 0.6077623762376237, &#39;macro avg&#39;: {&#39;f1-score&#39;: 0.6061252197245781, &#39;precision&#39;: 0.6328666845830312, &#39;recall&#39;: 0.6077623762376237, &#39;support&#39;: 25250}, &#39;weighted avg&#39;: {&#39;f1-score&#39;: 0.606125219724578, &#39;precision&#39;: 0.6328666845830311, &#39;recall&#39;: 0.6077623762376237, &#39;support&#39;: 25250}} . . Let&#39;s plot all our classes F1-Score . classification_report_dict[&quot;99&quot;][&quot;f1-score&quot;] . 0.35356200527704484 . # Create empty dictionary class_f1_scores = {} # Loop through classification report dictionary items for k, v in classification_report_dict.items(): if k == &quot;accuracy&quot;: # Stop once we get the accuracy key break else: #Add class names and f1-scores to new dictionary class_f1_scores[class_names[int(k)]] = v[&quot;f1-score&quot;] class_f1_scores . {&#39;apple_pie&#39;: 0.24056603773584903, &#39;baby_back_ribs&#39;: 0.5864406779661017, &#39;baklava&#39;: 0.6022304832713754, &#39;beef_carpaccio&#39;: 0.616822429906542, &#39;beef_tartare&#39;: 0.544080604534005, &#39;beet_salad&#39;: 0.41860465116279066, &#39;beignets&#39;: 0.7229357798165138, &#39;bibimbap&#39;: 0.7908902691511386, &#39;bread_pudding&#39;: 0.3866943866943867, &#39;breakfast_burrito&#39;: 0.5117370892018779, &#39;bruschetta&#39;: 0.5047619047619047, &#39;caesar_salad&#39;: 0.6161616161616161, &#39;cannoli&#39;: 0.6105610561056106, &#39;caprese_salad&#39;: 0.5775193798449612, &#39;carrot_cake&#39;: 0.574757281553398, &#39;ceviche&#39;: 0.36744186046511623, &#39;cheese_plate&#39;: 0.5654135338345864, &#39;cheesecake&#39;: 0.42546063651591287, &#39;chicken_curry&#39;: 0.5008403361344538, &#39;chicken_quesadilla&#39;: 0.6411889596602972, &#39;chicken_wings&#39;: 0.7123809523809523, &#39;chocolate_cake&#39;: 0.45261669024045265, &#39;chocolate_mousse&#39;: 0.3291592128801431, &#39;churros&#39;: 0.7134935304990757, &#39;clam_chowder&#39;: 0.7708779443254817, &#39;club_sandwich&#39;: 0.734020618556701, &#39;crab_cakes&#39;: 0.4625550660792952, &#39;creme_brulee&#39;: 0.7494824016563146, &#39;croque_madame&#39;: 0.6935483870967742, &#39;cup_cakes&#39;: 0.6910569105691057, &#39;deviled_eggs&#39;: 0.7476190476190476, &#39;donuts&#39;: 0.7357293868921776, &#39;dumplings&#39;: 0.7855787476280836, &#39;edamame&#39;: 0.9371428571428572, &#39;eggs_benedict&#39;: 0.7238805970149255, &#39;escargots&#39;: 0.715835140997831, &#39;falafel&#39;: 0.5475728155339805, &#39;filet_mignon&#39;: 0.3870056497175141, &#39;fish_and_chips&#39;: 0.6946902654867257, &#39;foie_gras&#39;: 0.29749103942652333, &#39;french_fries&#39;: 0.7622641509433963, &#39;french_onion_soup&#39;: 0.7029478458049886, &#39;french_toast&#39;: 0.537037037037037, &#39;fried_calamari&#39;: 0.6651884700665188, &#39;fried_rice&#39;: 0.5586034912718205, &#39;frozen_yogurt&#39;: 0.8114285714285714, &#39;garlic_bread&#39;: 0.5831202046035805, &#39;gnocchi&#39;: 0.4641509433962264, &#39;greek_salad&#39;: 0.577524893314367, &#39;grilled_cheese_sandwich&#39;: 0.47234042553191485, &#39;grilled_salmon&#39;: 0.45581395348837206, &#39;guacamole&#39;: 0.7783783783783783, &#39;gyoza&#39;: 0.6124401913875598, &#39;hamburger&#39;: 0.6759443339960238, &#39;hot_and_sour_soup&#39;: 0.8103448275862069, &#39;hot_dog&#39;: 0.7644444444444444, &#39;huevos_rancheros&#39;: 0.3398328690807799, &#39;hummus&#39;: 0.5209302325581396, &#39;ice_cream&#39;: 0.6233766233766233, &#39;lasagna&#39;: 0.486910994764398, &#39;lobster_bisque&#39;: 0.6885245901639344, &#39;lobster_roll_sandwich&#39;: 0.6495176848874598, &#39;macaroni_and_cheese&#39;: 0.5823389021479712, &#39;macarons&#39;: 0.895397489539749, &#39;miso_soup&#39;: 0.8129770992366412, &#39;mussels&#39;: 0.82, &#39;nachos&#39;: 0.44141689373297005, &#39;omelette&#39;: 0.47840531561461797, &#39;onion_rings&#39;: 0.832618025751073, &#39;oysters&#39;: 0.8340080971659919, &#39;pad_thai&#39;: 0.7101200686106347, &#39;paella&#39;: 0.5903307888040712, &#39;pancakes&#39;: 0.6468253968253969, &#39;panna_cotta&#39;: 0.4743589743589744, &#39;peking_duck&#39;: 0.658008658008658, &#39;pho&#39;: 0.8665377176015473, &#39;pizza&#39;: 0.7808764940239045, &#39;pork_chop&#39;: 0.30875576036866365, &#39;poutine&#39;: 0.7603305785123966, &#39;prime_rib&#39;: 0.571830985915493, &#39;pulled_pork_sandwich&#39;: 0.5870841487279843, &#39;ramen&#39;: 0.6756756756756757, &#39;ravioli&#39;: 0.34285714285714286, &#39;red_velvet_cake&#39;: 0.6711409395973154, &#39;risotto&#39;: 0.4653465346534653, &#39;samosa&#39;: 0.5525525525525525, &#39;sashimi&#39;: 0.7783783783783783, &#39;scallops&#39;: 0.3975409836065574, &#39;seaweed_salad&#39;: 0.8130081300813008, &#39;shrimp_and_grits&#39;: 0.4301675977653631, &#39;spaghetti_bolognese&#39;: 0.7881548974943051, &#39;spaghetti_carbonara&#39;: 0.84765625, &#39;spring_rolls&#39;: 0.6652977412731006, &#39;steak&#39;: 0.34234234234234234, &#39;strawberry_shortcake&#39;: 0.5714285714285714, &#39;sushi&#39;: 0.6710526315789473, &#39;tacos&#39;: 0.3809523809523809, &#39;takoyaki&#39;: 0.5644916540212443, &#39;tiramisu&#39;: 0.3858823529411765, &#39;tuna_tartare&#39;: 0.35356200527704484, &#39;waffles&#39;: 0.641025641025641} . . # Turn f1-scores into dataframe for visualization import pandas as pd f1_scores = pd.DataFrame({&quot;class_names&quot;: list(class_f1_scores.keys()), &quot;f1-score&quot;: list(class_f1_scores.values())}).sort_values(&quot;f1-score&quot;, ascending = False) . f1_scores . class_names f1-score . 33 edamame | 0.937143 | . 63 macarons | 0.895397 | . 75 pho | 0.866538 | . 91 spaghetti_carbonara | 0.847656 | . 69 oysters | 0.834008 | . ... ... | ... | . 56 huevos_rancheros | 0.339833 | . 22 chocolate_mousse | 0.329159 | . 77 pork_chop | 0.308756 | . 39 foie_gras | 0.297491 | . 0 apple_pie | 0.240566 | . 101 rows × 2 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(12, 25)) scores = ax.barh(range(len(f1_scores)), f1_scores[&quot;f1-score&quot;].values) ax.set_yticks(range(len(f1_scores))) ax.set_yticklabels(list(f1_scores[&quot;class_names&quot;])) ax.set_xlabel(&quot;f1-score&quot;) ax.set_title(&quot;F1-Scores for 10 Different Classes&quot;) ax.invert_yaxis(); # reverse the order def autolabel(rects): # Modified version of: https://matplotlib.org/examples/api/barchart_demo.html &quot;&quot;&quot; Attach a text label above each bar displaying its height (it&#39;s value). &quot;&quot;&quot; for rect in rects: width = rect.get_width() ax.text(1.03*width, rect.get_y() + rect.get_height()/1.5, f&quot;{width:.2f}&quot;, ha=&#39;center&#39;, va=&#39;bottom&#39;) autolabel(scores) . Visualizing predictions on custom images . How does our model go on food images not even in our test dataset . To visualize our model&#39;s predictions on our images we&#39;ll need a function to load and preprocess images, specifically it will need to: . Read in a target image file path suing tf.io.read_file() | Turn the image into a Tensor using tf.io.decode_image() | Resize the image tensor to be the same size as the images our model has trained on using tf.image.resize() | Scale the image to get all of the pixel values between 0 &amp; 1 (if necessary) | . def load_and_prep_image(filename, img_shape = 224, scale=True): &quot;&quot;&quot; Reads in an image from filename, turns it into a tensor and reshapes intoo specified shape (img_shape, color_channels = 3) Args: filename (str) : path to target image image_shape (int) : height/width dimension of target image size scale(bool) : scale pixel values from 0-255 to 0-1 or not &quot;&quot;&quot; # Read in the image img = tf.io.read_file(filename) # Decode the image into tensor img = tf.io.decode_image(img, channels = 3) # Resize the image img = tf.image.resize(img,[img_shape, img_shape]) # Scale(yes/no) if scale: # rescale the image (get all values between 0 and 1) return img/255. else: return img # Don&#39;t need to rescale images for EfficientNetB0 . Now, we got function to load and prepare target images, let&#39;s now write some code to visualize images, their target lanel and our model&#39;s predictions. . Specifically, we&#39;ll write some code to: . Load a few random images from the test dataset | Make predictions on the loaded images | Plot the original image(s) along with the model&#39;s predictions, prediction probabilty and ground truth labels | import os import random plt.figure(figsize=(17, 10)) for i in range(3): # Choose a random image from a random class class_name = random.choice(class_names) filename = random.choice(os.listdir(test_dir + &quot;/&quot; + class_name)) filepath = test_dir + class_name + &quot;/&quot; + filename # Load the image and make predictions img = load_and_prep_image(filepath, scale=False) # don&#39;t scale images for EfficientNet predictions pred_prob = model.predict(tf.expand_dims(img, axis=0)) # model accepts tensors of shape [None, 224, 224, 3] pred_class = class_names[pred_prob.argmax()] # find the predicted class # Plot the image(s) plt.subplot(1, 3, i+1) plt.imshow(img/255.) if class_name == pred_class: # Change the color of text based on whether prediction is right or wrong title_color = &quot;g&quot; else: title_color = &quot;r&quot; plt.title(f&quot;actual: {class_name}, pred: {pred_class}, prob: {pred_prob.max():.2f}&quot;, c=title_color) plt.axis(False); . Finding the most wrong predictions . A good way to inspect your model&#39;s performance is to view the wrong predictions with the highest prediction probability (or highest loss) | Can reveal insights such as: Data issues (wrong labels) | Confusing classes(get better/more diverse data) | . | . To find out where our model is most wrong, do the following: . Get all of the image file paths in the test datasets using list_files() method | Create a pandas DataFrame of the image filepaths, ground truth labels, predicted classes(from our model), max prediction probabilities, prediction_classnames and ground truth labels. | Use our DataFrame based on wrong predictions (where the ground truth label doesn&#39;t match the prediction) | Sort the DataFrame based on wrong predictions (have the highest prediction probability at the top) | Visualize the images with the highest prediction probabilites but have the wrong prediction. | filepaths = [] for filepath in test_data.list_files(&quot;101_food_classes_10_percent/test/*/*.jpg&quot;, shuffle=False): filepaths.append(filepath.numpy()) filepaths[:10] . [b&#39;101_food_classes_10_percent/test/apple_pie/1011328.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/101251.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/1034399.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/103801.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/1038694.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/1047447.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/1068632.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/110043.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/1106961.jpg&#39;, b&#39;101_food_classes_10_percent/test/apple_pie/1113017.jpg&#39;] . import pandas as pd pred_df = pd.DataFrame({&quot;img_path&quot;: filepaths, &quot;y_true&quot;: y_labels, &quot;y_pred&quot;: pred_classes, &quot;pred_conf&quot;: preds_probs.max(axis=1), &quot;y_true_classname&quot;: [class_names[i] for i in y_labels], &quot;y_pred_classname&quot;: [class_names[i] for i in pred_classes]}) # get the maximum prediction prob value pred_df . img_path y_true y_pred pred_conf y_true_classname y_pred_classname . 0 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 52 | 0.847419 | apple_pie | gyoza | . 1 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 0 | 0.964017 | apple_pie | apple_pie | . 2 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 0 | 0.959259 | apple_pie | apple_pie | . 3 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 80 | 0.658607 | apple_pie | pulled_pork_sandwich | . 4 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 79 | 0.367902 | apple_pie | prime_rib | . ... ... | ... | ... | ... | ... | ... | . 25245 b&#39;101_food_classes_10_percent/test/waffles/942... | 100 | 100 | 0.972823 | waffles | waffles | . 25246 b&#39;101_food_classes_10_percent/test/waffles/954... | 100 | 16 | 0.878027 | waffles | cheese_plate | . 25247 b&#39;101_food_classes_10_percent/test/waffles/961... | 100 | 100 | 0.537900 | waffles | waffles | . 25248 b&#39;101_food_classes_10_percent/test/waffles/970... | 100 | 94 | 0.501951 | waffles | strawberry_shortcake | . 25249 b&#39;101_food_classes_10_percent/test/waffles/971... | 100 | 100 | 0.690628 | waffles | waffles | . 25250 rows × 6 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pred_df[&quot;pred_correct&quot;] = pred_df[&quot;y_true&quot;] == pred_df[&quot;y_pred&quot;] pred_df.head() . img_path y_true y_pred pred_conf y_true_classname y_pred_classname pred_correct . 0 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 52 | 0.847419 | apple_pie | gyoza | False | . 1 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 0 | 0.964017 | apple_pie | apple_pie | True | . 2 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 0 | 0.959259 | apple_pie | apple_pie | True | . 3 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 80 | 0.658607 | apple_pie | pulled_pork_sandwich | False | . 4 b&#39;101_food_classes_10_percent/test/apple_pie/1... | 0 | 79 | 0.367902 | apple_pie | prime_rib | False | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; top_100_wrong = pred_df[pred_df[&quot;pred_correct&quot;] == False].sort_values(&quot;pred_conf&quot;, ascending = False)[:100] top_100_wrong . img_path y_true y_pred pred_conf y_true_classname y_pred_classname pred_correct . 21810 b&#39;101_food_classes_10_percent/test/scallops/17... | 87 | 29 | 0.999997 | scallops | cup_cakes | False | . 231 b&#39;101_food_classes_10_percent/test/apple_pie/8... | 0 | 100 | 0.999995 | apple_pie | waffles | False | . 15359 b&#39;101_food_classes_10_percent/test/lobster_rol... | 61 | 53 | 0.999988 | lobster_roll_sandwich | hamburger | False | . 23539 b&#39;101_food_classes_10_percent/test/strawberry_... | 94 | 83 | 0.999987 | strawberry_shortcake | red_velvet_cake | False | . 21400 b&#39;101_food_classes_10_percent/test/samosa/3140... | 85 | 92 | 0.999981 | samosa | spring_rolls | False | . ... ... | ... | ... | ... | ... | ... | ... | . 8763 b&#39;101_food_classes_10_percent/test/escargots/1... | 35 | 41 | 0.997169 | escargots | french_onion_soup | False | . 2663 b&#39;101_food_classes_10_percent/test/bruschetta/... | 10 | 61 | 0.997055 | bruschetta | lobster_roll_sandwich | False | . 7924 b&#39;101_food_classes_10_percent/test/donuts/3454... | 31 | 29 | 0.997020 | donuts | cup_cakes | False | . 18586 b&#39;101_food_classes_10_percent/test/peking_duck... | 74 | 39 | 0.996885 | peking_duck | foie_gras | False | . 3519 b&#39;101_food_classes_10_percent/test/carrot_cake... | 14 | 21 | 0.996842 | carrot_cake | chocolate_cake | False | . 100 rows × 7 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; images_to_view = 9 start_index = 0 # change the index to view more of the wrong predictions plt.figure(figsize=(15,10)) for i, row in enumerate(top_100_wrong[start_index:start_index+9].itertuples()): plt.subplot(3,3,i+1) img = load_and_prep_image(row[1], scale = False) _, _, _, _, pred_prob, y_true_classname, y_pred_classname, _ = row # only interested in a few parameters of each row plt.imshow(img/255.) plt.title(f&quot;actual: {y_true_classname}, pred: {y_pred_classname}, n prob: {pred_prob}&quot;) plt.axis(False) . These are the images where our model is predicting the image with high probability even when the prediction is not correct according to ground truth labels . Test our model on custom images . from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys !wget https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip unzip_data(&quot;custom_food_images.zip&quot;) . --2022-02-20 18:37:33-- https://storage.googleapis.com/ztm_tf_course/food_vision/custom_food_images.zip Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.216.128, 173.194.217.128, 173.194.218.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.216.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 13192985 (13M) [application/zip] Saving to: ‘custom_food_images.zip.2’ custom_food_images. 100%[===================&gt;] 12.58M --.-KB/s in 0.07s 2022-02-20 18:37:33 (174 MB/s) - ‘custom_food_images.zip.2’ saved [13192985/13192985] . import os custom_food_images = [&quot;custom_food_images/&quot; + img_path for img_path in os.listdir(&quot;custom_food_images&quot;)] custom_food_images . [&#39;custom_food_images/chicken_wings.jpeg&#39;, &#39;custom_food_images/hamburger.jpeg&#39;, &#39;custom_food_images/steak.jpeg&#39;, &#39;custom_food_images/ramen.jpeg&#39;, &#39;custom_food_images/pizza-dad.jpeg&#39;, &#39;custom_food_images/sushi.jpeg&#39;] . for img in custom_food_images: img = load_and_prep_image(img, scale=False) # load in target image and turn it into tensor pred_prob = model.predict(tf.expand_dims(img, axis=0)) # make prediction on image with shape [None, 224, 224, 3] pred_class = class_names[pred_prob.argmax()] # find the predicted class label # Plot the image with appropriate annotations plt.figure() plt.imshow(img/255.) # imshow() requires float inputs to be normalized plt.title(f&quot;pred: {pred_class}, prob: {pred_prob.max():.2f}&quot;) plt.axis(False) . References: . TensorFlow Developer Certificate in 2022: Zero to Mastery | Zero to Mastery Deep Learning with TensorFlow GitHub Repo *TensorFlow Documentation | Scikit-Learn Documentation | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/tensorflow/transfer-learning/large_scale_ml/2022/02/20/Transfer-learning-in-TensorFlow-Scaling-up.html",
            "relUrl": "/deep%20learning/neuralnetworks/tensorflow/transfer-learning/large_scale_ml/2022/02/20/Transfer-learning-in-TensorFlow-Scaling-up.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Transfer Learning with TensorFlow : Fine Tuning",
            "content": "Transfer Learning in TensorFlow : Fine Tuning . This Notebook is an account of my working for the Udemy course :TensorFlow Developer Certificate in 2022: Zero to Mastery. . Concepts covered in this Notebook: . Introduce fine-tuning transfer learning with TensorFlow. | Introduce the Keras Functional API to build models | Using a small dataset to experiment faster(e.g. 10% of training samples) | Data augmentation (making your training set more diverse without adding samples) | Running a series of experiments on our Food Vision data | Introduce the ModelCheckpoint callback to save intermediate training results. | . Create helper functions . In previous notebooks, we have created some helper functions for reusing them while evaluating and visualizing the results of our models. . So, it&#39;s a good idea to put functions you&#39;ll want to use again in a script you can download and import into your notebooks. . Below we download a file that contains all the functions we have created to help us during the model training in previous notebooks. . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py . --2022-02-20 05:18:46-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 10246 (10K) [text/plain] Saving to: ‘helper_functions.py’ helper_functions.py 100%[===================&gt;] 10.01K --.-KB/s in 0s 2022-02-20 05:18:46 (90.7 MB/s) - ‘helper_functions.py’ saved [10246/10246] . from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir . Note: If you are running this notebook in google colab, when it times out colab will delete helper_functions.py, so you&#39;ll have to redownload it if you want access to your helper functions. . Get the Data . Let&#39;s see how we cna use the pretrained models within tf.keras.applications and apply them to our own problem (eg. recognizing images of foods) . !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip unzip_data(&quot;10_food_classes_10_percent.zip&quot;) . --2022-02-20 05:18:50-- https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.120.128, 74.125.70.128, 74.125.69.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.120.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 168546183 (161M) [application/zip] Saving to: ‘10_food_classes_10_percent.zip’ 10_food_classes_10_ 100%[===================&gt;] 160.74M 210MB/s in 0.8s 2022-02-20 05:18:50 (210 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183] . walk_through_dir(&quot;10_food_classes_10_percent&quot;) . There are 2 directories and 0 images in &#39;10_food_classes_10_percent&#39;. There are 10 directories and 0 images in &#39;10_food_classes_10_percent/train&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/pizza&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/fried_rice&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/grilled_salmon&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/hamburger&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/steak&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/sushi&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/chicken_wings&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/ramen&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/ice_cream&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/chicken_curry&#39;. There are 10 directories and 0 images in &#39;10_food_classes_10_percent/test&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/pizza&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/hamburger&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/steak&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/sushi&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/chicken_wings&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/ramen&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/ice_cream&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/chicken_curry&#39;. . train_dir = &quot;10_food_classes_10_percent/train&quot; test_dir = &quot;10_food_classes_10_percent/test&quot; . import tensorflow as tf IMG_SIZE = (224,224) BATCH_SIZE = 32 train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory= train_dir, batch_size= BATCH_SIZE, image_size = IMG_SIZE, label_mode = &quot;categorical&quot;) test_data = tf.keras.preprocessing.image_dataset_from_directory(directory= test_dir, batch_size = BATCH_SIZE, image_size = IMG_SIZE, label_mode = &quot;categorical&quot;) . Found 750 files belonging to 10 classes. Found 2500 files belonging to 10 classes. . train_data_10_percent . &lt;BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))&gt; . train_data_10_percent.class_names . [&#39;chicken_curry&#39;, &#39;chicken_wings&#39;, &#39;fried_rice&#39;, &#39;grilled_salmon&#39;, &#39;hamburger&#39;, &#39;ice_cream&#39;, &#39;pizza&#39;, &#39;ramen&#39;, &#39;steak&#39;, &#39;sushi&#39;] . # see an example of a batch data for images, labels in train_data_10_percent.take(1): print(images, labels) . tf.Tensor( [[[[2.49285721e+02 2.37285721e+02 2.15285721e+02] [2.49642853e+02 2.37642853e+02 2.15642853e+02] [2.50719391e+02 2.38505112e+02 2.17147964e+02] ... [1.02734596e+02 5.24540863e+01 2.82450047e+01] [8.28316269e+01 4.35459671e+01 2.50919056e+01] [7.25765381e+01 3.89490776e+01 2.45051785e+01]] [[2.51331635e+02 2.40331635e+02 2.20331635e+02] [2.50928574e+02 2.39928574e+02 2.19928574e+02] [2.49801025e+02 2.38801025e+02 2.20688782e+02] ... [1.19739761e+02 5.73265800e+01 1.01123981e+01] [9.37603836e+01 4.14696159e+01 2.19920874e+00] [1.05264969e+02 5.65507469e+01 2.07395535e+01]] [[2.53000000e+02 2.41642853e+02 2.25857147e+02] [2.52056122e+02 2.40698975e+02 2.24913269e+02] [2.51000000e+02 2.39642853e+02 2.23857147e+02] ... [1.62510101e+02 8.69336243e+01 1.48163013e+01] [1.48438828e+02 7.55816879e+01 1.06531439e+01] [1.38101807e+02 6.69589539e+01 6.32120275e+00]] ... [[7.52704544e+01 1.87092133e+01 4.41836119e+00] [6.75867310e+01 1.34285698e+01 1.38775051e+00] [6.25663338e+01 1.29489927e+01 2.63772011e+00] ... [2.46550980e+02 2.32785736e+02 2.00831650e+02] [2.42341843e+02 2.28127579e+02 1.99127579e+02] [2.44923477e+02 2.30494949e+02 2.02137741e+02]] [[8.22144012e+01 2.20051937e+01 8.40826416e+00] [7.04388046e+01 1.41581659e+01 1.02039850e+00] [7.18009491e+01 1.86274719e+01 6.84175634e+00] ... [2.44714294e+02 2.29714294e+02 1.98428650e+02] [2.43061234e+02 2.29061234e+02 2.02066345e+02] [2.45596985e+02 2.31596985e+02 2.04668427e+02]] [[8.27750092e+01 2.07750072e+01 7.15269184e+00] [7.02039642e+01 1.21325331e+01 1.83662802e-01] [6.33011475e+01 9.57664299e+00 3.06152552e-01] ... [2.43709213e+02 2.27494949e+02 2.01775589e+02] [2.45714233e+02 2.29714233e+02 2.05520432e+02] [2.41714355e+02 2.26714355e+02 2.04428711e+02]]] [[[8.49846954e+01 7.61122513e+01 1.20969391e+02] [8.11377563e+01 6.66122437e+01 1.29566330e+02] [9.43112259e+01 7.86581650e+01 1.51076523e+02] ... [5.30764809e+01 4.94948196e+01 8.37856064e+01] [4.62142220e+01 3.90968246e+01 6.73110962e+01] [3.42702446e+01 2.16987476e+01 4.63416405e+01]] [[9.47040863e+01 8.37040863e+01 1.26346939e+02] [8.83673477e+01 7.36479645e+01 1.28005096e+02] [9.88724518e+01 8.07449036e+01 1.37811218e+02] ... [4.72702789e+01 5.57601547e+01 8.97294388e+01] [3.77091446e+01 4.37090912e+01 6.94335403e+01] [2.97856102e+01 2.77141457e+01 4.93110542e+01]] [[1.02204079e+02 8.90714264e+01 1.32500000e+02] [9.37959213e+01 8.39846954e+01 1.31510208e+02] [1.43352051e+02 1.32132660e+02 1.78704086e+02] ... [5.03825264e+01 6.33825264e+01 9.60253220e+01] [3.89591103e+01 4.64284668e+01 7.19743423e+01] [2.87141819e+01 2.90100098e+01 5.00100098e+01]] ... [[1.46147980e+02 1.51362274e+02 1.55647919e+02] [1.52000015e+02 1.57214310e+02 1.59928543e+02] [1.55025513e+02 1.62811234e+02 1.62836685e+02] ... [9.81480789e+01 4.58470612e+01 1.52449875e+01] [9.55765076e+01 4.28622742e+01 1.48315954e+01] [9.27905884e+01 3.90763588e+01 1.56579075e+01]] [[1.48928589e+02 1.54000000e+02 1.50142822e+02] [1.50994873e+02 1.56994873e+02 1.53137695e+02] [1.56173477e+02 1.63484680e+02 1.55785660e+02] ... [9.74947815e+01 5.72245941e+01 1.85102692e+01] [8.15609360e+01 4.09233055e+01 5.18866348e+00] [7.57805328e+01 3.60664253e+01 3.78078032e+00]] [[1.51056107e+02 1.50770401e+02 1.57341812e+02] [1.54668365e+02 1.55025513e+02 1.60882629e+02] [1.60076508e+02 1.62637741e+02 1.64989761e+02] ... [8.08877945e+01 4.93215256e+01 2.98368626e+01] [8.68162308e+01 5.76733513e+01 5.10049820e+01] [7.26017303e+01 4.30455780e+01 4.75455475e+01]]] [[[6.42857194e-01 2.64285707e+00 1.64285719e+00] [1.00000000e+00 3.00000000e+00 2.00000000e+00] [1.00000000e+00 3.00000000e+00 2.00000000e+00] ... [1.68571205e+01 1.96428566e+01 8.00006580e+00] [5.78567505e+00 1.07856750e+01 3.78567505e+00] [2.71432066e+00 8.94390011e+00 4.25516176e+00]] [[0.00000000e+00 2.00000000e+00 1.00000000e+00] [1.00000000e+00 3.00000000e+00 2.00000000e+00] [7.85714149e-01 2.78571415e+00 1.78571415e+00] ... [1.51734447e+01 1.79591808e+01 6.33169317e+00] [5.79079103e+00 1.07907915e+01 3.79079103e+00] [4.31126165e+00 1.13112621e+01 4.31126165e+00]] [[0.00000000e+00 2.00000000e+00 2.14285612e-01] [9.43877757e-01 2.94387770e+00 4.28571224e-01] [1.68367237e-01 2.16836715e+00 3.82652849e-01] ... [1.41937780e+01 1.69795132e+01 5.55098629e+00] [4.14284420e+00 9.14284420e+00 2.14284396e+00] [2.64800143e+00 9.64800167e+00 2.64800143e+00]] ... [[1.12581635e+02 8.12245178e+01 5.58163023e+00] [1.07801003e+02 7.64438858e+01 2.65814614e+00] [1.05714256e+02 7.33571396e+01 2.57141972e+00] ... [0.00000000e+00 3.21426392e+00 0.00000000e+00] [1.00000000e+00 3.00000000e+00 2.00000000e+00] [3.71435547e+00 2.71435547e+00 7.71435547e+00]] [[1.12357140e+02 8.33571396e+01 3.49996519e+00] [1.05423447e+02 7.64234467e+01 2.80595243e-01] [1.02428551e+02 7.24285507e+01 1.27519876e-01] ... [0.00000000e+00 3.21426392e+00 0.00000000e+00] [1.00000000e+00 3.00000000e+00 2.00000000e+00] [3.71435547e+00 2.71435547e+00 7.71435547e+00]] [[1.08714294e+02 8.07142944e+01 0.00000000e+00] [1.03000008e+02 7.40000076e+01 0.00000000e+00] [1.01989799e+02 7.19897995e+01 0.00000000e+00] ... [0.00000000e+00 3.21426392e+00 0.00000000e+00] [1.00000000e+00 3.00000000e+00 2.00000000e+00] [3.71435547e+00 2.71435547e+00 7.71435547e+00]]] ... [[[1.02911827e+02 7.52979889e+01 6.88616037e+00] [9.25312500e+01 6.86674118e+01 4.71651840e+00] [7.62087021e+01 6.07410698e+01 3.38839245e+00] ... [3.58181877e+01 3.62467155e+01 1.22467136e+01] [3.08817253e+01 2.62734623e+01 4.14288330e+00] [3.29284668e+01 2.79284668e+01 5.92846680e+00]] [[1.06181923e+02 8.02968750e+01 1.42611609e+01] [9.68281250e+01 7.60546875e+01 1.37142868e+01] [7.93515625e+01 6.38716469e+01 9.06026745e+00] ... [3.33047485e+01 3.37332764e+01 9.73327446e+00] [2.84118767e+01 2.62879715e+01 3.32927275e+00] [3.20847168e+01 2.70847168e+01 5.08471680e+00]] [[1.02643974e+02 8.01953125e+01 1.80424099e+01] [9.70066986e+01 7.76383972e+01 1.86551342e+01] [7.95089264e+01 6.45357132e+01 1.18359365e+01] ... [2.74207649e+01 2.78492928e+01 3.84929323e+00] [2.51049500e+01 2.31049500e+01 7.57842541e-01] [2.85679512e+01 2.35679512e+01 2.07023239e+00]] ... [[3.96082611e+01 3.87176361e+01 5.60825920e+00] [4.47142868e+01 4.38236618e+01 1.01629477e+01] [4.66618309e+01 4.53191948e+01 1.11729908e+01] ... [2.18277878e+02 2.17277878e+02 1.73277878e+02] [2.15878326e+02 2.14878326e+02 1.70878326e+02] [2.11285645e+02 2.10285645e+02 1.64285645e+02]] [[3.85725441e+01 3.49944191e+01 5.56361628e+00] [4.35993309e+01 4.05580368e+01 8.85714340e+00] [4.63058052e+01 4.38839302e+01 7.14955378e+00] ... [2.18428528e+02 2.17428528e+02 1.73428528e+02] [2.15887253e+02 2.14887253e+02 1.70887253e+02] [2.11285645e+02 2.10285645e+02 1.64285645e+02]] [[3.83069191e+01 3.43069191e+01 7.73995495e+00] [4.03303604e+01 3.72589302e+01 6.40178585e+00] [4.23895111e+01 4.03895111e+01 2.20535755e+00] ... [2.17385025e+02 2.16385025e+02 1.72385025e+02] [2.14997742e+02 2.13997742e+02 1.69997742e+02] [2.10426270e+02 2.09426270e+02 1.63426270e+02]]] [[[1.51994904e+02 1.03994904e+02 1.89948978e+01] [1.46545914e+02 1.00545921e+02 1.25459175e+01] [1.49387756e+02 1.03387756e+02 1.53877535e+01] ... [9.32192535e+01 9.10815125e+01 1.53774986e+01] [8.15866089e+01 8.33927460e+01 7.09683895e+00] [8.09031296e+01 8.39031296e+01 3.36228728e+00]] [[1.58816330e+02 1.12816322e+02 2.48163261e+01] [1.54499985e+02 1.08500000e+02 2.04999981e+01] [1.56403046e+02 1.10617348e+02 2.19744854e+01] ... [1.09525360e+02 1.06183571e+02 2.68978958e+01] [9.51120453e+01 9.35253448e+01 1.62549019e+01] [8.02756348e+01 8.30359039e+01 7.09245980e-01]] [[1.57801025e+02 1.11801018e+02 2.38010197e+01] [1.58214279e+02 1.12428574e+02 2.37857132e+01] [1.60255096e+02 1.15346939e+02 2.43010178e+01] ... [1.33377197e+02 1.28162933e+02 4.65201378e+01] [1.09887840e+02 1.07245010e+02 2.56582451e+01] [9.57136917e+01 9.65556107e+01 9.87191296e+00]] ... [[1.22418358e+02 9.69898300e+01 1.52040968e+01] [1.09907974e+02 8.49079742e+01 2.90797639e+00] [9.82447433e+01 7.32447433e+01 0.00000000e+00] ... [2.00117538e+02 1.66454239e+02 3.30716248e+01] [2.06484726e+02 1.73683685e+02 4.03980370e+01] [2.07433884e+02 1.74433884e+02 4.30053635e+01]] [[9.43879929e+01 6.93879929e+01 3.21487457e-01] [1.10525581e+02 8.55255814e+01 2.18883681e+00] [1.08959282e+02 8.46021347e+01 1.97966623e+00] ... [1.99678497e+02 1.64035706e+02 3.62499733e+01] [2.06418442e+02 1.71418442e+02 4.36940002e+01] [2.12540665e+02 1.78540665e+02 5.25406609e+01]] [[1.28990067e+02 1.03990059e+02 1.99900608e+01] [1.21633408e+02 9.66334076e+01 1.26334066e+01] [1.24398750e+02 1.00041611e+02 1.56130352e+01] ... [2.05040634e+02 1.69397842e+02 4.36121063e+01] [2.08188904e+02 1.73188904e+02 4.71889000e+01] [2.14341690e+02 1.80341690e+02 5.63416862e+01]]] [[[4.09438782e+01 4.09438782e+01 7.76581650e+01] [3.85000000e+01 3.74744873e+01 6.96938782e+01] [3.62908173e+01 3.56479568e+01 6.34336739e+01] ... [7.64949493e+01 2.91377411e+01 2.53520050e+01] [7.20255127e+01 2.40255146e+01 2.00255146e+01] [7.41990280e+01 2.61990261e+01 2.21990261e+01]] [[4.10459213e+01 4.16428604e+01 7.39030609e+01] [3.80051003e+01 3.70051003e+01 6.81530609e+01] [3.73163261e+01 3.68877525e+01 6.20612221e+01] ... [7.31428528e+01 2.57856483e+01 2.16019936e+01] [7.18571167e+01 2.38571167e+01 1.98571167e+01] [6.81428528e+01 2.01428566e+01 1.61428566e+01]] [[4.25000000e+01 4.22193871e+01 7.07857132e+01] [3.81020393e+01 3.76581612e+01 6.33877563e+01] [3.55918350e+01 3.36377563e+01 5.51173477e+01] ... [7.39081726e+01 2.63571644e+01 2.09540634e+01] [7.63417969e+01 2.73417950e+01 2.23417950e+01] [7.53675003e+01 2.63675022e+01 2.13675022e+01]] ... [[2.34913239e+02 2.30479507e+02 2.10418289e+02] [2.36357010e+02 2.30229477e+02 2.08969208e+02] [2.35724625e+02 2.24892929e+02 2.05321487e+02] ... [6.78313370e+01 3.74028130e+01 2.59742851e+01] [7.17549973e+01 4.08979454e+01 2.96836796e+01] [6.80202942e+01 3.55917664e+01 2.43775024e+01]] [[2.28367554e+02 2.17081787e+02 1.95816498e+02] [2.40433746e+02 2.28010208e+02 2.06153091e+02] [2.35801147e+02 2.17530670e+02 1.97887848e+02] ... [5.86427917e+01 3.14846630e+01 2.01581287e+01] [6.47856750e+01 3.77856750e+01 2.87856750e+01] [6.65967789e+01 3.95967751e+01 3.05967751e+01]] [[2.11456894e+02 1.92385361e+02 1.70742538e+02] [1.98146255e+02 1.75197174e+02 1.54528839e+02] [1.94248489e+02 1.65952484e+02 1.47523941e+02] ... [6.87808914e+01 4.08574219e+01 2.81993027e+01] [5.90919418e+01 3.10919399e+01 2.00919399e+01] [6.14847527e+01 3.24847527e+01 2.44847546e+01]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor( [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(32, 10), dtype=float32) . . Modelling experiments we are going to run: . Experiment Data Preprocessing Model . Model 0(baseline) | 10 classes of Food101 data(random 10% training data only) | None | Feature Extractor: EfficientNetB0 (pre-trained on ImageNet, all layers frozen) with no top | . Model 1 | 10 classes of Food101 data(random 1% training dta only) | Random Flip, Rotation, Zoom, Height, Width datat augmentation | Same as Model 0 | . Model 2 | Same as Model 0 | Same as Model 1 | Same as Model 0 | . Model 3 | Same as Model 0 | Same as Model 1 | Fine tuning: Model 2 (EfficientNetB0 pre-trained on ImageNet) with top layer trained on custom data, top 10 layers unfrozen | . Model 4 | 10 classes of Food101 data(100% training data | Same as Model 1 | Same as Model 3 | . Keras Functional API: . # Creating a model with the Functional API inputs = tf.keras.layers.Input(shape = (28,28)) x = tf.keras.layers.Flatten()(inputs) x = tf.keras.layers.Dense(64, activation = &quot;relu&quot;)(x) x = tf.keras.layers.Dense(64, activation = &quot;relu&quot;)(x) outputs = tf.keras.layers.Dense(10, activation = &quot;softmax&quot;)(x) functional_model = tf.keras.Model(inputs,outputs,name = &quot;functional model&quot;) functional_model.compile( loss = tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics =[&quot;accuracy&quot;] ) functional_model.fit(X_train, y_train, batch_size = 32, epochs =5) . Model 0: Building a transfer learning model using the Keras Functional API . The Sequential API is straight-forward, it runs our layers in sequential order. . But the functional API give us more flexibility with our models. . base_model = tf.keras.applications.EfficientNetB0(include_top = False) # 2. Freeze the base model(so the underlying pre-trained patterns aren&#39;t updated) base_model.trainable = False # 3. Create inputs into our model inputs = tf.keras.layers.Input(shape=(224,224,3), name= &quot;input_layer&quot;) # 4. If using a model likeResNet50V2 you will need to normalize the inputs(you don&#39;t have to for EfficientNet(s)) # x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs) # 5. Pass the inputs to the base_model x = base_model(inputs) print(f&quot;Shape after passing inputs through base model: {x.shape}&quot;) # 6. Average pool the output of the base model(aggregate all the most important info., reduce no. of compuatations) x = tf.keras.layers.GlobalAveragePooling2D(name = &quot;global_average_pooling_layer&quot;)(x) print(f&quot;Shape after GlobalAveragePooling2D: {x.shape}&quot;) # 7. Create the output activation layer outputs = tf.keras.layers.Dense(10,activation = &quot;softmax&quot;, name = &quot;output_layer&quot;)(x) # 8. Combine the inputs with the ouputs into a model model_0 = tf.keras.Model(inputs,outputs) # 9. Compile the model model_0.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # 10. Fit the model history_10_percent = model_0.fit(train_data_10_percent, epochs = 5, steps_per_epoch = len(train_data_10_percent), validation_data = test_data, validation_steps = int(0.25*len(test_data)), callbacks = [create_tensorboard_callback(dir_name = &quot;transfer_learning&quot;, experiment_name =&quot;10_percent_feature_extraction&quot;)]) . Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5 16711680/16705208 [==============================] - 0s 0us/step 16719872/16705208 [==============================] - 0s 0us/step Shape after passing inputs through base model: (None, 7, 7, 1280) Shape after GlobalAveragePooling2D: (None, 1280) Saving TensorBoard log files to: transfer_learning/10_percent_feature_extraction/20220220-051900 Epoch 1/5 24/24 [==============================] - 28s 455ms/step - loss: 1.8831 - accuracy: 0.4107 - val_loss: 1.3535 - val_accuracy: 0.7023 Epoch 2/5 24/24 [==============================] - 9s 361ms/step - loss: 1.1441 - accuracy: 0.7613 - val_loss: 0.9226 - val_accuracy: 0.8092 Epoch 3/5 24/24 [==============================] - 9s 357ms/step - loss: 0.8421 - accuracy: 0.8080 - val_loss: 0.7516 - val_accuracy: 0.8240 Epoch 4/5 24/24 [==============================] - 7s 274ms/step - loss: 0.6958 - accuracy: 0.8400 - val_loss: 0.6333 - val_accuracy: 0.8454 Epoch 5/5 24/24 [==============================] - 9s 357ms/step - loss: 0.5975 - accuracy: 0.8627 - val_loss: 0.6452 - val_accuracy: 0.8355 . model_0.evaluate(test_data) . 79/79 [==============================] - 12s 139ms/step - loss: 0.6102 - accuracy: 0.8412 . [0.6101745367050171, 0.8411999940872192] . for layer_number, layer in enumerate(base_model.layers): print(layer_number, layer.name) . 0 input_1 1 rescaling 2 normalization 3 stem_conv_pad 4 stem_conv 5 stem_bn 6 stem_activation 7 block1a_dwconv 8 block1a_bn 9 block1a_activation 10 block1a_se_squeeze 11 block1a_se_reshape 12 block1a_se_reduce 13 block1a_se_expand 14 block1a_se_excite 15 block1a_project_conv 16 block1a_project_bn 17 block2a_expand_conv 18 block2a_expand_bn 19 block2a_expand_activation 20 block2a_dwconv_pad 21 block2a_dwconv 22 block2a_bn 23 block2a_activation 24 block2a_se_squeeze 25 block2a_se_reshape 26 block2a_se_reduce 27 block2a_se_expand 28 block2a_se_excite 29 block2a_project_conv 30 block2a_project_bn 31 block2b_expand_conv 32 block2b_expand_bn 33 block2b_expand_activation 34 block2b_dwconv 35 block2b_bn 36 block2b_activation 37 block2b_se_squeeze 38 block2b_se_reshape 39 block2b_se_reduce 40 block2b_se_expand 41 block2b_se_excite 42 block2b_project_conv 43 block2b_project_bn 44 block2b_drop 45 block2b_add 46 block3a_expand_conv 47 block3a_expand_bn 48 block3a_expand_activation 49 block3a_dwconv_pad 50 block3a_dwconv 51 block3a_bn 52 block3a_activation 53 block3a_se_squeeze 54 block3a_se_reshape 55 block3a_se_reduce 56 block3a_se_expand 57 block3a_se_excite 58 block3a_project_conv 59 block3a_project_bn 60 block3b_expand_conv 61 block3b_expand_bn 62 block3b_expand_activation 63 block3b_dwconv 64 block3b_bn 65 block3b_activation 66 block3b_se_squeeze 67 block3b_se_reshape 68 block3b_se_reduce 69 block3b_se_expand 70 block3b_se_excite 71 block3b_project_conv 72 block3b_project_bn 73 block3b_drop 74 block3b_add 75 block4a_expand_conv 76 block4a_expand_bn 77 block4a_expand_activation 78 block4a_dwconv_pad 79 block4a_dwconv 80 block4a_bn 81 block4a_activation 82 block4a_se_squeeze 83 block4a_se_reshape 84 block4a_se_reduce 85 block4a_se_expand 86 block4a_se_excite 87 block4a_project_conv 88 block4a_project_bn 89 block4b_expand_conv 90 block4b_expand_bn 91 block4b_expand_activation 92 block4b_dwconv 93 block4b_bn 94 block4b_activation 95 block4b_se_squeeze 96 block4b_se_reshape 97 block4b_se_reduce 98 block4b_se_expand 99 block4b_se_excite 100 block4b_project_conv 101 block4b_project_bn 102 block4b_drop 103 block4b_add 104 block4c_expand_conv 105 block4c_expand_bn 106 block4c_expand_activation 107 block4c_dwconv 108 block4c_bn 109 block4c_activation 110 block4c_se_squeeze 111 block4c_se_reshape 112 block4c_se_reduce 113 block4c_se_expand 114 block4c_se_excite 115 block4c_project_conv 116 block4c_project_bn 117 block4c_drop 118 block4c_add 119 block5a_expand_conv 120 block5a_expand_bn 121 block5a_expand_activation 122 block5a_dwconv 123 block5a_bn 124 block5a_activation 125 block5a_se_squeeze 126 block5a_se_reshape 127 block5a_se_reduce 128 block5a_se_expand 129 block5a_se_excite 130 block5a_project_conv 131 block5a_project_bn 132 block5b_expand_conv 133 block5b_expand_bn 134 block5b_expand_activation 135 block5b_dwconv 136 block5b_bn 137 block5b_activation 138 block5b_se_squeeze 139 block5b_se_reshape 140 block5b_se_reduce 141 block5b_se_expand 142 block5b_se_excite 143 block5b_project_conv 144 block5b_project_bn 145 block5b_drop 146 block5b_add 147 block5c_expand_conv 148 block5c_expand_bn 149 block5c_expand_activation 150 block5c_dwconv 151 block5c_bn 152 block5c_activation 153 block5c_se_squeeze 154 block5c_se_reshape 155 block5c_se_reduce 156 block5c_se_expand 157 block5c_se_excite 158 block5c_project_conv 159 block5c_project_bn 160 block5c_drop 161 block5c_add 162 block6a_expand_conv 163 block6a_expand_bn 164 block6a_expand_activation 165 block6a_dwconv_pad 166 block6a_dwconv 167 block6a_bn 168 block6a_activation 169 block6a_se_squeeze 170 block6a_se_reshape 171 block6a_se_reduce 172 block6a_se_expand 173 block6a_se_excite 174 block6a_project_conv 175 block6a_project_bn 176 block6b_expand_conv 177 block6b_expand_bn 178 block6b_expand_activation 179 block6b_dwconv 180 block6b_bn 181 block6b_activation 182 block6b_se_squeeze 183 block6b_se_reshape 184 block6b_se_reduce 185 block6b_se_expand 186 block6b_se_excite 187 block6b_project_conv 188 block6b_project_bn 189 block6b_drop 190 block6b_add 191 block6c_expand_conv 192 block6c_expand_bn 193 block6c_expand_activation 194 block6c_dwconv 195 block6c_bn 196 block6c_activation 197 block6c_se_squeeze 198 block6c_se_reshape 199 block6c_se_reduce 200 block6c_se_expand 201 block6c_se_excite 202 block6c_project_conv 203 block6c_project_bn 204 block6c_drop 205 block6c_add 206 block6d_expand_conv 207 block6d_expand_bn 208 block6d_expand_activation 209 block6d_dwconv 210 block6d_bn 211 block6d_activation 212 block6d_se_squeeze 213 block6d_se_reshape 214 block6d_se_reduce 215 block6d_se_expand 216 block6d_se_excite 217 block6d_project_conv 218 block6d_project_bn 219 block6d_drop 220 block6d_add 221 block7a_expand_conv 222 block7a_expand_bn 223 block7a_expand_activation 224 block7a_dwconv 225 block7a_bn 226 block7a_activation 227 block7a_se_squeeze 228 block7a_se_reshape 229 block7a_se_reduce 230 block7a_se_expand 231 block7a_se_excite 232 block7a_project_conv 233 block7a_project_bn 234 top_conv 235 top_bn 236 top_activation . . 236 layers in EfficientNetB0. The EfficientNetB0 architecture already has the first layers with normalization so we don&#39;t need to do the rescaling. . # Let&#39;s check the summary of the base model i.e. EfficientNetB0 Model base_model.summary() . Model: &#34;efficientnetb0&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, None, None, 0 [] 3)] rescaling (Rescaling) (None, None, None, 0 [&#39;input_1[0][0]&#39;] 3) normalization (Normalization) (None, None, None, 7 [&#39;rescaling[0][0]&#39;] 3) stem_conv_pad (ZeroPadding2D) (None, None, None, 0 [&#39;normalization[0][0]&#39;] 3) stem_conv (Conv2D) (None, None, None, 864 [&#39;stem_conv_pad[0][0]&#39;] 32) stem_bn (BatchNormalization) (None, None, None, 128 [&#39;stem_conv[0][0]&#39;] 32) stem_activation (Activation) (None, None, None, 0 [&#39;stem_bn[0][0]&#39;] 32) block1a_dwconv (DepthwiseConv2 (None, None, None, 288 [&#39;stem_activation[0][0]&#39;] D) 32) block1a_bn (BatchNormalization (None, None, None, 128 [&#39;block1a_dwconv[0][0]&#39;] ) 32) block1a_activation (Activation (None, None, None, 0 [&#39;block1a_bn[0][0]&#39;] ) 32) block1a_se_squeeze (GlobalAver (None, 32) 0 [&#39;block1a_activation[0][0]&#39;] agePooling2D) block1a_se_reshape (Reshape) (None, 1, 1, 32) 0 [&#39;block1a_se_squeeze[0][0]&#39;] block1a_se_reduce (Conv2D) (None, 1, 1, 8) 264 [&#39;block1a_se_reshape[0][0]&#39;] block1a_se_expand (Conv2D) (None, 1, 1, 32) 288 [&#39;block1a_se_reduce[0][0]&#39;] block1a_se_excite (Multiply) (None, None, None, 0 [&#39;block1a_activation[0][0]&#39;, 32) &#39;block1a_se_expand[0][0]&#39;] block1a_project_conv (Conv2D) (None, None, None, 512 [&#39;block1a_se_excite[0][0]&#39;] 16) block1a_project_bn (BatchNorma (None, None, None, 64 [&#39;block1a_project_conv[0][0]&#39;] lization) 16) block2a_expand_conv (Conv2D) (None, None, None, 1536 [&#39;block1a_project_bn[0][0]&#39;] 96) block2a_expand_bn (BatchNormal (None, None, None, 384 [&#39;block2a_expand_conv[0][0]&#39;] ization) 96) block2a_expand_activation (Act (None, None, None, 0 [&#39;block2a_expand_bn[0][0]&#39;] ivation) 96) block2a_dwconv_pad (ZeroPaddin (None, None, None, 0 [&#39;block2a_expand_activation[0][0] g2D) 96) &#39;] block2a_dwconv (DepthwiseConv2 (None, None, None, 864 [&#39;block2a_dwconv_pad[0][0]&#39;] D) 96) block2a_bn (BatchNormalization (None, None, None, 384 [&#39;block2a_dwconv[0][0]&#39;] ) 96) block2a_activation (Activation (None, None, None, 0 [&#39;block2a_bn[0][0]&#39;] ) 96) block2a_se_squeeze (GlobalAver (None, 96) 0 [&#39;block2a_activation[0][0]&#39;] agePooling2D) block2a_se_reshape (Reshape) (None, 1, 1, 96) 0 [&#39;block2a_se_squeeze[0][0]&#39;] block2a_se_reduce (Conv2D) (None, 1, 1, 4) 388 [&#39;block2a_se_reshape[0][0]&#39;] block2a_se_expand (Conv2D) (None, 1, 1, 96) 480 [&#39;block2a_se_reduce[0][0]&#39;] block2a_se_excite (Multiply) (None, None, None, 0 [&#39;block2a_activation[0][0]&#39;, 96) &#39;block2a_se_expand[0][0]&#39;] block2a_project_conv (Conv2D) (None, None, None, 2304 [&#39;block2a_se_excite[0][0]&#39;] 24) block2a_project_bn (BatchNorma (None, None, None, 96 [&#39;block2a_project_conv[0][0]&#39;] lization) 24) block2b_expand_conv (Conv2D) (None, None, None, 3456 [&#39;block2a_project_bn[0][0]&#39;] 144) block2b_expand_bn (BatchNormal (None, None, None, 576 [&#39;block2b_expand_conv[0][0]&#39;] ization) 144) block2b_expand_activation (Act (None, None, None, 0 [&#39;block2b_expand_bn[0][0]&#39;] ivation) 144) block2b_dwconv (DepthwiseConv2 (None, None, None, 1296 [&#39;block2b_expand_activation[0][0] D) 144) &#39;] block2b_bn (BatchNormalization (None, None, None, 576 [&#39;block2b_dwconv[0][0]&#39;] ) 144) block2b_activation (Activation (None, None, None, 0 [&#39;block2b_bn[0][0]&#39;] ) 144) block2b_se_squeeze (GlobalAver (None, 144) 0 [&#39;block2b_activation[0][0]&#39;] agePooling2D) block2b_se_reshape (Reshape) (None, 1, 1, 144) 0 [&#39;block2b_se_squeeze[0][0]&#39;] block2b_se_reduce (Conv2D) (None, 1, 1, 6) 870 [&#39;block2b_se_reshape[0][0]&#39;] block2b_se_expand (Conv2D) (None, 1, 1, 144) 1008 [&#39;block2b_se_reduce[0][0]&#39;] block2b_se_excite (Multiply) (None, None, None, 0 [&#39;block2b_activation[0][0]&#39;, 144) &#39;block2b_se_expand[0][0]&#39;] block2b_project_conv (Conv2D) (None, None, None, 3456 [&#39;block2b_se_excite[0][0]&#39;] 24) block2b_project_bn (BatchNorma (None, None, None, 96 [&#39;block2b_project_conv[0][0]&#39;] lization) 24) block2b_drop (Dropout) (None, None, None, 0 [&#39;block2b_project_bn[0][0]&#39;] 24) block2b_add (Add) (None, None, None, 0 [&#39;block2b_drop[0][0]&#39;, 24) &#39;block2a_project_bn[0][0]&#39;] block3a_expand_conv (Conv2D) (None, None, None, 3456 [&#39;block2b_add[0][0]&#39;] 144) block3a_expand_bn (BatchNormal (None, None, None, 576 [&#39;block3a_expand_conv[0][0]&#39;] ization) 144) block3a_expand_activation (Act (None, None, None, 0 [&#39;block3a_expand_bn[0][0]&#39;] ivation) 144) block3a_dwconv_pad (ZeroPaddin (None, None, None, 0 [&#39;block3a_expand_activation[0][0] g2D) 144) &#39;] block3a_dwconv (DepthwiseConv2 (None, None, None, 3600 [&#39;block3a_dwconv_pad[0][0]&#39;] D) 144) block3a_bn (BatchNormalization (None, None, None, 576 [&#39;block3a_dwconv[0][0]&#39;] ) 144) block3a_activation (Activation (None, None, None, 0 [&#39;block3a_bn[0][0]&#39;] ) 144) block3a_se_squeeze (GlobalAver (None, 144) 0 [&#39;block3a_activation[0][0]&#39;] agePooling2D) block3a_se_reshape (Reshape) (None, 1, 1, 144) 0 [&#39;block3a_se_squeeze[0][0]&#39;] block3a_se_reduce (Conv2D) (None, 1, 1, 6) 870 [&#39;block3a_se_reshape[0][0]&#39;] block3a_se_expand (Conv2D) (None, 1, 1, 144) 1008 [&#39;block3a_se_reduce[0][0]&#39;] block3a_se_excite (Multiply) (None, None, None, 0 [&#39;block3a_activation[0][0]&#39;, 144) &#39;block3a_se_expand[0][0]&#39;] block3a_project_conv (Conv2D) (None, None, None, 5760 [&#39;block3a_se_excite[0][0]&#39;] 40) block3a_project_bn (BatchNorma (None, None, None, 160 [&#39;block3a_project_conv[0][0]&#39;] lization) 40) block3b_expand_conv (Conv2D) (None, None, None, 9600 [&#39;block3a_project_bn[0][0]&#39;] 240) block3b_expand_bn (BatchNormal (None, None, None, 960 [&#39;block3b_expand_conv[0][0]&#39;] ization) 240) block3b_expand_activation (Act (None, None, None, 0 [&#39;block3b_expand_bn[0][0]&#39;] ivation) 240) block3b_dwconv (DepthwiseConv2 (None, None, None, 6000 [&#39;block3b_expand_activation[0][0] D) 240) &#39;] block3b_bn (BatchNormalization (None, None, None, 960 [&#39;block3b_dwconv[0][0]&#39;] ) 240) block3b_activation (Activation (None, None, None, 0 [&#39;block3b_bn[0][0]&#39;] ) 240) block3b_se_squeeze (GlobalAver (None, 240) 0 [&#39;block3b_activation[0][0]&#39;] agePooling2D) block3b_se_reshape (Reshape) (None, 1, 1, 240) 0 [&#39;block3b_se_squeeze[0][0]&#39;] block3b_se_reduce (Conv2D) (None, 1, 1, 10) 2410 [&#39;block3b_se_reshape[0][0]&#39;] block3b_se_expand (Conv2D) (None, 1, 1, 240) 2640 [&#39;block3b_se_reduce[0][0]&#39;] block3b_se_excite (Multiply) (None, None, None, 0 [&#39;block3b_activation[0][0]&#39;, 240) &#39;block3b_se_expand[0][0]&#39;] block3b_project_conv (Conv2D) (None, None, None, 9600 [&#39;block3b_se_excite[0][0]&#39;] 40) block3b_project_bn (BatchNorma (None, None, None, 160 [&#39;block3b_project_conv[0][0]&#39;] lization) 40) block3b_drop (Dropout) (None, None, None, 0 [&#39;block3b_project_bn[0][0]&#39;] 40) block3b_add (Add) (None, None, None, 0 [&#39;block3b_drop[0][0]&#39;, 40) &#39;block3a_project_bn[0][0]&#39;] block4a_expand_conv (Conv2D) (None, None, None, 9600 [&#39;block3b_add[0][0]&#39;] 240) block4a_expand_bn (BatchNormal (None, None, None, 960 [&#39;block4a_expand_conv[0][0]&#39;] ization) 240) block4a_expand_activation (Act (None, None, None, 0 [&#39;block4a_expand_bn[0][0]&#39;] ivation) 240) block4a_dwconv_pad (ZeroPaddin (None, None, None, 0 [&#39;block4a_expand_activation[0][0] g2D) 240) &#39;] block4a_dwconv (DepthwiseConv2 (None, None, None, 2160 [&#39;block4a_dwconv_pad[0][0]&#39;] D) 240) block4a_bn (BatchNormalization (None, None, None, 960 [&#39;block4a_dwconv[0][0]&#39;] ) 240) block4a_activation (Activation (None, None, None, 0 [&#39;block4a_bn[0][0]&#39;] ) 240) block4a_se_squeeze (GlobalAver (None, 240) 0 [&#39;block4a_activation[0][0]&#39;] agePooling2D) block4a_se_reshape (Reshape) (None, 1, 1, 240) 0 [&#39;block4a_se_squeeze[0][0]&#39;] block4a_se_reduce (Conv2D) (None, 1, 1, 10) 2410 [&#39;block4a_se_reshape[0][0]&#39;] block4a_se_expand (Conv2D) (None, 1, 1, 240) 2640 [&#39;block4a_se_reduce[0][0]&#39;] block4a_se_excite (Multiply) (None, None, None, 0 [&#39;block4a_activation[0][0]&#39;, 240) &#39;block4a_se_expand[0][0]&#39;] block4a_project_conv (Conv2D) (None, None, None, 19200 [&#39;block4a_se_excite[0][0]&#39;] 80) block4a_project_bn (BatchNorma (None, None, None, 320 [&#39;block4a_project_conv[0][0]&#39;] lization) 80) block4b_expand_conv (Conv2D) (None, None, None, 38400 [&#39;block4a_project_bn[0][0]&#39;] 480) block4b_expand_bn (BatchNormal (None, None, None, 1920 [&#39;block4b_expand_conv[0][0]&#39;] ization) 480) block4b_expand_activation (Act (None, None, None, 0 [&#39;block4b_expand_bn[0][0]&#39;] ivation) 480) block4b_dwconv (DepthwiseConv2 (None, None, None, 4320 [&#39;block4b_expand_activation[0][0] D) 480) &#39;] block4b_bn (BatchNormalization (None, None, None, 1920 [&#39;block4b_dwconv[0][0]&#39;] ) 480) block4b_activation (Activation (None, None, None, 0 [&#39;block4b_bn[0][0]&#39;] ) 480) block4b_se_squeeze (GlobalAver (None, 480) 0 [&#39;block4b_activation[0][0]&#39;] agePooling2D) block4b_se_reshape (Reshape) (None, 1, 1, 480) 0 [&#39;block4b_se_squeeze[0][0]&#39;] block4b_se_reduce (Conv2D) (None, 1, 1, 20) 9620 [&#39;block4b_se_reshape[0][0]&#39;] block4b_se_expand (Conv2D) (None, 1, 1, 480) 10080 [&#39;block4b_se_reduce[0][0]&#39;] block4b_se_excite (Multiply) (None, None, None, 0 [&#39;block4b_activation[0][0]&#39;, 480) &#39;block4b_se_expand[0][0]&#39;] block4b_project_conv (Conv2D) (None, None, None, 38400 [&#39;block4b_se_excite[0][0]&#39;] 80) block4b_project_bn (BatchNorma (None, None, None, 320 [&#39;block4b_project_conv[0][0]&#39;] lization) 80) block4b_drop (Dropout) (None, None, None, 0 [&#39;block4b_project_bn[0][0]&#39;] 80) block4b_add (Add) (None, None, None, 0 [&#39;block4b_drop[0][0]&#39;, 80) &#39;block4a_project_bn[0][0]&#39;] block4c_expand_conv (Conv2D) (None, None, None, 38400 [&#39;block4b_add[0][0]&#39;] 480) block4c_expand_bn (BatchNormal (None, None, None, 1920 [&#39;block4c_expand_conv[0][0]&#39;] ization) 480) block4c_expand_activation (Act (None, None, None, 0 [&#39;block4c_expand_bn[0][0]&#39;] ivation) 480) block4c_dwconv (DepthwiseConv2 (None, None, None, 4320 [&#39;block4c_expand_activation[0][0] D) 480) &#39;] block4c_bn (BatchNormalization (None, None, None, 1920 [&#39;block4c_dwconv[0][0]&#39;] ) 480) block4c_activation (Activation (None, None, None, 0 [&#39;block4c_bn[0][0]&#39;] ) 480) block4c_se_squeeze (GlobalAver (None, 480) 0 [&#39;block4c_activation[0][0]&#39;] agePooling2D) block4c_se_reshape (Reshape) (None, 1, 1, 480) 0 [&#39;block4c_se_squeeze[0][0]&#39;] block4c_se_reduce (Conv2D) (None, 1, 1, 20) 9620 [&#39;block4c_se_reshape[0][0]&#39;] block4c_se_expand (Conv2D) (None, 1, 1, 480) 10080 [&#39;block4c_se_reduce[0][0]&#39;] block4c_se_excite (Multiply) (None, None, None, 0 [&#39;block4c_activation[0][0]&#39;, 480) &#39;block4c_se_expand[0][0]&#39;] block4c_project_conv (Conv2D) (None, None, None, 38400 [&#39;block4c_se_excite[0][0]&#39;] 80) block4c_project_bn (BatchNorma (None, None, None, 320 [&#39;block4c_project_conv[0][0]&#39;] lization) 80) block4c_drop (Dropout) (None, None, None, 0 [&#39;block4c_project_bn[0][0]&#39;] 80) block4c_add (Add) (None, None, None, 0 [&#39;block4c_drop[0][0]&#39;, 80) &#39;block4b_add[0][0]&#39;] block5a_expand_conv (Conv2D) (None, None, None, 38400 [&#39;block4c_add[0][0]&#39;] 480) block5a_expand_bn (BatchNormal (None, None, None, 1920 [&#39;block5a_expand_conv[0][0]&#39;] ization) 480) block5a_expand_activation (Act (None, None, None, 0 [&#39;block5a_expand_bn[0][0]&#39;] ivation) 480) block5a_dwconv (DepthwiseConv2 (None, None, None, 12000 [&#39;block5a_expand_activation[0][0] D) 480) &#39;] block5a_bn (BatchNormalization (None, None, None, 1920 [&#39;block5a_dwconv[0][0]&#39;] ) 480) block5a_activation (Activation (None, None, None, 0 [&#39;block5a_bn[0][0]&#39;] ) 480) block5a_se_squeeze (GlobalAver (None, 480) 0 [&#39;block5a_activation[0][0]&#39;] agePooling2D) block5a_se_reshape (Reshape) (None, 1, 1, 480) 0 [&#39;block5a_se_squeeze[0][0]&#39;] block5a_se_reduce (Conv2D) (None, 1, 1, 20) 9620 [&#39;block5a_se_reshape[0][0]&#39;] block5a_se_expand (Conv2D) (None, 1, 1, 480) 10080 [&#39;block5a_se_reduce[0][0]&#39;] block5a_se_excite (Multiply) (None, None, None, 0 [&#39;block5a_activation[0][0]&#39;, 480) &#39;block5a_se_expand[0][0]&#39;] block5a_project_conv (Conv2D) (None, None, None, 53760 [&#39;block5a_se_excite[0][0]&#39;] 112) block5a_project_bn (BatchNorma (None, None, None, 448 [&#39;block5a_project_conv[0][0]&#39;] lization) 112) block5b_expand_conv (Conv2D) (None, None, None, 75264 [&#39;block5a_project_bn[0][0]&#39;] 672) block5b_expand_bn (BatchNormal (None, None, None, 2688 [&#39;block5b_expand_conv[0][0]&#39;] ization) 672) block5b_expand_activation (Act (None, None, None, 0 [&#39;block5b_expand_bn[0][0]&#39;] ivation) 672) block5b_dwconv (DepthwiseConv2 (None, None, None, 16800 [&#39;block5b_expand_activation[0][0] D) 672) &#39;] block5b_bn (BatchNormalization (None, None, None, 2688 [&#39;block5b_dwconv[0][0]&#39;] ) 672) block5b_activation (Activation (None, None, None, 0 [&#39;block5b_bn[0][0]&#39;] ) 672) block5b_se_squeeze (GlobalAver (None, 672) 0 [&#39;block5b_activation[0][0]&#39;] agePooling2D) block5b_se_reshape (Reshape) (None, 1, 1, 672) 0 [&#39;block5b_se_squeeze[0][0]&#39;] block5b_se_reduce (Conv2D) (None, 1, 1, 28) 18844 [&#39;block5b_se_reshape[0][0]&#39;] block5b_se_expand (Conv2D) (None, 1, 1, 672) 19488 [&#39;block5b_se_reduce[0][0]&#39;] block5b_se_excite (Multiply) (None, None, None, 0 [&#39;block5b_activation[0][0]&#39;, 672) &#39;block5b_se_expand[0][0]&#39;] block5b_project_conv (Conv2D) (None, None, None, 75264 [&#39;block5b_se_excite[0][0]&#39;] 112) block5b_project_bn (BatchNorma (None, None, None, 448 [&#39;block5b_project_conv[0][0]&#39;] lization) 112) block5b_drop (Dropout) (None, None, None, 0 [&#39;block5b_project_bn[0][0]&#39;] 112) block5b_add (Add) (None, None, None, 0 [&#39;block5b_drop[0][0]&#39;, 112) &#39;block5a_project_bn[0][0]&#39;] block5c_expand_conv (Conv2D) (None, None, None, 75264 [&#39;block5b_add[0][0]&#39;] 672) block5c_expand_bn (BatchNormal (None, None, None, 2688 [&#39;block5c_expand_conv[0][0]&#39;] ization) 672) block5c_expand_activation (Act (None, None, None, 0 [&#39;block5c_expand_bn[0][0]&#39;] ivation) 672) block5c_dwconv (DepthwiseConv2 (None, None, None, 16800 [&#39;block5c_expand_activation[0][0] D) 672) &#39;] block5c_bn (BatchNormalization (None, None, None, 2688 [&#39;block5c_dwconv[0][0]&#39;] ) 672) block5c_activation (Activation (None, None, None, 0 [&#39;block5c_bn[0][0]&#39;] ) 672) block5c_se_squeeze (GlobalAver (None, 672) 0 [&#39;block5c_activation[0][0]&#39;] agePooling2D) block5c_se_reshape (Reshape) (None, 1, 1, 672) 0 [&#39;block5c_se_squeeze[0][0]&#39;] block5c_se_reduce (Conv2D) (None, 1, 1, 28) 18844 [&#39;block5c_se_reshape[0][0]&#39;] block5c_se_expand (Conv2D) (None, 1, 1, 672) 19488 [&#39;block5c_se_reduce[0][0]&#39;] block5c_se_excite (Multiply) (None, None, None, 0 [&#39;block5c_activation[0][0]&#39;, 672) &#39;block5c_se_expand[0][0]&#39;] block5c_project_conv (Conv2D) (None, None, None, 75264 [&#39;block5c_se_excite[0][0]&#39;] 112) block5c_project_bn (BatchNorma (None, None, None, 448 [&#39;block5c_project_conv[0][0]&#39;] lization) 112) block5c_drop (Dropout) (None, None, None, 0 [&#39;block5c_project_bn[0][0]&#39;] 112) block5c_add (Add) (None, None, None, 0 [&#39;block5c_drop[0][0]&#39;, 112) &#39;block5b_add[0][0]&#39;] block6a_expand_conv (Conv2D) (None, None, None, 75264 [&#39;block5c_add[0][0]&#39;] 672) block6a_expand_bn (BatchNormal (None, None, None, 2688 [&#39;block6a_expand_conv[0][0]&#39;] ization) 672) block6a_expand_activation (Act (None, None, None, 0 [&#39;block6a_expand_bn[0][0]&#39;] ivation) 672) block6a_dwconv_pad (ZeroPaddin (None, None, None, 0 [&#39;block6a_expand_activation[0][0] g2D) 672) &#39;] block6a_dwconv (DepthwiseConv2 (None, None, None, 16800 [&#39;block6a_dwconv_pad[0][0]&#39;] D) 672) block6a_bn (BatchNormalization (None, None, None, 2688 [&#39;block6a_dwconv[0][0]&#39;] ) 672) block6a_activation (Activation (None, None, None, 0 [&#39;block6a_bn[0][0]&#39;] ) 672) block6a_se_squeeze (GlobalAver (None, 672) 0 [&#39;block6a_activation[0][0]&#39;] agePooling2D) block6a_se_reshape (Reshape) (None, 1, 1, 672) 0 [&#39;block6a_se_squeeze[0][0]&#39;] block6a_se_reduce (Conv2D) (None, 1, 1, 28) 18844 [&#39;block6a_se_reshape[0][0]&#39;] block6a_se_expand (Conv2D) (None, 1, 1, 672) 19488 [&#39;block6a_se_reduce[0][0]&#39;] block6a_se_excite (Multiply) (None, None, None, 0 [&#39;block6a_activation[0][0]&#39;, 672) &#39;block6a_se_expand[0][0]&#39;] block6a_project_conv (Conv2D) (None, None, None, 129024 [&#39;block6a_se_excite[0][0]&#39;] 192) block6a_project_bn (BatchNorma (None, None, None, 768 [&#39;block6a_project_conv[0][0]&#39;] lization) 192) block6b_expand_conv (Conv2D) (None, None, None, 221184 [&#39;block6a_project_bn[0][0]&#39;] 1152) block6b_expand_bn (BatchNormal (None, None, None, 4608 [&#39;block6b_expand_conv[0][0]&#39;] ization) 1152) block6b_expand_activation (Act (None, None, None, 0 [&#39;block6b_expand_bn[0][0]&#39;] ivation) 1152) block6b_dwconv (DepthwiseConv2 (None, None, None, 28800 [&#39;block6b_expand_activation[0][0] D) 1152) &#39;] block6b_bn (BatchNormalization (None, None, None, 4608 [&#39;block6b_dwconv[0][0]&#39;] ) 1152) block6b_activation (Activation (None, None, None, 0 [&#39;block6b_bn[0][0]&#39;] ) 1152) block6b_se_squeeze (GlobalAver (None, 1152) 0 [&#39;block6b_activation[0][0]&#39;] agePooling2D) block6b_se_reshape (Reshape) (None, 1, 1, 1152) 0 [&#39;block6b_se_squeeze[0][0]&#39;] block6b_se_reduce (Conv2D) (None, 1, 1, 48) 55344 [&#39;block6b_se_reshape[0][0]&#39;] block6b_se_expand (Conv2D) (None, 1, 1, 1152) 56448 [&#39;block6b_se_reduce[0][0]&#39;] block6b_se_excite (Multiply) (None, None, None, 0 [&#39;block6b_activation[0][0]&#39;, 1152) &#39;block6b_se_expand[0][0]&#39;] block6b_project_conv (Conv2D) (None, None, None, 221184 [&#39;block6b_se_excite[0][0]&#39;] 192) block6b_project_bn (BatchNorma (None, None, None, 768 [&#39;block6b_project_conv[0][0]&#39;] lization) 192) block6b_drop (Dropout) (None, None, None, 0 [&#39;block6b_project_bn[0][0]&#39;] 192) block6b_add (Add) (None, None, None, 0 [&#39;block6b_drop[0][0]&#39;, 192) &#39;block6a_project_bn[0][0]&#39;] block6c_expand_conv (Conv2D) (None, None, None, 221184 [&#39;block6b_add[0][0]&#39;] 1152) block6c_expand_bn (BatchNormal (None, None, None, 4608 [&#39;block6c_expand_conv[0][0]&#39;] ization) 1152) block6c_expand_activation (Act (None, None, None, 0 [&#39;block6c_expand_bn[0][0]&#39;] ivation) 1152) block6c_dwconv (DepthwiseConv2 (None, None, None, 28800 [&#39;block6c_expand_activation[0][0] D) 1152) &#39;] block6c_bn (BatchNormalization (None, None, None, 4608 [&#39;block6c_dwconv[0][0]&#39;] ) 1152) block6c_activation (Activation (None, None, None, 0 [&#39;block6c_bn[0][0]&#39;] ) 1152) block6c_se_squeeze (GlobalAver (None, 1152) 0 [&#39;block6c_activation[0][0]&#39;] agePooling2D) block6c_se_reshape (Reshape) (None, 1, 1, 1152) 0 [&#39;block6c_se_squeeze[0][0]&#39;] block6c_se_reduce (Conv2D) (None, 1, 1, 48) 55344 [&#39;block6c_se_reshape[0][0]&#39;] block6c_se_expand (Conv2D) (None, 1, 1, 1152) 56448 [&#39;block6c_se_reduce[0][0]&#39;] block6c_se_excite (Multiply) (None, None, None, 0 [&#39;block6c_activation[0][0]&#39;, 1152) &#39;block6c_se_expand[0][0]&#39;] block6c_project_conv (Conv2D) (None, None, None, 221184 [&#39;block6c_se_excite[0][0]&#39;] 192) block6c_project_bn (BatchNorma (None, None, None, 768 [&#39;block6c_project_conv[0][0]&#39;] lization) 192) block6c_drop (Dropout) (None, None, None, 0 [&#39;block6c_project_bn[0][0]&#39;] 192) block6c_add (Add) (None, None, None, 0 [&#39;block6c_drop[0][0]&#39;, 192) &#39;block6b_add[0][0]&#39;] block6d_expand_conv (Conv2D) (None, None, None, 221184 [&#39;block6c_add[0][0]&#39;] 1152) block6d_expand_bn (BatchNormal (None, None, None, 4608 [&#39;block6d_expand_conv[0][0]&#39;] ization) 1152) block6d_expand_activation (Act (None, None, None, 0 [&#39;block6d_expand_bn[0][0]&#39;] ivation) 1152) block6d_dwconv (DepthwiseConv2 (None, None, None, 28800 [&#39;block6d_expand_activation[0][0] D) 1152) &#39;] block6d_bn (BatchNormalization (None, None, None, 4608 [&#39;block6d_dwconv[0][0]&#39;] ) 1152) block6d_activation (Activation (None, None, None, 0 [&#39;block6d_bn[0][0]&#39;] ) 1152) block6d_se_squeeze (GlobalAver (None, 1152) 0 [&#39;block6d_activation[0][0]&#39;] agePooling2D) block6d_se_reshape (Reshape) (None, 1, 1, 1152) 0 [&#39;block6d_se_squeeze[0][0]&#39;] block6d_se_reduce (Conv2D) (None, 1, 1, 48) 55344 [&#39;block6d_se_reshape[0][0]&#39;] block6d_se_expand (Conv2D) (None, 1, 1, 1152) 56448 [&#39;block6d_se_reduce[0][0]&#39;] block6d_se_excite (Multiply) (None, None, None, 0 [&#39;block6d_activation[0][0]&#39;, 1152) &#39;block6d_se_expand[0][0]&#39;] block6d_project_conv (Conv2D) (None, None, None, 221184 [&#39;block6d_se_excite[0][0]&#39;] 192) block6d_project_bn (BatchNorma (None, None, None, 768 [&#39;block6d_project_conv[0][0]&#39;] lization) 192) block6d_drop (Dropout) (None, None, None, 0 [&#39;block6d_project_bn[0][0]&#39;] 192) block6d_add (Add) (None, None, None, 0 [&#39;block6d_drop[0][0]&#39;, 192) &#39;block6c_add[0][0]&#39;] block7a_expand_conv (Conv2D) (None, None, None, 221184 [&#39;block6d_add[0][0]&#39;] 1152) block7a_expand_bn (BatchNormal (None, None, None, 4608 [&#39;block7a_expand_conv[0][0]&#39;] ization) 1152) block7a_expand_activation (Act (None, None, None, 0 [&#39;block7a_expand_bn[0][0]&#39;] ivation) 1152) block7a_dwconv (DepthwiseConv2 (None, None, None, 10368 [&#39;block7a_expand_activation[0][0] D) 1152) &#39;] block7a_bn (BatchNormalization (None, None, None, 4608 [&#39;block7a_dwconv[0][0]&#39;] ) 1152) block7a_activation (Activation (None, None, None, 0 [&#39;block7a_bn[0][0]&#39;] ) 1152) block7a_se_squeeze (GlobalAver (None, 1152) 0 [&#39;block7a_activation[0][0]&#39;] agePooling2D) block7a_se_reshape (Reshape) (None, 1, 1, 1152) 0 [&#39;block7a_se_squeeze[0][0]&#39;] block7a_se_reduce (Conv2D) (None, 1, 1, 48) 55344 [&#39;block7a_se_reshape[0][0]&#39;] block7a_se_expand (Conv2D) (None, 1, 1, 1152) 56448 [&#39;block7a_se_reduce[0][0]&#39;] block7a_se_excite (Multiply) (None, None, None, 0 [&#39;block7a_activation[0][0]&#39;, 1152) &#39;block7a_se_expand[0][0]&#39;] block7a_project_conv (Conv2D) (None, None, None, 368640 [&#39;block7a_se_excite[0][0]&#39;] 320) block7a_project_bn (BatchNorma (None, None, None, 1280 [&#39;block7a_project_conv[0][0]&#39;] lization) 320) top_conv (Conv2D) (None, None, None, 409600 [&#39;block7a_project_bn[0][0]&#39;] 1280) top_bn (BatchNormalization) (None, None, None, 5120 [&#39;top_conv[0][0]&#39;] 1280) top_activation (Activation) (None, None, None, 0 [&#39;top_bn[0][0]&#39;] 1280) ================================================================================================== Total params: 4,049,571 Trainable params: 0 Non-trainable params: 4,049,571 __________________________________________________________________________________________________ . . model_0.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (InputLayer) [(None, 224, 224, 3)] 0 efficientnetb0 (Functional) (None, None, None, 1280) 4049571 global_average_pooling_laye (None, 1280) 0 r (GlobalAveragePooling2D) output_layer (Dense) (None, 10) 12810 ================================================================= Total params: 4,062,381 Trainable params: 12,810 Non-trainable params: 4,049,571 _________________________________________________________________ . import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) plot_loss_curves(history_10_percent) . Getting a Feature vector from a trained model . Let&#39;s demonstrate the Global Average Pooling 2D layer... . We have a tensor after our model goes through base_model of shape (None, 7, 7, 1280). . But then when it passes through GlobalAveragePooling2D, it turns into (None, 1280). . Let&#39;s use a similar shaped tensor of (1,4,4,3) and then pass it to GlobalAveragePooling2D. . input_shape = (1,4,4,3) # Create a random tensor tf.random.set_seed(42) input_tensor = tf.random.normal(input_shape) print(f&quot;Random input tensor: n {input_tensor} n&quot;) # Pass the random tensor to the GlobalAveragePooling 2D layer global_average_pooled_tensor = tf.keras.layers.GlobalAveragePooling2D()(input_tensor) print(f&quot;2D global average pooled random tensor: n {global_average_pooled_tensor} n&quot;) # Check the shape of the different tensors print(f&quot;Shape of input tensor: {input_tensor.shape} n&quot;) print(f&quot;Shape of Global Average Pooled 2D tensor : {global_average_pooled_tensor.shape} n&quot;) . Random input tensor: [[[[ 0.3274685 -0.8426258 0.3194337 ] [-1.4075519 -2.3880599 -1.0392479 ] [-0.5573232 0.539707 1.6994323 ] [ 0.28893656 -1.5066116 -0.2645474 ]] [[-0.59722406 -1.9171132 -0.62044144] [ 0.8504023 -0.40604794 -3.0258412 ] [ 0.9058464 0.29855987 -0.22561555] [-0.7616443 -1.8917141 -0.93847126]] [[ 0.77852213 -0.47338897 0.97772694] [ 0.24694404 0.20573747 -0.5256233 ] [ 0.32410017 0.02545409 -0.10638497] [-0.6369475 1.1603122 0.2507359 ]] [[-0.41728503 0.4012578 -1.4145443 ] [-0.5931857 -1.6617213 0.33567193] [ 0.10815629 0.23479682 -0.56668764] [-0.35819843 0.88698614 0.52744764]]]] 2D global average pooled random tensor: [[-0.09368646 -0.45840448 -0.2885598 ]] Shape of input tensor: (1, 4, 4, 3) Shape of Global Average Pooled 2D tensor : (1, 3) . tf.reduce_mean(input_tensor, axis = [1,2]) . &lt;tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[-0.09368646, -0.45840448, -0.2885598 ]], dtype=float32)&gt; . # Define the input shape input_shape = (1,4,4,3) # Create a random tensor tf.random.set_seed(42) input_tensor = tf.random.normal(input_shape) print(f&quot;Random input tensor: n {input_tensor} n&quot;) # Pass the random tensor to the GlobalAveragePooling 2D layer global_max_pooled_tensor = tf.keras.layers.GlobalMaxPooling2D()(input_tensor) print(f&quot;2D global max pooled random tensor: n {global_max_pooled_tensor} n&quot;) # Check the shape of the different tensors print(f&quot;Shape of input tensor: {input_tensor.shape} n&quot;) print(f&quot;Shape of Global Max Pooled 2D tensor : {global_max_pooled_tensor.shape} n&quot;) . Random input tensor: [[[[ 0.3274685 -0.8426258 0.3194337 ] [-1.4075519 -2.3880599 -1.0392479 ] [-0.5573232 0.539707 1.6994323 ] [ 0.28893656 -1.5066116 -0.2645474 ]] [[-0.59722406 -1.9171132 -0.62044144] [ 0.8504023 -0.40604794 -3.0258412 ] [ 0.9058464 0.29855987 -0.22561555] [-0.7616443 -1.8917141 -0.93847126]] [[ 0.77852213 -0.47338897 0.97772694] [ 0.24694404 0.20573747 -0.5256233 ] [ 0.32410017 0.02545409 -0.10638497] [-0.6369475 1.1603122 0.2507359 ]] [[-0.41728503 0.4012578 -1.4145443 ] [-0.5931857 -1.6617213 0.33567193] [ 0.10815629 0.23479682 -0.56668764] [-0.35819843 0.88698614 0.52744764]]]] 2D global max pooled random tensor: [[0.9058464 1.1603122 1.6994323]] Shape of input tensor: (1, 4, 4, 3) Shape of Global Max Pooled 2D tensor : (1, 3) . Note: One of the reasons feature extraction transfer learning is named how it is becuase what often happens is pre-trained model outputs a feature vector, a long tensor of number which represents the learned representation of the model on a particular sample, in our case, this is the output of the tf.keras.layers.GlobalAveragePooling2D() layer) which can then be used to extract patterns out of our own specific problem. . Feature Vector: . A feature vector is a learned representation of the input data (a compressed form of the input data based on how the model sees it) | . Running transfer learning experiments . We have seen the incredible results transfer learning can get with only 10% of training data, but how does it go with 1% of training data. We will set up a couple of experiments to find out. . model_1 - use feature extraction transfer learning with 1% of data with data augmentation. . | model_2 - use feature extraction transfer learning with 10% of the training data with data augmentation. . | model_3 - use fine-tuning transfer learning on 10% of the training data with data augmentation. . | model_4 - use fine-tuning transfer learning on 100% of the training data with data augmentation. | Note: throughout all experiments the same test dataset will be used to evaluate our model. This ensures consistency across evaluation metrics. . Getting and preprocessing data for model_1 . !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip unzip_data(&quot;10_food_classes_1_percent.zip&quot;) . --2022-02-20 05:20:41-- https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_1_percent.zip Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.120.128, 74.125.70.128, 74.125.69.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.120.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 133612354 (127M) [application/zip] Saving to: ‘10_food_classes_1_percent.zip’ 10_food_classes_1_p 100%[===================&gt;] 127.42M 160MB/s in 0.8s 2022-02-20 05:20:42 (160 MB/s) - ‘10_food_classes_1_percent.zip’ saved [133612354/133612354] . train_dir_1_percent = &quot;10_food_classes_1_percent/train&quot; test_dir = &quot;10_food_classes_1_percent/test&quot; . walk_through_dir(&quot;10_food_classes_1_percent&quot;) . There are 2 directories and 0 images in &#39;10_food_classes_1_percent&#39;. There are 10 directories and 0 images in &#39;10_food_classes_1_percent/train&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/pizza&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/fried_rice&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/grilled_salmon&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/hamburger&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/steak&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/sushi&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/chicken_wings&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/ramen&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/ice_cream&#39;. There are 0 directories and 7 images in &#39;10_food_classes_1_percent/train/chicken_curry&#39;. There are 10 directories and 0 images in &#39;10_food_classes_1_percent/test&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/pizza&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/hamburger&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/steak&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/sushi&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/chicken_wings&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/ramen&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/ice_cream&#39;. There are 0 directories and 250 images in &#39;10_food_classes_1_percent/test/chicken_curry&#39;. . IMG_SIZE = (224,224) BATCH_SIZE = 32 train_data_1_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_1_percent, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE, batch_size = BATCH_SIZE) # default is 32 test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE, batch_size=BATCH_SIZE) . Found 70 files belonging to 10 classes. Found 2500 files belonging to 10 classes. . Let&#39;s look at the Food Vision dataset we are using: . Dataset Name Source Classes Traning data Testing data . pizza_steak | Food101 | pizza,steak(2) | 750 images of pizza and steak(same as original Food101 dataset) | 250 images of pizza and steak (same as original Food101 dataset | . 10_food_classes_1_percent | Same as above | Chicken curry, Chicken wings, fried rice, grilled salmon, hamburger, ice cream, pizza, ramen, steak, sushi(10) | 7 randomly selected images of each (1% of original training data) | 250 images of each class(same as original Food101 dataset | . 10_food_classes_10_percent | Same as above | Same as above | 75 randomly selected images of each class(10% of original training data) | Same as above | . 10_food_classes_100_percent | Same as above | Same as above | 750 images of each class (100% of original training data) | Same as above | . 101_food_classes_10_percenet | Same as above | All classes from Food101(101) | 75 images of each class (10% of original Food101 dataset) | 250 images of each class (same as original Food101 dataset | . Adding data augmentation into the model . To add data augmentation right into our models, we can use the layers inside: . tf.keras.layers.experimental.preprocessing() | . We can see the benefits of doing this withing the TensorFlow data augmentation documentation: https://www.tensorflow.org/tutorials/images/data_augmentation . Main Benefits: . Preprocessing of image (augmenting them) happens on the GPU (much faster) rather than CPU. | Image data augmentation only happens during training, so we can still export our whole model and use it elsewhere. | . Example of using data augmentation as the first layer within a model (EfficientNetB0). . The data augmentation transformations we&#39;re going to use are: . RandomFlip - flips image on horizontal or vertical axis. | RandomRotation - randomly rotates image by a specified amount. | RandomZoom - randomly zooms into an image by specified amount. | RandomHeight - randomly shifts image height by a specified amount. | RandomWidth - randomly shifts image width by a specified amount. | Rescaling - normalizes the image pixel values to be between 0 and 1, this is worth mentioning because it is required for some image models but since we&#39;re using the tf.keras.applications implementation of EfficientNetB0, it&#39;s not required. | . import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing # Create data augmentation stage with horizontal flipping, rotation, zoom, rotations etc. data_augmentation = keras.Sequential([ preprocessing.RandomFlip(&quot;horizontal&quot;), preprocessing.RandomRotation(0.2), preprocessing.RandomZoom(0.2), preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), # preprocessing.Rescale(1./255) # Keep for model like ResNet50V2 but Efficient has in-built rescaling ], name = &quot;data_augmentation&quot;) . Visualize our data augmentation layer . import matplotlib.pyplot as plt import matplotlib.image as mpimg import os import random target_class = random.choice(train_data_1_percent.class_names) # choose a random class target_dir = &quot;10_food_classes_1_percent/train/&quot; + target_class # create the target directory random_image = random.choice(os.listdir(target_dir)) # choose a random image from target directory random_image_path = target_dir + &quot;/&quot; + random_image # create the choosen random image path img = mpimg.imread(random_image_path) # read in the chosen target image plt.imshow(img) # plot the target image plt.title(f&quot;Original random image from class: {target_class}&quot;) plt.axis(False); # turn off the axes # Augment the image augmented_img = data_augmentation(tf.expand_dims(img, axis=0)) # data augmentation model requires shape (None, height, width, 3) plt.figure() plt.imshow(tf.squeeze(augmented_img)/255.) # requires normalization after augmentation plt.title(f&quot;Augmented random image from class: {target_class}&quot;) plt.axis(False); . Model 1 : Feature extraction Transfer learning on 1% of data with data augmentation . input_shape = (224,224,3) base_model = tf.keras.applications.EfficientNetB0(include_top = False) base_model.trainable = False # Create input layers inputs = layers.Input(shape = input_shape) # Add in data augmentation Sequential model as a layer x = data_augmentation(inputs) # Give base model the inputs (after augmentation) and don&#39;t train it x = base_model(x, training = False) # Pool output features of the base model x = layers.GlobalAveragePooling2D()(x) # Put a dense layer on as the output outputs = layers.Dense(10, activation = &quot;softmax&quot;, name= &quot;output_layer&quot;)(x) # Make a model using the inputs and outputs model_1 = keras.Model(inputs,outputs) # Compile the Model model_1.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit the Model history_1_percent = model_1.fit(train_data_1_percent, epochs = 5, steps_per_epoch = len(train_data_1_percent), validation_data = test_data, validation_steps = int(0.25*len(test_data)), callbacks = [create_tensorboard_callback(dir_name=&quot;transfer_learning&quot;, experiment_name = &quot;1_percent_data_aug&quot;)]) . Saving TensorBoard log files to: transfer_learning/1_percent_data_aug/20220220-052221 Epoch 1/5 3/3 [==============================] - 15s 3s/step - loss: 2.3906 - accuracy: 0.1000 - val_loss: 2.2103 - val_accuracy: 0.1760 Epoch 2/5 3/3 [==============================] - 5s 2s/step - loss: 2.1522 - accuracy: 0.2000 - val_loss: 2.1057 - val_accuracy: 0.2599 Epoch 3/5 3/3 [==============================] - 5s 2s/step - loss: 1.9507 - accuracy: 0.3571 - val_loss: 2.0142 - val_accuracy: 0.3125 Epoch 4/5 3/3 [==============================] - 6s 3s/step - loss: 1.7978 - accuracy: 0.5143 - val_loss: 1.9147 - val_accuracy: 0.3882 Epoch 5/5 3/3 [==============================] - 5s 2s/step - loss: 1.6239 - accuracy: 0.6286 - val_loss: 1.8459 - val_accuracy: 0.4293 . model_1.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_5 (InputLayer) [(None, 224, 224, 3)] 0 data_augmentation (Sequenti (None, None, None, 3) 0 al) efficientnetb0 (Functional) (None, None, None, 1280) 4049571 global_average_pooling2d_1 (None, 1280) 0 (GlobalAveragePooling2D) output_layer (Dense) (None, 10) 12810 ================================================================= Total params: 4,062,381 Trainable params: 12,810 Non-trainable params: 4,049,571 _________________________________________________________________ . results_1_percent_data_aug = model_1.evaluate(test_data) results_1_percent_data_aug . 79/79 [==============================] - 11s 132ms/step - loss: 1.8305 - accuracy: 0.4460 . [1.8304558992385864, 0.44600000977516174] . plot_loss_curves(history_1_percent) . Model 2: Feature extraction transfer learning model with 10% of data and data augmentation . #!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip ##unzip_data(10_food_classes_10_percent) train_dir_10_percent = &quot;10_food_classes_10_percent/train&quot; test_dir = &quot;10_food_classes_10_percent/test&quot; . IMG_SIZE . (224, 224) . walk_through_dir(&quot;10_food_classes_10_percent&quot;) . There are 2 directories and 0 images in &#39;10_food_classes_10_percent&#39;. There are 10 directories and 0 images in &#39;10_food_classes_10_percent/train&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/pizza&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/fried_rice&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/grilled_salmon&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/hamburger&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/steak&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/sushi&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/chicken_wings&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/ramen&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/ice_cream&#39;. There are 0 directories and 75 images in &#39;10_food_classes_10_percent/train/chicken_curry&#39;. There are 10 directories and 0 images in &#39;10_food_classes_10_percent/test&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/pizza&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/hamburger&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/steak&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/sushi&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/chicken_wings&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/ramen&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/ice_cream&#39;. There are 0 directories and 250 images in &#39;10_food_classes_10_percent/test/chicken_curry&#39;. . import tensorflow as tf IMG_SIZE = (224,224) train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(train_dir_10_percent, label_mode = &quot;categorical&quot;, image_size = (224,224)) test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE) . Found 750 files belonging to 10 classes. Found 2500 files belonging to 10 classes. . import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras.layers.experimental import preprocessing from tensorflow.keras.models import Sequential # Build data augmentation layer data_augmentation = Sequential([ preprocessing.RandomFlip(&#39;horizontal&#39;), preprocessing.RandomHeight(0.2), preprocessing.RandomWidth(0.2), preprocessing.RandomZoom(0.2), preprocessing.RandomRotation(0.2), # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNet ], name=&quot;data_augmentation&quot;) # Setup the input shape to our model input_shape = (224, 224, 3) # Create a frozen base model base_model = tf.keras.applications.EfficientNetB0(include_top=False) base_model.trainable = False # Create input and output layers inputs = layers.Input(shape=input_shape, name=&quot;input_layer&quot;) # create input layer x = data_augmentation(inputs) # augment our training images x = base_model(x, training=False) # pass augmented images to base model but keep it in inference mode, so batchnorm layers don&#39;t get updated: https://keras.io/guides/transfer_learning/#build-a-model x = layers.GlobalAveragePooling2D(name=&quot;global_average_pooling_layer&quot;)(x) outputs = layers.Dense(10, activation=&quot;softmax&quot;, name=&quot;output_layer&quot;)(x) model_2 = tf.keras.Model(inputs, outputs) # Compile model_2.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(lr=0.001), # use Adam optimizer with base learning rate metrics=[&quot;accuracy&quot;]) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . model_2.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (InputLayer) [(None, 224, 224, 3)] 0 data_augmentation (Sequenti (None, 224, 224, 3) 0 al) efficientnetb0 (Functional) (None, None, None, 1280) 4049571 global_average_pooling_laye (None, 1280) 0 r (GlobalAveragePooling2D) output_layer (Dense) (None, 10) 12810 ================================================================= Total params: 4,062,381 Trainable params: 12,810 Non-trainable params: 4,049,571 _________________________________________________________________ . Creating a ModelCheckpoint Callback . Callbacks are a took which can add helpful functionality to your models during training, evaluation or inference. | Some popular callbacks include: | . Callback name Use Case Code . TensorBoard | Log the performance of multiple models and then view and compare these models in a visual way on Tensor Board. Helpfule to compare teh results of different models on your data | tf.keras.callbacks.TensorBoard() | . Model Checkpointing | Save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can&#39;t be done in one sitting | tf.keras.callbacks.ModelCheckpoint() | . Early Stopping | Leave your model training for arbitray amount of time and have it stop training automatically when it ceases to improve. Helpful when you&#39;ve got a large dataset and don&#39;t know long training will take | tf.keras.callbacks.EarlyStopping() | . checkpoint_path = &quot;ten_percent_model_checkpoints_weights/checkpoint.ckpt&quot; # Create a modelcheckpoint callback that saves the model&#39;s weights only checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path, weights_only = True, save_best_only = False, save_freq= &quot;epoch&quot;, #save every epoch verbose = 1) . Fit Model 2 passing in the ModelCheckpoint callback . initial_epochs = 5 history_10_percent_data_aug = model_2.fit(train_data_10_percent, epochs = initial_epochs, validation_data= test_data, validation_steps = int(0.25* len(test_data)), callbacks = [create_tensorboard_callback(dir_name = &quot;transfer_learning&quot;, experiment_name = &quot;10_percent_data_aug&quot;), checkpoint_callback]) . Saving TensorBoard log files to: transfer_learning/10_percent_data_aug/20220220-062505 Epoch 1/5 24/24 [==============================] - ETA: 0s - loss: 2.0047 - accuracy: 0.3213 Epoch 1: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt INFO:tensorflow:Assets written to: ten_percent_model_checkpoints_weights/checkpoint.ckpt/assets 24/24 [==============================] - 81s 3s/step - loss: 2.0047 - accuracy: 0.3213 - val_loss: 1.4956 - val_accuracy: 0.6250 Epoch 2/5 24/24 [==============================] - ETA: 0s - loss: 1.3760 - accuracy: 0.6747 Epoch 2: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt INFO:tensorflow:Assets written to: ten_percent_model_checkpoints_weights/checkpoint.ckpt/assets 24/24 [==============================] - 93s 4s/step - loss: 1.3760 - accuracy: 0.6747 - val_loss: 1.0669 - val_accuracy: 0.7632 Epoch 3/5 24/24 [==============================] - ETA: 0s - loss: 1.0676 - accuracy: 0.7360 Epoch 3: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt INFO:tensorflow:Assets written to: ten_percent_model_checkpoints_weights/checkpoint.ckpt/assets 24/24 [==============================] - 69s 3s/step - loss: 1.0676 - accuracy: 0.7360 - val_loss: 0.8876 - val_accuracy: 0.7796 Epoch 4/5 24/24 [==============================] - ETA: 0s - loss: 0.9203 - accuracy: 0.7640 Epoch 4: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt INFO:tensorflow:Assets written to: ten_percent_model_checkpoints_weights/checkpoint.ckpt/assets 24/24 [==============================] - 66s 3s/step - loss: 0.9203 - accuracy: 0.7640 - val_loss: 0.7766 - val_accuracy: 0.8125 Epoch 5/5 24/24 [==============================] - ETA: 0s - loss: 0.8018 - accuracy: 0.7987 Epoch 5: saving model to ten_percent_model_checkpoints_weights/checkpoint.ckpt INFO:tensorflow:Assets written to: ten_percent_model_checkpoints_weights/checkpoint.ckpt/assets 24/24 [==============================] - 68s 3s/step - loss: 0.8018 - accuracy: 0.7987 - val_loss: 0.6841 - val_accuracy: 0.8174 . model_0.evaluate(test_data) . 79/79 [==============================] - 12s 133ms/step - loss: 0.6102 - accuracy: 0.8412 . [0.6101743578910828, 0.8411999940872192] . results_10_percent_data_aug = model_2.evaluate(test_data) results_10_percent_data_aug . 79/79 [==============================] - 11s 132ms/step - loss: 0.6865 - accuracy: 0.8160 . [0.6864515542984009, 0.8159999847412109] . plot_loss_curves(history_10_percent_data_aug) . Loading in checkpointed weights . Loading in checkpointed weights returns a model to a specific checkpoint . model_2.load_weights(checkpoint_path) . &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcd9fd4e290&gt; . loaded_weights_model_results = model_2.evaluate(test_data) . 79/79 [==============================] - 12s 140ms/step - loss: 0.6865 - accuracy: 0.8160 . results_10_percent_data_aug == loaded_weights_model_results . True . Model 3: Fine-tuning an existing model on 10% of the data . Note: Fine-tuning usually works best after training a feature extraction model with large amounts of custom data. . model_2.layers . [&lt;keras.engine.input_layer.InputLayer at 0x7fcda877ea10&gt;, &lt;keras.engine.sequential.Sequential at 0x7fcda8755810&gt;, &lt;keras.engine.functional.Functional at 0x7fcda82e9650&gt;, &lt;keras.layers.pooling.GlobalAveragePooling2D at 0x7fcda877fcd0&gt;, &lt;keras.layers.core.dense.Dense at 0x7fcda8231690&gt;] . for layer in model_2.layers: print(layer, layer.trainable) . # Checkout the trainable layers in our model (EfficientNetB0) for i, layer in enumerate(model_2.layers[2].layers): print(i, layer.name , layer.trainable) . 0 input_7 False 1 rescaling_4 False 2 normalization_4 False 3 stem_conv_pad False 4 stem_conv False 5 stem_bn False 6 stem_activation False 7 block1a_dwconv False 8 block1a_bn False 9 block1a_activation False 10 block1a_se_squeeze False 11 block1a_se_reshape False 12 block1a_se_reduce False 13 block1a_se_expand False 14 block1a_se_excite False 15 block1a_project_conv False 16 block1a_project_bn False 17 block2a_expand_conv False 18 block2a_expand_bn False 19 block2a_expand_activation False 20 block2a_dwconv_pad False 21 block2a_dwconv False 22 block2a_bn False 23 block2a_activation False 24 block2a_se_squeeze False 25 block2a_se_reshape False 26 block2a_se_reduce False 27 block2a_se_expand False 28 block2a_se_excite False 29 block2a_project_conv False 30 block2a_project_bn False 31 block2b_expand_conv False 32 block2b_expand_bn False 33 block2b_expand_activation False 34 block2b_dwconv False 35 block2b_bn False 36 block2b_activation False 37 block2b_se_squeeze False 38 block2b_se_reshape False 39 block2b_se_reduce False 40 block2b_se_expand False 41 block2b_se_excite False 42 block2b_project_conv False 43 block2b_project_bn False 44 block2b_drop False 45 block2b_add False 46 block3a_expand_conv False 47 block3a_expand_bn False 48 block3a_expand_activation False 49 block3a_dwconv_pad False 50 block3a_dwconv False 51 block3a_bn False 52 block3a_activation False 53 block3a_se_squeeze False 54 block3a_se_reshape False 55 block3a_se_reduce False 56 block3a_se_expand False 57 block3a_se_excite False 58 block3a_project_conv False 59 block3a_project_bn False 60 block3b_expand_conv False 61 block3b_expand_bn False 62 block3b_expand_activation False 63 block3b_dwconv False 64 block3b_bn False 65 block3b_activation False 66 block3b_se_squeeze False 67 block3b_se_reshape False 68 block3b_se_reduce False 69 block3b_se_expand False 70 block3b_se_excite False 71 block3b_project_conv False 72 block3b_project_bn False 73 block3b_drop False 74 block3b_add False 75 block4a_expand_conv False 76 block4a_expand_bn False 77 block4a_expand_activation False 78 block4a_dwconv_pad False 79 block4a_dwconv False 80 block4a_bn False 81 block4a_activation False 82 block4a_se_squeeze False 83 block4a_se_reshape False 84 block4a_se_reduce False 85 block4a_se_expand False 86 block4a_se_excite False 87 block4a_project_conv False 88 block4a_project_bn False 89 block4b_expand_conv False 90 block4b_expand_bn False 91 block4b_expand_activation False 92 block4b_dwconv False 93 block4b_bn False 94 block4b_activation False 95 block4b_se_squeeze False 96 block4b_se_reshape False 97 block4b_se_reduce False 98 block4b_se_expand False 99 block4b_se_excite False 100 block4b_project_conv False 101 block4b_project_bn False 102 block4b_drop False 103 block4b_add False 104 block4c_expand_conv False 105 block4c_expand_bn False 106 block4c_expand_activation False 107 block4c_dwconv False 108 block4c_bn False 109 block4c_activation False 110 block4c_se_squeeze False 111 block4c_se_reshape False 112 block4c_se_reduce False 113 block4c_se_expand False 114 block4c_se_excite False 115 block4c_project_conv False 116 block4c_project_bn False 117 block4c_drop False 118 block4c_add False 119 block5a_expand_conv False 120 block5a_expand_bn False 121 block5a_expand_activation False 122 block5a_dwconv False 123 block5a_bn False 124 block5a_activation False 125 block5a_se_squeeze False 126 block5a_se_reshape False 127 block5a_se_reduce False 128 block5a_se_expand False 129 block5a_se_excite False 130 block5a_project_conv False 131 block5a_project_bn False 132 block5b_expand_conv False 133 block5b_expand_bn False 134 block5b_expand_activation False 135 block5b_dwconv False 136 block5b_bn False 137 block5b_activation False 138 block5b_se_squeeze False 139 block5b_se_reshape False 140 block5b_se_reduce False 141 block5b_se_expand False 142 block5b_se_excite False 143 block5b_project_conv False 144 block5b_project_bn False 145 block5b_drop False 146 block5b_add False 147 block5c_expand_conv False 148 block5c_expand_bn False 149 block5c_expand_activation False 150 block5c_dwconv False 151 block5c_bn False 152 block5c_activation False 153 block5c_se_squeeze False 154 block5c_se_reshape False 155 block5c_se_reduce False 156 block5c_se_expand False 157 block5c_se_excite False 158 block5c_project_conv False 159 block5c_project_bn False 160 block5c_drop False 161 block5c_add False 162 block6a_expand_conv False 163 block6a_expand_bn False 164 block6a_expand_activation False 165 block6a_dwconv_pad False 166 block6a_dwconv False 167 block6a_bn False 168 block6a_activation False 169 block6a_se_squeeze False 170 block6a_se_reshape False 171 block6a_se_reduce False 172 block6a_se_expand False 173 block6a_se_excite False 174 block6a_project_conv False 175 block6a_project_bn False 176 block6b_expand_conv False 177 block6b_expand_bn False 178 block6b_expand_activation False 179 block6b_dwconv False 180 block6b_bn False 181 block6b_activation False 182 block6b_se_squeeze False 183 block6b_se_reshape False 184 block6b_se_reduce False 185 block6b_se_expand False 186 block6b_se_excite False 187 block6b_project_conv False 188 block6b_project_bn False 189 block6b_drop False 190 block6b_add False 191 block6c_expand_conv False 192 block6c_expand_bn False 193 block6c_expand_activation False 194 block6c_dwconv False 195 block6c_bn False 196 block6c_activation False 197 block6c_se_squeeze False 198 block6c_se_reshape False 199 block6c_se_reduce False 200 block6c_se_expand False 201 block6c_se_excite False 202 block6c_project_conv False 203 block6c_project_bn False 204 block6c_drop False 205 block6c_add False 206 block6d_expand_conv False 207 block6d_expand_bn False 208 block6d_expand_activation False 209 block6d_dwconv False 210 block6d_bn False 211 block6d_activation False 212 block6d_se_squeeze False 213 block6d_se_reshape False 214 block6d_se_reduce False 215 block6d_se_expand False 216 block6d_se_excite False 217 block6d_project_conv False 218 block6d_project_bn False 219 block6d_drop False 220 block6d_add False 221 block7a_expand_conv False 222 block7a_expand_bn False 223 block7a_expand_activation False 224 block7a_dwconv False 225 block7a_bn False 226 block7a_activation False 227 block7a_se_squeeze False 228 block7a_se_reshape False 229 block7a_se_reduce False 230 block7a_se_expand False 231 block7a_se_excite False 232 block7a_project_conv False 233 block7a_project_bn False 234 top_conv False 235 top_bn False 236 top_activation False . . print(len(model_2.layers[2].trainable_variables)) . 0 . Currently there are zero trainable variables in our base model . base_model.trainable = True # Freeze all layers except for the last 10 for layer in base_model.layers[:-10]: layer.trainable= False # Recompile the model (we have to recompile our model every time we make change) model_2.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.0001), # when fine-tuning you typically want to lower the lr by 10x metrics = [&quot;accuracy&quot;]) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Note: When using fine-tuning it&#39;s best practics to lower your learning rate. This is a hyperparamter you can tune. But a good rule of thumb is at least 10x (though different sources will claim other values) Resource: Universal Language Model Fine-tuning for Text Classification paper by Jeremy Howard and Sebastian Ruder. . # Check which layers are tunable(trainable) for layer_number, layer in enumerate(model_2.layers[2].layers): print(layer_number, layer.name, layer.trainable) . 0 input_7 False 1 rescaling_4 False 2 normalization_4 False 3 stem_conv_pad False 4 stem_conv False 5 stem_bn False 6 stem_activation False 7 block1a_dwconv False 8 block1a_bn False 9 block1a_activation False 10 block1a_se_squeeze False 11 block1a_se_reshape False 12 block1a_se_reduce False 13 block1a_se_expand False 14 block1a_se_excite False 15 block1a_project_conv False 16 block1a_project_bn False 17 block2a_expand_conv False 18 block2a_expand_bn False 19 block2a_expand_activation False 20 block2a_dwconv_pad False 21 block2a_dwconv False 22 block2a_bn False 23 block2a_activation False 24 block2a_se_squeeze False 25 block2a_se_reshape False 26 block2a_se_reduce False 27 block2a_se_expand False 28 block2a_se_excite False 29 block2a_project_conv False 30 block2a_project_bn False 31 block2b_expand_conv False 32 block2b_expand_bn False 33 block2b_expand_activation False 34 block2b_dwconv False 35 block2b_bn False 36 block2b_activation False 37 block2b_se_squeeze False 38 block2b_se_reshape False 39 block2b_se_reduce False 40 block2b_se_expand False 41 block2b_se_excite False 42 block2b_project_conv False 43 block2b_project_bn False 44 block2b_drop False 45 block2b_add False 46 block3a_expand_conv False 47 block3a_expand_bn False 48 block3a_expand_activation False 49 block3a_dwconv_pad False 50 block3a_dwconv False 51 block3a_bn False 52 block3a_activation False 53 block3a_se_squeeze False 54 block3a_se_reshape False 55 block3a_se_reduce False 56 block3a_se_expand False 57 block3a_se_excite False 58 block3a_project_conv False 59 block3a_project_bn False 60 block3b_expand_conv False 61 block3b_expand_bn False 62 block3b_expand_activation False 63 block3b_dwconv False 64 block3b_bn False 65 block3b_activation False 66 block3b_se_squeeze False 67 block3b_se_reshape False 68 block3b_se_reduce False 69 block3b_se_expand False 70 block3b_se_excite False 71 block3b_project_conv False 72 block3b_project_bn False 73 block3b_drop False 74 block3b_add False 75 block4a_expand_conv False 76 block4a_expand_bn False 77 block4a_expand_activation False 78 block4a_dwconv_pad False 79 block4a_dwconv False 80 block4a_bn False 81 block4a_activation False 82 block4a_se_squeeze False 83 block4a_se_reshape False 84 block4a_se_reduce False 85 block4a_se_expand False 86 block4a_se_excite False 87 block4a_project_conv False 88 block4a_project_bn False 89 block4b_expand_conv False 90 block4b_expand_bn False 91 block4b_expand_activation False 92 block4b_dwconv False 93 block4b_bn False 94 block4b_activation False 95 block4b_se_squeeze False 96 block4b_se_reshape False 97 block4b_se_reduce False 98 block4b_se_expand False 99 block4b_se_excite False 100 block4b_project_conv False 101 block4b_project_bn False 102 block4b_drop False 103 block4b_add False 104 block4c_expand_conv False 105 block4c_expand_bn False 106 block4c_expand_activation False 107 block4c_dwconv False 108 block4c_bn False 109 block4c_activation False 110 block4c_se_squeeze False 111 block4c_se_reshape False 112 block4c_se_reduce False 113 block4c_se_expand False 114 block4c_se_excite False 115 block4c_project_conv False 116 block4c_project_bn False 117 block4c_drop False 118 block4c_add False 119 block5a_expand_conv False 120 block5a_expand_bn False 121 block5a_expand_activation False 122 block5a_dwconv False 123 block5a_bn False 124 block5a_activation False 125 block5a_se_squeeze False 126 block5a_se_reshape False 127 block5a_se_reduce False 128 block5a_se_expand False 129 block5a_se_excite False 130 block5a_project_conv False 131 block5a_project_bn False 132 block5b_expand_conv False 133 block5b_expand_bn False 134 block5b_expand_activation False 135 block5b_dwconv False 136 block5b_bn False 137 block5b_activation False 138 block5b_se_squeeze False 139 block5b_se_reshape False 140 block5b_se_reduce False 141 block5b_se_expand False 142 block5b_se_excite False 143 block5b_project_conv False 144 block5b_project_bn False 145 block5b_drop False 146 block5b_add False 147 block5c_expand_conv False 148 block5c_expand_bn False 149 block5c_expand_activation False 150 block5c_dwconv False 151 block5c_bn False 152 block5c_activation False 153 block5c_se_squeeze False 154 block5c_se_reshape False 155 block5c_se_reduce False 156 block5c_se_expand False 157 block5c_se_excite False 158 block5c_project_conv False 159 block5c_project_bn False 160 block5c_drop False 161 block5c_add False 162 block6a_expand_conv False 163 block6a_expand_bn False 164 block6a_expand_activation False 165 block6a_dwconv_pad False 166 block6a_dwconv False 167 block6a_bn False 168 block6a_activation False 169 block6a_se_squeeze False 170 block6a_se_reshape False 171 block6a_se_reduce False 172 block6a_se_expand False 173 block6a_se_excite False 174 block6a_project_conv False 175 block6a_project_bn False 176 block6b_expand_conv False 177 block6b_expand_bn False 178 block6b_expand_activation False 179 block6b_dwconv False 180 block6b_bn False 181 block6b_activation False 182 block6b_se_squeeze False 183 block6b_se_reshape False 184 block6b_se_reduce False 185 block6b_se_expand False 186 block6b_se_excite False 187 block6b_project_conv False 188 block6b_project_bn False 189 block6b_drop False 190 block6b_add False 191 block6c_expand_conv False 192 block6c_expand_bn False 193 block6c_expand_activation False 194 block6c_dwconv False 195 block6c_bn False 196 block6c_activation False 197 block6c_se_squeeze False 198 block6c_se_reshape False 199 block6c_se_reduce False 200 block6c_se_expand False 201 block6c_se_excite False 202 block6c_project_conv False 203 block6c_project_bn False 204 block6c_drop False 205 block6c_add False 206 block6d_expand_conv False 207 block6d_expand_bn False 208 block6d_expand_activation False 209 block6d_dwconv False 210 block6d_bn False 211 block6d_activation False 212 block6d_se_squeeze False 213 block6d_se_reshape False 214 block6d_se_reduce False 215 block6d_se_expand False 216 block6d_se_excite False 217 block6d_project_conv False 218 block6d_project_bn False 219 block6d_drop False 220 block6d_add False 221 block7a_expand_conv False 222 block7a_expand_bn False 223 block7a_expand_activation False 224 block7a_dwconv False 225 block7a_bn False 226 block7a_activation False 227 block7a_se_squeeze True 228 block7a_se_reshape True 229 block7a_se_reduce True 230 block7a_se_expand True 231 block7a_se_excite True 232 block7a_project_conv True 233 block7a_project_bn True 234 top_conv True 235 top_bn True 236 top_activation True . . print(len(model_2.trainable_variables)) . 12 . fine_tune_epochs = initial_epochs +5 # Refit the mdoel(same as model_2 except with more trainable layers) history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent, epochs = fine_tune_epochs, validation_data = test_data, validation_steps = int(0.25* len(test_data)), initial_epoch = history_10_percent_data_aug.epoch[-1], #start training from prev last epoch callbacks = [create_tensorboard_callback(dir_name = &quot;transfer_learning&quot;, experiment_name = &quot;10_percent_fine_tune_last_10&quot;)]) . Saving TensorBoard log files to: transfer_learning/10_percent_fine_tune_last_10/20220220-071428 Epoch 5/10 24/24 [==============================] - 28s 734ms/step - loss: 0.6990 - accuracy: 0.8000 - val_loss: 0.5796 - val_accuracy: 0.8224 Epoch 6/10 24/24 [==============================] - 14s 556ms/step - loss: 0.5585 - accuracy: 0.8360 - val_loss: 0.5457 - val_accuracy: 0.8273 Epoch 7/10 24/24 [==============================] - 13s 531ms/step - loss: 0.5267 - accuracy: 0.8360 - val_loss: 0.5517 - val_accuracy: 0.8240 Epoch 8/10 24/24 [==============================] - 14s 586ms/step - loss: 0.4685 - accuracy: 0.8520 - val_loss: 0.5281 - val_accuracy: 0.8273 Epoch 9/10 24/24 [==============================] - 12s 493ms/step - loss: 0.3966 - accuracy: 0.8827 - val_loss: 0.5169 - val_accuracy: 0.8273 Epoch 10/10 24/24 [==============================] - 13s 514ms/step - loss: 0.3901 - accuracy: 0.8827 - val_loss: 0.5140 - val_accuracy: 0.8355 . results_fine_tune_10_percent = model_2.evaluate(test_data) . 79/79 [==============================] - 11s 136ms/step - loss: 0.4815 - accuracy: 0.8392 . plot_loss_curves(history_fine_10_percent_data_aug) . The plot_loss_curves function works great with models which have only been fit once, however, we want something to compare one series of running fit() with another(e.g. before and after fine-tuning) . def compare_history(original_history, new_history, initial_epochs = 5): &quot;&quot;&quot; Compares two TensorFlow History objects &quot;&quot;&quot; # Get original history measurements acc = original_history.history[&quot;accuracy&quot;] loss = original_history.history[&quot;loss&quot;] val_acc = original_history.history[&quot;val_accuracy&quot;] val_loss = original_history.history[&quot;val_loss&quot;] # Combine original history total_acc = acc + new_history.history[&quot;accuracy&quot;] total_loss = loss + new_history.history[&quot;loss&quot;] total_val_acc = val_acc + new_history.history[&quot;val_accuracy&quot;] total_val_loss = val_loss + new_history.history[&quot;val_loss&quot;] # Make plot for Accuracy plt.figure(figsize = (8,8)) plt.subplot(2,1,1) plt.plot(total_acc, label = &quot;Training Accuracy&quot;) plt.plot(total_val_acc, label = &quot;Val Accuracy&quot;) plt.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label = &quot;Start Fine tuning&quot;) plt.legend(loc = &quot;lower right&quot;) plt.title(&quot;Trainable and Validation Accuracy&quot;) # Make plot for Loss plt.figure(figsize = (8,8)) plt.subplot(2,1,2) plt.plot(total_loss, label = &quot;Training Loss&quot;) plt.plot(total_val_loss, label = &quot;Val Loss&quot;) plt.plot([initial_epochs-1, initial_epochs-1], plt.ylim(), label = &quot;Start Fine tuning&quot;) plt.legend(loc = &quot;upper right&quot;) plt.title(&quot;Trainable and Validation Accuracy&quot;) . compare_history(history_10_percent_data_aug, history_fine_10_percent_data_aug, initial_epochs = 5) . Model 4: Fine-tuning and existing model on the full dataset . !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip unzip_data(&quot;10_food_classes_all_data.zip&quot;) . --2022-02-20 07:57:52-- https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.136.128, 142.250.148.128, 108.177.112.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.136.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 519183241 (495M) [application/zip] Saving to: ‘10_food_classes_all_data.zip’ 10_food_classes_all 100%[===================&gt;] 495.13M 189MB/s in 2.6s 2022-02-20 07:57:55 (189 MB/s) - ‘10_food_classes_all_data.zip’ saved [519183241/519183241] . train_dir_all_data = &quot;10_food_classes_all_data/train&quot; test_dir = &quot;10_food_classes_all_data/test&quot; . walk_through_dir(&quot;10_food_classes_all_data&quot;) . There are 2 directories and 0 images in &#39;10_food_classes_all_data&#39;. There are 10 directories and 0 images in &#39;10_food_classes_all_data/train&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/pizza&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/fried_rice&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/grilled_salmon&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/hamburger&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/steak&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/sushi&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/chicken_wings&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/ramen&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/ice_cream&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/chicken_curry&#39;. There are 10 directories and 0 images in &#39;10_food_classes_all_data/test&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/pizza&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/hamburger&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/steak&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/sushi&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/chicken_wings&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/ramen&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/ice_cream&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/chicken_curry&#39;. . import tensorflow as tf IMG_SIZE = (224,224) train_data_10_classes_full = tf.keras.preprocessing.image_dataset_from_directory(train_dir_all_data, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE) test_data = tf.keras.preprocessing.image_dataset_from_directory(test_dir, label_mode = &quot;categorical&quot;, image_size = IMG_SIZE) . Found 7500 files belonging to 10 classes. Found 2500 files belonging to 10 classes. . The test dataset we have loaded in is the same as what we&#39;ve been using for previous experiments( all experiments have used the same test dataset). . Let&#39;s verify this... . model_2.evaluate(test_data) . 79/79 [==============================] - 12s 141ms/step - loss: 0.4815 - accuracy: 0.8392 . [0.48150259256362915, 0.8392000198364258] . results_fine_tune_10_percent . [0.48150259256362915, 0.8392000198364258] . To train a fine-tuning model (model_4) we need to revert model_2 back to its feature extraction weights . # the same stage the 10 percent data model was fine-tuned from model_2.load_weights(checkpoint_path) . &lt;tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcdab2c5950&gt; . model_2.evaluate(test_data) . 79/79 [==============================] - 12s 140ms/step - loss: 0.6865 - accuracy: 0.8160 . [0.6864517331123352, 0.8159999847412109] . results_10_percent_data_aug . [0.6864515542984009, 0.8159999847412109] . What we have done till now: . Trained a feature extraction transfer learning model for 5 epochs on 10% of the data with data augmentation (model_2) and we saved the model&#39;s weights using ModelCheckpoint callback. . | Fine-tuned the same model on the same 10% of the data for further 5 epochs with the top 10 layers of the base model unfrozen (model_3). . | Saved the results and training logs each time . | Reloaded the model from step 1 to do the same steps as step 2 except this time we are going to use all the data (model_4). . | for layer_number, layer in enumerate(model_2.layers): print(layer_number, layer.name, layer.trainable) . 0 input_layer True 1 data_augmentation True 2 efficientnetb0 True 3 global_average_pooling_layer True 4 output_layer True . # Let&#39;s drill into our base_model (EfficientNetB0) check what layers are trainable for layer_number, layer in enumerate(model_2.layers[2].layers): print(layer_number, layer.name, layer.trainable) . 0 input_7 False 1 rescaling_4 False 2 normalization_4 False 3 stem_conv_pad False 4 stem_conv False 5 stem_bn False 6 stem_activation False 7 block1a_dwconv False 8 block1a_bn False 9 block1a_activation False 10 block1a_se_squeeze False 11 block1a_se_reshape False 12 block1a_se_reduce False 13 block1a_se_expand False 14 block1a_se_excite False 15 block1a_project_conv False 16 block1a_project_bn False 17 block2a_expand_conv False 18 block2a_expand_bn False 19 block2a_expand_activation False 20 block2a_dwconv_pad False 21 block2a_dwconv False 22 block2a_bn False 23 block2a_activation False 24 block2a_se_squeeze False 25 block2a_se_reshape False 26 block2a_se_reduce False 27 block2a_se_expand False 28 block2a_se_excite False 29 block2a_project_conv False 30 block2a_project_bn False 31 block2b_expand_conv False 32 block2b_expand_bn False 33 block2b_expand_activation False 34 block2b_dwconv False 35 block2b_bn False 36 block2b_activation False 37 block2b_se_squeeze False 38 block2b_se_reshape False 39 block2b_se_reduce False 40 block2b_se_expand False 41 block2b_se_excite False 42 block2b_project_conv False 43 block2b_project_bn False 44 block2b_drop False 45 block2b_add False 46 block3a_expand_conv False 47 block3a_expand_bn False 48 block3a_expand_activation False 49 block3a_dwconv_pad False 50 block3a_dwconv False 51 block3a_bn False 52 block3a_activation False 53 block3a_se_squeeze False 54 block3a_se_reshape False 55 block3a_se_reduce False 56 block3a_se_expand False 57 block3a_se_excite False 58 block3a_project_conv False 59 block3a_project_bn False 60 block3b_expand_conv False 61 block3b_expand_bn False 62 block3b_expand_activation False 63 block3b_dwconv False 64 block3b_bn False 65 block3b_activation False 66 block3b_se_squeeze False 67 block3b_se_reshape False 68 block3b_se_reduce False 69 block3b_se_expand False 70 block3b_se_excite False 71 block3b_project_conv False 72 block3b_project_bn False 73 block3b_drop False 74 block3b_add False 75 block4a_expand_conv False 76 block4a_expand_bn False 77 block4a_expand_activation False 78 block4a_dwconv_pad False 79 block4a_dwconv False 80 block4a_bn False 81 block4a_activation False 82 block4a_se_squeeze False 83 block4a_se_reshape False 84 block4a_se_reduce False 85 block4a_se_expand False 86 block4a_se_excite False 87 block4a_project_conv False 88 block4a_project_bn False 89 block4b_expand_conv False 90 block4b_expand_bn False 91 block4b_expand_activation False 92 block4b_dwconv False 93 block4b_bn False 94 block4b_activation False 95 block4b_se_squeeze False 96 block4b_se_reshape False 97 block4b_se_reduce False 98 block4b_se_expand False 99 block4b_se_excite False 100 block4b_project_conv False 101 block4b_project_bn False 102 block4b_drop False 103 block4b_add False 104 block4c_expand_conv False 105 block4c_expand_bn False 106 block4c_expand_activation False 107 block4c_dwconv False 108 block4c_bn False 109 block4c_activation False 110 block4c_se_squeeze False 111 block4c_se_reshape False 112 block4c_se_reduce False 113 block4c_se_expand False 114 block4c_se_excite False 115 block4c_project_conv False 116 block4c_project_bn False 117 block4c_drop False 118 block4c_add False 119 block5a_expand_conv False 120 block5a_expand_bn False 121 block5a_expand_activation False 122 block5a_dwconv False 123 block5a_bn False 124 block5a_activation False 125 block5a_se_squeeze False 126 block5a_se_reshape False 127 block5a_se_reduce False 128 block5a_se_expand False 129 block5a_se_excite False 130 block5a_project_conv False 131 block5a_project_bn False 132 block5b_expand_conv False 133 block5b_expand_bn False 134 block5b_expand_activation False 135 block5b_dwconv False 136 block5b_bn False 137 block5b_activation False 138 block5b_se_squeeze False 139 block5b_se_reshape False 140 block5b_se_reduce False 141 block5b_se_expand False 142 block5b_se_excite False 143 block5b_project_conv False 144 block5b_project_bn False 145 block5b_drop False 146 block5b_add False 147 block5c_expand_conv False 148 block5c_expand_bn False 149 block5c_expand_activation False 150 block5c_dwconv False 151 block5c_bn False 152 block5c_activation False 153 block5c_se_squeeze False 154 block5c_se_reshape False 155 block5c_se_reduce False 156 block5c_se_expand False 157 block5c_se_excite False 158 block5c_project_conv False 159 block5c_project_bn False 160 block5c_drop False 161 block5c_add False 162 block6a_expand_conv False 163 block6a_expand_bn False 164 block6a_expand_activation False 165 block6a_dwconv_pad False 166 block6a_dwconv False 167 block6a_bn False 168 block6a_activation False 169 block6a_se_squeeze False 170 block6a_se_reshape False 171 block6a_se_reduce False 172 block6a_se_expand False 173 block6a_se_excite False 174 block6a_project_conv False 175 block6a_project_bn False 176 block6b_expand_conv False 177 block6b_expand_bn False 178 block6b_expand_activation False 179 block6b_dwconv False 180 block6b_bn False 181 block6b_activation False 182 block6b_se_squeeze False 183 block6b_se_reshape False 184 block6b_se_reduce False 185 block6b_se_expand False 186 block6b_se_excite False 187 block6b_project_conv False 188 block6b_project_bn False 189 block6b_drop False 190 block6b_add False 191 block6c_expand_conv False 192 block6c_expand_bn False 193 block6c_expand_activation False 194 block6c_dwconv False 195 block6c_bn False 196 block6c_activation False 197 block6c_se_squeeze False 198 block6c_se_reshape False 199 block6c_se_reduce False 200 block6c_se_expand False 201 block6c_se_excite False 202 block6c_project_conv False 203 block6c_project_bn False 204 block6c_drop False 205 block6c_add False 206 block6d_expand_conv False 207 block6d_expand_bn False 208 block6d_expand_activation False 209 block6d_dwconv False 210 block6d_bn False 211 block6d_activation False 212 block6d_se_squeeze False 213 block6d_se_reshape False 214 block6d_se_reduce False 215 block6d_se_expand False 216 block6d_se_excite False 217 block6d_project_conv False 218 block6d_project_bn False 219 block6d_drop False 220 block6d_add False 221 block7a_expand_conv False 222 block7a_expand_bn False 223 block7a_expand_activation False 224 block7a_dwconv False 225 block7a_bn False 226 block7a_activation False 227 block7a_se_squeeze True 228 block7a_se_reshape True 229 block7a_se_reduce True 230 block7a_se_expand True 231 block7a_se_excite True 232 block7a_project_conv True 233 block7a_project_bn True 234 top_conv True 235 top_bn True 236 top_activation True . . model_2.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.0001), metrics = [&quot;accuracy&quot;]) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . fine_tune_epochs = initial_epochs + 5 history_fine_10_classes_full = model_2.fit(train_data_10_classes_full, epochs = fine_tune_epochs, validation_data = test_data, validation_steps = int(0.25*len(test_data)), initial_epoch = history_10_percent_data_aug.epoch[-1], callbacks = [create_tensorboard_callback(dir_name = &quot;transfer_learning&quot;, experiment_name = &quot;full_10_classes_fine_tune_last_10&quot;)]) . Saving TensorBoard log files to: transfer_learning/full_10_classes_fine_tune_last_10/20220220-085835 Epoch 5/10 235/235 [==============================] - 97s 366ms/step - loss: 0.7321 - accuracy: 0.7609 - val_loss: 0.4013 - val_accuracy: 0.8635 Epoch 6/10 235/235 [==============================] - 79s 335ms/step - loss: 0.6010 - accuracy: 0.8061 - val_loss: 0.3560 - val_accuracy: 0.8882 Epoch 7/10 235/235 [==============================] - 75s 314ms/step - loss: 0.5271 - accuracy: 0.8305 - val_loss: 0.3115 - val_accuracy: 0.9013 Epoch 8/10 235/235 [==============================] - 70s 293ms/step - loss: 0.4879 - accuracy: 0.8433 - val_loss: 0.3053 - val_accuracy: 0.9013 Epoch 9/10 235/235 [==============================] - 64s 270ms/step - loss: 0.4517 - accuracy: 0.8539 - val_loss: 0.2934 - val_accuracy: 0.9161 Epoch 10/10 235/235 [==============================] - 63s 265ms/step - loss: 0.4240 - accuracy: 0.8677 - val_loss: 0.2869 - val_accuracy: 0.9194 . results_fine_tune_full_data = model_2.evaluate(test_data) results_fine_tune_full_data . 79/79 [==============================] - 11s 133ms/step - loss: 0.3047 - accuracy: 0.9052 . [0.3046596944332123, 0.9052000045776367] . compare_history(original_history= history_10_percent_data_aug, new_history = history_fine_10_classes_full, initial_epochs = 5) . Viewing our experiment data on TensorBoard . Note: Anything you upload to TensorBoard.dev is going to be public. So, if you have private data, do not upload. . # Upload TensorBoard dev records #!tensorboard dev upload --logdir ./transfer_learning # --name &quot;Transfer Learning Experiments with 10 FOOD101 classes&quot; # --description &quot; A series of different transfer learning experiments with varying amount of data&quot; # --one_shot # Exits the uploader once its finished uploading #Run the above line by taking of the comments for the last four lines to upload the experiments to tensorboard . My TensorBoard Experiments are avaliable at : https://tensorboard.dev/experiment/0ZGWuA1vTv2NAdhoRmATDw/#scalars . # !tensorboard dev list # TO delete a particular experiment #!tensorboard dev delete --experiment_id {type the id here} . References: . TensorFlow Developer Certificate in 2022: Zero to Mastery Udemy course | Zero to Mastery Deep Learning with TensorFlow GitHub Repo | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/tensorflow/transfer-learning/fine-tuning/2022/02/20/Transfer-Learning-in-TensorFlow-Fine-Tuning.html",
            "relUrl": "/deep%20learning/neuralnetworks/tensorflow/transfer-learning/fine-tuning/2022/02/20/Transfer-Learning-in-TensorFlow-Fine-Tuning.html",
            "date": " • Feb 20, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Transfer Learning with TensorFlow : Feature Extraction",
            "content": "Transfer Learning with TensorFlow : Feature Extraction . This Notebook is an account of my working for the Udemy course :TensorFlow Developer Certificate in 2022: Zero to Mastery. Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task. . It is a popular approach in deep learning where pre-trained models are used as the starting point on computer vision and natural language processing tasks given the vast compute and time resources required to develop neural network models on these problems and from the huge jumps in skill that they provide on related problems. . Concepts covered in this Notebook: . Introduce transfer learning with TensorFlow | Using a small dataset to experiment faster(10% of training samples) | Building a transfer learning feature extraction model with TensorFlow Hub. | Use TensorBoard to track modelling experiments and results | . Why use transfer learning? . Can leverage an existing neural network architecture proven to work on problems similar to our own | Can leverage a working network architecture which has already learned patterns on similar data to our own (often results in great ML products with less data) | . Example of tranfer learning use cases: . Computer Vision (using ImageNet for our Image classification problems) | Natural Language Processing (detecting spam mails - spam filter) | . Downloading and getting familiar with data . import zipfile # Download the data !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip # Unzip the downloaded file zip_ref = zipfile.ZipFile(&quot;10_food_classes_10_percent.zip&quot;) zip_ref.extractall() zip_ref.close() . --2022-02-18 07:42:56-- https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.5.128, 74.125.133.128, 74.125.140.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.5.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 168546183 (161M) [application/zip] Saving to: ‘10_food_classes_10_percent.zip’ 10_food_classes_10_ 100%[===================&gt;] 160.74M 60.3MB/s in 2.7s 2022-02-18 07:42:59 (60.3 MB/s) - ‘10_food_classes_10_percent.zip’ saved [168546183/168546183] . import os # Walk through 10 percent data dir and list number of files for dirpath, dirnames, filenames in os.walk(&quot;10_food_classes_10_percent)&quot;): print(f&quot;There are {len(dirnames)} directories and {len(filenames)} images in &#39;{dirpath}&#39;.&quot;) . Creating data loader (preparing the data) . We&#39;ll use the ImageDataGenerator class to load in our images in batches. . from tensorflow.keras.preprocessing.image import ImageDataGenerator IMAGE_SHAPE = (224,224) BATCH_SIZE = 32 train_dir = &quot;10_food_classes_10_percent/train&quot; test_dir = &quot;10_food_classes_10_percent/test&quot; train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale = 1/255.) print(&quot;Training images:&quot;) train_data_10_percent = train_datagen.flow_from_directory(train_dir, target_size = IMAGE_SHAPE, batch_size = BATCH_SIZE, class_mode = &quot;categorical&quot;) print(&quot;Testing Images:&quot;) test_data = test_datagen.flow_from_directory(test_dir, target_size = IMAGE_SHAPE, batch_size = 32, class_mode =&quot;categorical&quot;) . Training images: Found 750 images belonging to 10 classes. Testing Images: Found 2500 images belonging to 10 classes. . Setting up callbacks (things to run whilst our model trains) . Callbacks are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks: . Tracking experiments with the TensorBoard callback | Model checkpoint with the ModelCheckpoint callback | Stopping a model from training (before it trains too long and overfits) with the EarlyStopping callback | . Some popular callbacks include: . Callback name Use case Code . TensorBoard | Log the performance of multiple models and then view and compare these models in a visual way on TensorBoard (a dashboard for inspecting neural network parameters). Helpful to compare the results of different models on your data | tf.keras.callbacks.TensorBoard() | . Model checkpointing | Save your model as it trains so you can stop training if needed and come back to continue off where you left. Helpful if training takes a long time and can&#39;t be done in one sitting | tf.keras.callbacks.ModelCheckpoint() | . Early Stopping | Leave your model training for an arbitary amount of time and have it stop training automatically when it ceases to improve. Helpful when you&#39;ve got a large dataset and don&#39;t know how long training will take | tf.keras.callbacks.EarlyStopping() | . import datetime import tensorflow as tf def create_tensorboard_callback(dir_name, experiment_name): log_dir = dir_name + &quot;/&quot; + experiment_name + &quot;/&quot; + datetime.datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;) tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_dir) print(f&quot;Saving TensorBoard log files to : {log_dir}&quot;) return tensorboard_callback . Note: You can customize the directory where your TensorBoard logs (model training metrics) get saved to whatever you like. The log_dir parameter we&#39;ve created above is only one option. . Creating models using Tensorflow Hub . TensorFlow Hub is the repository of pre-trained machine learning models created with tensorflow. . We are going to do a similar process, except the majority of our model&#39;s layers are going to come from TensorFlow Hub. . TensorFlow Hub . efficientnet_url = &quot;https://tfhub.dev/google/efficientnet/b0/classification/1&quot; resnet_url = &quot;https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5&quot; . import tensorflow as tf import tensorflow_hub as hub from tensorflow.keras import layers . def create_model(model_url, num_classes = 10): &quot;&quot;&quot; Takes a TensorFlow Hub URL and creates a Keras Sequential model with it Args: model_url(str): A TensorFlow Hub features extraction URL. num_classes(int) : Number of output neurons in the output layer, should be equal to number of target classes, default 10. Returns: An uncompiled Keras Sequential model with model_url as feature extractor layers and Dense output layers with num_classes output neurons. &quot;&quot;&quot; # Download the pretrained model and save it feature_extractor_layer = hub.KerasLayer(model_url, trainable =False, name = &quot;feature_extraction_layer&quot;, input_shape = IMAGE_SHAPE+(3,)) # Freeze the already learned parameters # Create our model model = tf.keras.Sequential([ feature_extractor_layer, layers.Dense(num_classes, activation = &quot;softmax&quot;, name = &quot;output_layer&quot;) ]) return model . Creating and testing ResNet TensorFlow Hub Feature Extraction model . resnet_model = create_model(resnet_url, num_classes=train_data_10_percent.num_classes) . resnet_model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) . resnet_model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= feature_extraction_layer (K (None, 2048) 23564800 erasLayer) output_layer (Dense) (None, 10) 20490 ================================================================= Total params: 23,585,290 Trainable params: 20,490 Non-trainable params: 23,564,800 _________________________________________________________________ . whoaa..the number of parameters are ~23 Million. But only ~20,000 are trainable parameters other are from the freeze model we uploaded from the tensorflow hub. Here&#39;s look at the Resnet50 architecture: . What our current model looks like. A ResNet50V2 backbone with a custom dense layer on top (10 classes instead of 1000 ImageNet classes). Note: The Image shows ResNet34 instead of ResNet50. Image source: https://arxiv.org/abs/1512.03385. . resnet_history = resnet_model.fit(train_data_10_percent, epochs = 5, steps_per_epoch =len(train_data_10_percent), validation_data = test_data, validation_steps = len(test_data), callbacks = [create_tensorboard_callback(dir_name=&quot;tensorflow_hub&quot;, experiment_name =&quot;resnet50V2&quot;)]) . Saving TensorBoard log files to : tensorflow_hub/resnet50V2/20220218-074315 Epoch 1/5 24/24 [==============================] - 39s 1s/step - loss: 1.9621 - accuracy: 0.3333 - val_loss: 1.2971 - val_accuracy: 0.5844 Epoch 2/5 24/24 [==============================] - 26s 1s/step - loss: 0.9486 - accuracy: 0.7053 - val_loss: 0.8931 - val_accuracy: 0.7084 Epoch 3/5 24/24 [==============================] - 26s 1s/step - loss: 0.6548 - accuracy: 0.8067 - val_loss: 0.7746 - val_accuracy: 0.7440 Epoch 4/5 24/24 [==============================] - 21s 905ms/step - loss: 0.5017 - accuracy: 0.8707 - val_loss: 0.7280 - val_accuracy: 0.7560 Epoch 5/5 24/24 [==============================] - 21s 908ms/step - loss: 0.4017 - accuracy: 0.9133 - val_loss: 0.6811 - val_accuracy: 0.7732 . Our transfer learning feature extractor model outperformed all of the previous models we built by hand. We have achieved around ~91% accuracy!! with only 10% of the data. we have a validation accuracy ~77% . # we can put a function like this into a script called &quot;helper.py and import &quot; import matplotlib.pyplot as plt plt.style.use(&#39;dark_background&#39;) # set dark background for plots def plot_loss_curves(history): &quot;&quot;&quot; Returns training and validation metrics Args: history : TensorFlow History object. Returns: plots of training/validation loss and accuracy metrics &quot;&quot;&quot; loss = history.history[&quot;loss&quot;] val_loss = history.history[&quot;val_loss&quot;] accuracy = history.history[&quot;accuracy&quot;] val_accuracy = history.history[&quot;val_accuracy&quot;] epochs = range(len(history.history[&quot;loss&quot;])) # Plot loss plt.plot(epochs, loss, label = &quot;training_loss&quot;) plt.plot(epochs, val_loss, label = &quot;validation_loss&quot;) plt.title(&quot;Loss&quot;) plt.xlabel(&quot;Epochs&quot;) plt.legend() # Plot Accuracy plt.figure() plt.plot(epochs, accuracy, label =&quot;training_accuracy&quot;) plt.plot(epochs, val_accuracy, label = &quot;validation_accuracy&quot;) plt.title(&quot;Accuracy&quot;) plt.xlabel(&quot;Accuracy&quot;) plt.legend(); . plot_loss_curves(resnet_history) . Creating and testing EfficientNetB0 TensorFlow Hub Feature Extraction model . efficientnet_model = create_model(model_url=efficientnet_url, num_classes=train_data_10_percent.num_classes) # Compile the EfficientNet model efficientnet_model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit EfficientNet model to 10% of training data efficientnet_history = efficientnet_model.fit(train_data_10_percent, epochs =5, steps_per_epoch =len(train_data_10_percent), validation_data = test_data, validation_steps = len(test_data), callbacks = [create_tensorboard_callback(dir_name =&quot;tensorflow_hub&quot;, experiment_name=&quot;efficientnetb0&quot;)]) . Saving TensorBoard log files to : tensorflow_hub/efficientnetb0/20220218-075921 Epoch 1/5 24/24 [==============================] - 26s 902ms/step - loss: 1.8412 - accuracy: 0.4000 - val_loss: 0.9458 - val_accuracy: 0.7020 Epoch 2/5 24/24 [==============================] - 19s 820ms/step - loss: 0.7660 - accuracy: 0.7600 - val_loss: 0.6298 - val_accuracy: 0.7928 Epoch 3/5 24/24 [==============================] - 20s 847ms/step - loss: 0.5194 - accuracy: 0.8480 - val_loss: 0.5491 - val_accuracy: 0.8240 Epoch 4/5 24/24 [==============================] - 19s 836ms/step - loss: 0.3962 - accuracy: 0.8907 - val_loss: 0.5118 - val_accuracy: 0.8316 Epoch 5/5 24/24 [==============================] - 19s 834ms/step - loss: 0.3268 - accuracy: 0.9187 - val_loss: 0.4939 - val_accuracy: 0.8420 . We have done really well with EfficientNetb0 model. we have training accuracy ~91% and validation accuracy ~84%. . plot_loss_curves(efficientnet_history) . efficientnet_model.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= feature_extraction_layer (K (None, 1000) 5330564 erasLayer) output_layer (Dense) (None, 10) 10010 ================================================================= Total params: 5,340,574 Trainable params: 10,010 Non-trainable params: 5,330,564 _________________________________________________________________ . resnet_model.summary() . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= feature_extraction_layer (K (None, 2048) 23564800 erasLayer) output_layer (Dense) (None, 10) 20490 ================================================================= Total params: 23,585,290 Trainable params: 20,490 Non-trainable params: 23,564,800 _________________________________________________________________ . The EfficientNet architecture performs really good even though the number of parameters in the model are far less than ResNet model. . len(efficientnet_model.layers[0].weights) . 311 . our Neural network learns these weights/parameters for extracting and generalizing features for better prediction on new data . Different types of transfer learning . &quot;As is&quot; transfer learning - using an existing model with no changes(eg. using ImageNet model on 1000 ImageNet classes, none of your own) | &quot;Feature extraction&quot; transfer learning - use the pre-learned patterns of an existing model (eg. EffiientNetB0 trained on ImageNet) and adjust the output layer for your own problem(eg. 1000 classes -&gt; 10 classes of food) | &quot;Fine tuning&quot; transfer learning - use the prelearned patterns of an existing model and &quot;fine-tune&quot; many or all of the underlying layers(including new output layers) | . The different kinds of transfer learning. An original model, a feature extraction model (only top 2-3 layers change) and a fine-tuning model (many or all of original model get changed). . Comparing our model&#39;s results using TensorBoard . What is TensorBoard? . A way to visually explore your machine learning models performance and internals. | Host, track and share your machine learning experiments on TensorBoard.dev Note: When you upload things to TensorBoard.dev, your experiments are public. So, if you&#39;re running private experiments (things you don&#39;t want other to see) do not upload them to TensorBoard.dev . | . # Upload TensorBoard dev records !tensorboard dev upload --logdir ./tensorflow_hub/ --name &quot;EfficientNetB0 vs. ResNet50V2&quot; --description &quot;Comparing two different TF Hub Feature extraction model architectures using 10% of the training data&quot; --one_shot . !tensorboard dev list . https://tensorboard.dev/experiment/1OBT56EnQ7GbJSOdsOKjZg/ Name EfficientNetB0 vs. ResNet50V2 Description Comparing two different TF Hub Feature extraction model architectures using 10% of the training data Id 1OBT56EnQ7GbJSOdsOKjZg Created 2022-02-18 08:32:35 (5 minutes ago) Updated 2022-02-18 08:32:38 (5 minutes ago) Runs 4 Tags 5 Scalars 60 Tensor bytes 0 Binary object bytes 3145194 Total: 1 experiment(s) . # !tensorboard dev delete --experiment_id [copy your id here] . Weights&amp;Biases also integreates with TensorFlow so we can use that as visualization tool. . References: . weights&amp;bisases | TensorBoard experiment | TensorFlow Developer Certificate in 2022: Zero to Mastery | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deep%20learning/neuralnetworks/tensorflow/transfer-learning/feature-extraction/2022/02/18/Transfer-Learning-in-TensorFlow-Feature-Extraction.html",
            "relUrl": "/deep%20learning/neuralnetworks/tensorflow/transfer-learning/feature-extraction/2022/02/18/Transfer-Learning-in-TensorFlow-Feature-Extraction.html",
            "date": " • Feb 18, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Convolutional Neural Networks and Computer Vision with TensorFlow",
            "content": "Convolutional Neural Networks and Computer Vision with TensorFlow . This Notebook is an account of working for the Udemy course by Daniel Bourke: TensorFlow Developer Certificate in 2022: Zero to Mastery . Concepts covered in this Notebook: . Getting a dataset to work with | Architecture of a convolutional neural network(CNN) with TensorFlow. | An end-to-end binary image classification problem | Steps in modelling with CNNs: Creating a CNN | Compiling a model | Fitting a Model | evaluating a model | . | An end-to-end mutli-class image classification problem | Making predictions on our own custom images. | . Introduction to CNNs and Computer vision with TensorFlow . Computer vision is the practice of writing algorithms which can discover patterns in visual data. Such as the camera of a self-driving car, recognizing the car infront. . Architecture of a CNN . Hyperparameter/Layer type What does it do? Typical Values . Input image(s) | Target images you&#39;d like to discover patterns in | Whatever you can take a photo or video of | . Input layer | Takes in target images and preprocesses them for further layers | input_shape = [batch_size, image_height, image_width, color_channels] | . Convolution layer | Extracts/learns the most important features from target images | Multiple, can create with tf.keras.layers.ConvXD (X can be multiple values | . Hidden activation | Adds non-linearity to learned features(non-straight lines) | Usually ReLU (`tf.keras.activations.relu) | . Pooling layer | Reduces the dimensionality of learned image features | Average(tf.keras.layers.AvgPool2D) or Max(tf.keras.layers.MaxPool2D) | . Fully Connected layer | Further refines learned features from convolution layers | tf.keras.layers.Dense | . Output layer | Takes learned features and outputs them in shape of target labels | output_shape = [number_of_classes] (e.g. 3 for pizza, steack or sushi) | . Output activation | Adds non-linearity to output layer | tf.keras.activations.sigmoid (binary classification) or tf.keras.activations.softmax (multi-class classification) | . An example of a CNN model in TensorFlow: . # 1. Create a CNN Model (same as Tiny VGG model) cnn_model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(filters=10, kernel_size = 3, # can also be (3,3) activation = &#39;relu&#39;, input_shape = (224,224,3)), # Specify the input shape(height, width, colour channels) tf.keras.layers.Conv2D(10,3, activation = &quot;relu&quot;), tf.keras.layers.MaxPool2D(pool_size= 2, # pool_size can be (2,2) padding = &quot;valid&quot;), # Padding can also be &#39;same&#39; tf.keras.layers.Conv2D(10,3, activation = &quot;relu&quot;), tf.keras.layers.Conv2D(10,3, activation = &quot;relu&quot;), # activation = &#39;relu&#39; == tf.keras.layers.Activaions(tf.nn.relu) tf.keras.MaxPool2D(2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) # Binary activation output ]) # 2. Compile the model cnn_model.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history = cnn_model.fit(train_data, epochs = 5) . Get the data . The images we&#39;re working with are from the Food101 dataset (101 different classes of food): kaggle Food101 Dataset . However we&#39;ve modified it to only use two classes (pizza &amp; steak) using the image_data_modification Notebook . Note: We start with smaller dataset so we can experiment quickly and figure what works and what doesn&#39;t work before scaling up. . import zipfile !wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip # Unzip the downloaded file zip_ref = zipfile.ZipFile(&quot;pizza_steak.zip&quot;) zip_ref.extractall() zip_ref.close() . --2022-02-17 16:13:56-- https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.125.128, 142.250.157.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 109540975 (104M) [application/zip] Saving to: ‘pizza_steak.zip’ pizza_steak.zip 100%[===================&gt;] 104.47M 152MB/s in 0.7s 2022-02-17 16:13:57 (152 MB/s) - ‘pizza_steak.zip’ saved [109540975/109540975] . Inspect the data . !ls pizza_steak . test train . !ls pizza_steak/train . pizza steak . # Let&#39;s check what is inside the pizza training data !ls pizza_steak/train/pizza . 1008104.jpg 1638227.jpg 2235981.jpg 2774899.jpg 3464027.jpg 576236.jpg 1008144.jpg 1646974.jpg 2236914.jpg 2775763.jpg 3464858.jpg 5764.jpg 1008844.jpg 1649108.jpg 2241448.jpg 2778214.jpg 3473991.jpg 579691.jpg 1008941.jpg 1649276.jpg 2247711.jpg 277963.jpg 3475936.jpg 593400.jpg 1011404.jpg 1652943.jpg 2253670.jpg 2785084.jpg 3478964.jpg 59445.jpg 102037.jpg 1654444.jpg 2255361.jpg 2793535.jpg 3479875.jpg 596494.jpg 1026922.jpg 1660415.jpg 2274117.jpg 27963.jpg 3479936.jpg 598381.jpg 1029698.jpg 1663749.jpg 2279642.jpg 2800325.jpg 3484590.jpg 604977.jpg 1033251.jpg 1665654.jpg 2280345.jpg 2811032.jpg 3493457.jpg 608085.jpg 1035854.jpg 166823.jpg 2285269.jpg 282013.jpg 349946.jpg 618021.jpg 1038357.jpg 1670471.jpg 2285942.jpg 2821034.jpg 350358.jpg 61822.jpg 1040878.jpg 1671531.jpg 228778.jpg 2821048.jpg 3505182.jpg 618348.jpg 1044524.jpg 1678284.jpg 2291093.jpg 2827938.jpg 3512070.jpg 625687.jpg 1044789.jpg 1681043.jpg 2292986.jpg 2831983.jpg 3514408.jpg 626170.jpg 1047561.jpg 1686908.jpg 2293453.jpg 2844278.jpg 352051.jpg 626902.jpg 1048649.jpg 1687681.jpg 2301105.jpg 2849924.jpg 3530210.jpg 63480.jpg 1054420.jpg 168879.jpg 2304021.jpg 2852301.jpg 3536393.jpg 647215.jpg 1055065.jpg 1688838.jpg 2312987.jpg 2855844.jpg 3546278.jpg 652004.jpg 105910.jpg 169318.jpg 23199.jpg 287000.jpg 3549765.jpg 656817.jpg 1065078.jpg 169720.jpg 2321465.jpg 2877565.jpg 3550805.jpg 662526.jpg 1069629.jpg 1705747.jpg 232976.jpg 2881282.jpg 3554287.jpg 663285.jpg 1075568.jpg 1705773.jpg 2330965.jpg 2885050.jpg 3555299.jpg 665900.jpg 1076699.jpg 1708197.jpg 233143.jpg 2885796.jpg 3557127.jpg 667309.jpg 1083380.jpg 1717790.jpg 2331467.jpg 2902766.jpg 3574192.jpg 668944.jpg 1084888.jpg 1742542.jpg 2361973.jpg 2916034.jpg 358178.jpg 670201.jpg 1088332.jpg 1743389.jpg 2365046.jpg 2922019.jpg 3589437.jpg 674188.jpg 1089334.jpg 175626.jpg 2382016.jpg 2924941.jpg 3595758.jpg 676432.jpg 1097980.jpg 1757288.jpg 238843.jpg 2933332.jpg 3597955.jpg 682201.jpg 1098197.jpg 1761451.jpg 2397868.jpg 29417.jpg 3614525.jpg 68684.jpg 1105700.jpg 1763205.jpg 2410138.jpg 2951831.jpg 3628930.jpg 698251.jpg 1107714.jpg 1774438.jpg 2412237.jpg 2952219.jpg 3629996.jpg 702165.jpg 111051.jpg 1778167.jpg 2412970.jpg 2959665.jpg 3644733.jpg 704161.jpg 1110966.jpg 1786840.jpg 2421445.jpg 2965.jpg 3653528.jpg 709273.jpg 1123386.jpg 179165.jpg 2426686.jpg 2967846.jpg 3653643.jpg 709947.jpg 1137400.jpg 1795316.jpg 2428085.jpg 2980131.jpg 3660716.jpg 712149.jpg 1138936.jpg 1806491.jpg 242813.jpg 2989328.jpg 3663580.jpg 714991.jpg 1143057.jpg 1810844.jpg 2432061.jpg 2990023.jpg 3675128.jpg 715169.jpg 1157438.jpg 1818014.jpg 2439992.jpg 2990186.jpg 3678290.jpg 717350.jpg 1159797.jpg 1828050.jpg 2441328.jpg 2992084.jpg 368644.jpg 721383.jpg 1165451.jpg 1836888.jpg 2443498.jpg 299535.jpg 369017.jpg 724445.jpg 1173913.jpg 1839077.jpg 244505.jpg 2995731.jpg 3693710.jpg 72716.jpg 1183278.jpg 1870865.jpg 2448844.jpg 2999507.jpg 3699992.jpg 739735.jpg 1202925.jpg 1871498.jpg 2451169.jpg 3000535.jpg 3702863.jpg 740385.jpg 1205154.jpg 1877103.jpg 2456207.jpg 300869.jpg 3703769.jpg 741491.jpg 1207213.jpg 1878005.jpg 2462190.jpg 3018077.jpg 3704879.jpg 741883.jpg 1209973.jpg 1881674.jpg 2467990.jpg 3020376.jpg 3705479.jpg 755968.jpg 121834.jpg 1888911.jpg 2468499.jpg 3023774.jpg 370643.jpg 759025.jpg 12301.jpg 1890444.jpg 2470671.jpg 302591.jpg 3712344.jpg 759873.jpg 1234172.jpg 1895479.jpg 2471646.jpg 3039549.jpg 3713343.jpg 762788.jpg 1243215.jpg 1897129.jpg 2473559.jpg 3042454.jpg 372275.jpg 764429.jpg 1245628.jpg 1898723.jpg 2476468.jpg 3055697.jpg 3742272.jpg 765000.jpg 1247645.jpg 1899562.jpg 2481333.jpg 3057192.jpg 3745884.jpg 765799.jpg 1248346.jpg 1899785.jpg 248252.jpg 3063955.jpg 3749515.jpg 768276.jpg 1248478.jpg 1900585.jpg 2486277.jpg 3066951.jpg 375401.jpg 771878.jpg 1260554.jpg 1906287.jpg 2487039.jpg 307677.jpg 3763593.jpg 774142.jpg 1267359.jpg 1907713.jpg 2490163.jpg 3082068.jpg 376417.jpg 77677.jpg 1269960.jpg 1912976.jpg 2491110.jpg 3082443.jpg 3766053.jpg 786995.jpg 1270986.jpg 1914969.jpg 2492287.jpg 3084957.jpg 3766476.jpg 790432.jpg 12718.jpg 1915343.jpg 2493954.jpg 3102271.jpg 3767723.jpg 790841.jpg 1284978.jpg 1916846.jpg 2501636.jpg 3105724.jpg 3767773.jpg 792093.jpg 1285298.jpg 1934355.jpg 2501961.jpg 3109486.jpg 3772054.jpg 799874.jpg 1287004.jpg 1944600.jpg 2502234.jpg 312479.jpg 3778801.jpg 803243.jpg 1289139.jpg 1947572.jpg 2508157.jpg 3128495.jpg 3790235.jpg 807128.jpg 129536.jpg 1950499.jpg 2511911.jpg 3148119.jpg 3793314.jpg 809024.jpg 1312761.jpg 1951130.jpg 2516510.jpg 3164761.jpg 3798959.jpg 812349.jpg 131561.jpg 1958364.jpg 2519291.jpg 3168266.jpg 3803596.jpg 816577.jpg 132484.jpg 1964051.jpg 2529205.jpg 3170114.jpg 3821701.jpg 816729.jpg 132554.jpg 1968947.jpg 253127.jpg 3173779.jpg 3822139.jpg 819547.jpg 1325918.jpg 1973447.jpg 2534774.jpg 317861.jpg 3826377.jpg 823104.jpg 1326065.jpg 1980167.jpg 2556273.jpg 3185774.jpg 382829.jpg 82578.jpg 1327402.jpg 1981348.jpg 2557340.jpg 3191035.jpg 3830773.jpg 82772.jpg 1336882.jpg 1984976.jpg 2560539.jpg 3193599.jpg 38349.jpg 829229.jpg 134462.jpg 1987634.jpg 2569760.jpg 3196721.jpg 384215.jpg 83538.jpg 1344966.jpg 1988629.jpg 2570329.jpg 32004.jpg 3845083.jpg 835833.jpg 1351146.jpg 199019.jpg 2572958.jpg 320570.jpg 3860002.jpg 839461.jpg 1351631.jpg 1998483.jpg 2574896.jpg 3207504.jpg 3862243.jpg 853441.jpg 1370319.jpg 2005870.jpg 2576168.jpg 3214153.jpg 3864383.jpg 857888.jpg 1383291.jpg 2010437.jpg 2577373.jpg 3256974.jpg 3871666.jpg 861771.jpg 1384464.jpg 2014717.jpg 2577377.jpg 3264148.jpg 3873326.jpg 866834.jpg 138855.jpg 2019441.jpg 2581276.jpg 32666.jpg 387697.jpg 868789.jpg 1390308.jpg 2019583.jpg 2584745.jpg 326809.jpg 3882444.jpg 869763.jpg 13983.jpg 2021516.jpg 2587918.jpg 3269634.jpg 3906901.jpg 872094.jpg 1399531.jpg 2026009.jpg 2587921.jpg 327415.jpg 3910117.jpg 874288.jpg 140031.jpg 2032236.jpg 259449.jpg 3281494.jpg 3913912.jpg 875262.jpg 1403878.jpg 2035248.jpg 2602611.jpg 329302.jpg 3917951.jpg 875856.jpg 1407753.jpg 203831.jpg 2605343.jpg 3297714.jpg 393658.jpg 877881.jpg 1412034.jpg 2044732.jpg 2606727.jpg 3312584.jpg 394049.jpg 878377.jpg 1413289.jpg 2077999.jpg 262133.jpg 3314176.jpg 394590.jpg 884986.jpg 141507.jpg 2078141.jpg 2621534.jpg 3314535.jpg 395034.jpg 886505.jpg 1423515.jpg 2078208.jpg 2622336.jpg 332231.jpg 395960.jpg 8917.jpg 1425089.jpg 2091857.jpg 2639094.jpg 3324050.jpg 398565.jpg 893644.jpg 1426781.jpg 2097315.jpg 2640502.jpg 3326344.jpg 401701.jpg 896448.jpg 143453.jpg 2098014.jpg 2664219.jpg 3333459.jpg 401979.jpg 898119.jpg 1454995.jpg 2104569.jpg 2666066.jpg 3337370.jpg 40231.jpg 898303.jpg 1468795.jpg 2110257.jpg 2667244.jpg 3338774.jpg 403431.jpg 898843.jpg 147785.jpg 2112757.jpg 2667255.jpg 333985.jpg 413710.jpg 89892.jpg 1499661.jpg 2121603.jpg 2667824.jpg 3342039.jpg 413789.jpg 899818.jpg 1504421.jpg 2126352.jpg 2670730.jpg 3366256.jpg 419516.jpg 899959.jpg 1504719.jpg 2126709.jpg 2671508.jpg 3367113.jpg 422261.jpg 904938.jpg 1507039.jpg 2135635.jpg 2674351.jpg 337272.jpg 424288.jpg 910419.jpg 1512514.jpg 2137341.jpg 2687575.jpg 3376519.jpg 430118.jpg 917774.jpg 1524599.jpg 2142812.jpg 2693334.jpg 3379038.jpg 44449.jpg 918506.jpg 1524655.jpg 214728.jpg 269396.jpg 3382880.jpg 448519.jpg 920219.jpg 1535273.jpg 2148129.jpg 2694223.jpg 3383977.jpg 452989.jpg 920595.jpg 1544197.jpg 2154394.jpg 2697971.jpg 3384856.jpg 464388.jpg 926046.jpg 1552253.jpg 2155475.jpg 2700543.jpg 338838.jpg 465454.jpg 928663.jpg 1553353.jpg 2155735.jpg 2702825.jpg 3391208.jpg 467986.jpg 928670.jpg 1571074.jpg 2161241.jpg 2705497.jpg 3392649.jpg 474493.jpg 929067.jpg 1572608.jpg 2164255.jpg 2707814.jpg 3392671.jpg 489347.jpg 937915.jpg 1573562.jpg 216720.jpg 2711828.jpg 3393898.jpg 489532.jpg 938821.jpg 1576248.jpg 2172850.jpg 271592.jpg 3397336.jpg 495892.jpg 93961.jpg 1577871.jpg 218142.jpg 271675.jpg 3398309.jpg 513754.jpg 947246.jpg 1584379.jpg 218711.jpg 271779.jpg 3399610.jpg 514014.jpg 951953.jpg 1593665.jpg 2187466.jpg 2722646.jpg 3401720.jpg 517902.jpg 959901.jpg 1593835.jpg 2188452.jpg 2723529.jpg 3401767.jpg 518527.jpg 962785.jpg 1600705.jpg 2190018.jpg 2739039.jpg 340814.jpg 527199.jpg 966644.jpg 1608000.jpg 220190.jpg 2739100.jpg 3425999.jpg 52934.jpg 967694.jpg 1617418.jpg 220910.jpg 2742044.jpg 3426946.jpg 532970.jpg 970073.jpg 1620560.jpg 221048.jpg 274945.jpg 3427699.jpg 543556.jpg 972000.jpg 1620761.jpg 2215531.jpg 2754150.jpg 3434372.jpg 54461.jpg 976382.jpg 1625147.jpg 2217956.jpg 2755875.jpg 3441394.jpg 54540.jpg 979955.jpg 163039.jpg 2224099.jpg 2757327.jpg 3443136.jpg 545561.jpg 979998.jpg 1633289.jpg 2224828.jpg 2760984.jpg 3456440.jpg 56449.jpg 985164.jpg 1635386.jpg 2228322.jpg 276803.jpg 3462250.jpg 568383.jpg 98617.jpg 1636299.jpg 2231356.jpg 2769168.jpg 34632.jpg 568995.jpg 998719.jpg . . import os # Walk through pizza_steak directory and list number of files for dirpath, dirnames, filenames, in os.walk(&quot;pizza_steak&quot;): print(f&quot;There are {len(dirnames)} directories and {len(filenames)} images in {dirpath}&quot;) . There are 2 directories and 0 images in pizza_steak There are 2 directories and 0 images in pizza_steak/test There are 0 directories and 250 images in pizza_steak/test/steak There are 0 directories and 250 images in pizza_steak/test/pizza There are 2 directories and 0 images in pizza_steak/train There are 0 directories and 750 images in pizza_steak/train/steak There are 0 directories and 750 images in pizza_steak/train/pizza . !ls -la pizza_steak . total 16 drwxr-xr-x 4 root root 4096 Feb 17 16:13 . drwxr-xr-x 1 root root 4096 Feb 17 16:13 .. drwxr-xr-x 4 root root 4096 Feb 17 16:13 test drwxr-xr-x 4 root root 4096 Feb 17 16:13 train . num_steak_images_train = len(os.listdir(&quot;pizza_steak/train/steak&quot;)) num_steak_images_train . 750 . Note: To visualize our images, first let&#39;s get the class names programmatically . import pathlib import numpy as np data_dir = pathlib.Path(&quot;pizza_steak/train&quot;) class_names = np.array(sorted([item.name for item in data_dir.glob(&quot;*&quot;)])) # Created a list of classnames from subdirectories print(class_names) . [&#39;pizza&#39; &#39;steak&#39;] . import matplotlib.pyplot as plt import matplotlib.image as mpimg import random plt.style.use(&#39;dark_background&#39;) def view_random_image(target_dir, target_class): # Setup target directory (we&#39;ll view images from here) target_folder = target_dir+target_class # Get a random image path random_image = random.sample(os.listdir(target_folder), 1) # Read in the image and plot it using matplotlib img = mpimg.imread(target_folder + &quot;/&quot; + random_image[0]) plt.imshow(img) plt.title(target_class) plt.axis(&quot;off&quot;); print(f&quot;Image shape: {img.shape}&quot;) # show the shape of the image return img . img = view_random_image(target_dir=&quot;pizza_steak/train/&quot;, target_class=&quot;steak&quot;) . Image shape: (512, 512, 3) . import tensorflow as tf tf.constant(img) . &lt;tf.Tensor: shape=(512, 512, 3), dtype=uint8, numpy= array([[[19, 11, 9], [21, 13, 11], [22, 14, 12], ..., [19, 36, 2], [19, 36, 2], [21, 38, 2]], [[20, 12, 10], [22, 14, 12], [23, 15, 13], ..., [19, 36, 2], [18, 35, 1], [19, 36, 2]], [[21, 13, 11], [22, 14, 12], [24, 16, 14], ..., [20, 35, 2], [19, 34, 1], [19, 34, 1]], ..., [[29, 10, 12], [31, 11, 12], [34, 10, 8], ..., [ 1, 1, 3], [ 1, 1, 3], [ 1, 1, 3]], [[27, 11, 14], [29, 11, 11], [32, 11, 10], ..., [ 2, 2, 4], [ 1, 1, 3], [ 1, 1, 3]], [[25, 10, 13], [26, 10, 11], [32, 11, 10], ..., [ 2, 1, 6], [ 1, 0, 5], [ 1, 0, 5]]], dtype=uint8)&gt; . . img.shape # Returns width, height, colour channels . (512, 512, 3) . # Normalize the pixel values (Data preprocessing step) img/255. . array([[[0.0745098 , 0.04313725, 0.03529412], [0.08235294, 0.05098039, 0.04313725], [0.08627451, 0.05490196, 0.04705882], ..., [0.0745098 , 0.14117647, 0.00784314], [0.0745098 , 0.14117647, 0.00784314], [0.08235294, 0.14901961, 0.00784314]], [[0.07843137, 0.04705882, 0.03921569], [0.08627451, 0.05490196, 0.04705882], [0.09019608, 0.05882353, 0.05098039], ..., [0.0745098 , 0.14117647, 0.00784314], [0.07058824, 0.1372549 , 0.00392157], [0.0745098 , 0.14117647, 0.00784314]], [[0.08235294, 0.05098039, 0.04313725], [0.08627451, 0.05490196, 0.04705882], [0.09411765, 0.0627451 , 0.05490196], ..., [0.07843137, 0.1372549 , 0.00784314], [0.0745098 , 0.13333333, 0.00392157], [0.0745098 , 0.13333333, 0.00392157]], ..., [[0.11372549, 0.03921569, 0.04705882], [0.12156863, 0.04313725, 0.04705882], [0.13333333, 0.03921569, 0.03137255], ..., [0.00392157, 0.00392157, 0.01176471], [0.00392157, 0.00392157, 0.01176471], [0.00392157, 0.00392157, 0.01176471]], [[0.10588235, 0.04313725, 0.05490196], [0.11372549, 0.04313725, 0.04313725], [0.1254902 , 0.04313725, 0.03921569], ..., [0.00784314, 0.00784314, 0.01568627], [0.00392157, 0.00392157, 0.01176471], [0.00392157, 0.00392157, 0.01176471]], [[0.09803922, 0.03921569, 0.05098039], [0.10196078, 0.03921569, 0.04313725], [0.1254902 , 0.04313725, 0.03921569], ..., [0.00784314, 0.00392157, 0.02352941], [0.00392157, 0. , 0.01960784], [0.00392157, 0. , 0.01960784]]]) . . An end-to-end example . Let&#39;s build a convolutional neural network to find patterns in our images, more specifically we need to: . Load our images | Preprocess our images | Compile our CNN | Fit the CNN to our training data | . . It took so much time for one epoch becuase we are not using any GPU accelerator. It doesn&#39;t have anything to do with our code but convolution layers take up a lot of computation and for a dataset that big the normal CPU takes a lot of time. GPU are excellent in number crunching with very fast speeds. So, let&#39;s change the runtime type and Use GPU Accelerator. . import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator # Set the seed tf.random.set_seed(42) # Preprocess data (get all of the pixel values between 0 and 1) # This is also called scaling/normalization. train_datagen = ImageDataGenerator(rescale = 1./255) valid_datagen = ImageDataGenerator(rescale = 1./255) # Setup paths to our data directories train_dir = &quot;/content/pizza_steak/train&quot; test_dir = &quot;pizza_steak/test&quot; # Import data from directories and turn it into batches train_data = train_datagen.flow_from_directory(directory = train_dir, batch_size = 32, # Number of image to process at a time target_size = (224,224), class_mode = &quot;binary&quot;, seed = 42) valid_data = valid_datagen.flow_from_directory(directory = test_dir, batch_size = 32, target_size = (224,224), class_mode = &quot;binary&quot;, seed = 42) # Build a CNN model(same as tiny VGG on the CNN explainer website) model_1 = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(filters = 10, kernel_size = 3, activation = &quot;relu&quot;, input_shape = (224,224,3)), tf.keras.layers.Conv2D(10,3,activation = &quot;relu&quot;), tf.keras.layers.MaxPool2D(pool_size = 2, padding = &quot;valid&quot;), tf.keras.layers.Conv2D(10,3, activation = &quot;relu&quot;), tf.keras.layers.Conv2D(10,3, activation = &quot;relu&quot;), tf.keras.layers.MaxPool2D(2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # Compile our CNN model_1.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit the model history_1 = model_1.fit(train_data, epochs = 5, steps_per_epoch= len(train_data), validation_data = valid_data, validation_steps = len(valid_data)) . Found 1500 images belonging to 2 classes. Found 500 images belonging to 2 classes. Epoch 1/5 47/47 [==============================] - 12s 245ms/step - loss: 0.5455 - accuracy: 0.7160 - val_loss: 0.4042 - val_accuracy: 0.8160 Epoch 2/5 47/47 [==============================] - 11s 238ms/step - loss: 0.4120 - accuracy: 0.8240 - val_loss: 0.3212 - val_accuracy: 0.8700 Epoch 3/5 47/47 [==============================] - 11s 238ms/step - loss: 0.3803 - accuracy: 0.8400 - val_loss: 0.3324 - val_accuracy: 0.8600 Epoch 4/5 47/47 [==============================] - 11s 237ms/step - loss: 0.3441 - accuracy: 0.8547 - val_loss: 0.3056 - val_accuracy: 0.8780 Epoch 5/5 47/47 [==============================] - 11s 238ms/step - loss: 0.3099 - accuracy: 0.8653 - val_loss: 0.3038 - val_accuracy: 0.8660 . If it is taking long for the first epoch make sure you have changed the runtime type to GPU. You can change it by going to the runtime option and in the change runtime type selected GPU as your hardware accelerator. . Our model above performed really well. It is prediction at an accuracy of 87% on the validation data( the model hasn&#39;t seen this data during training) . model_1.summary() . Model: &#34;sequential_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_8 (Conv2D) (None, 222, 222, 10) 280 conv2d_9 (Conv2D) (None, 220, 220, 10) 910 max_pooling2d_3 (MaxPooling (None, 110, 110, 10) 0 2D) conv2d_10 (Conv2D) (None, 108, 108, 10) 910 conv2d_11 (Conv2D) (None, 106, 106, 10) 910 max_pooling2d_4 (MaxPooling (None, 53, 53, 10) 0 2D) flatten_1 (Flatten) (None, 28090) 0 dense_1 (Dense) (None, 1) 28091 ================================================================= Total params: 31,101 Trainable params: 31,101 Non-trainable params: 0 _________________________________________________________________ . Using the same model as before (Non-CNN model) . Let&#39;s replicate the model we have built in a previous post. . The model we are building is from TensorFlow Playground . tf.random.set_seed(42) # Create a model to replicate the TensorFlow playground model model_2 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (224,224,3)), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # Compile the model model_2.compile(loss = &quot;binary_crossentropy&quot;, optimizer =tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit the model history_2 = model_2.fit(train_data, epochs = 5, steps_per_epoch = len(train_data), validation_data=valid_data, validation_steps = len(valid_data)) . Epoch 1/5 47/47 [==============================] - 10s 207ms/step - loss: 0.6961 - accuracy: 0.4967 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 2/5 47/47 [==============================] - 9s 199ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000 Epoch 3/5 47/47 [==============================] - 9s 198ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 Epoch 4/5 47/47 [==============================] - 9s 198ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 Epoch 5/5 47/47 [==============================] - 9s 202ms/step - loss: 0.6932 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5000 . model_2.summary() . Model: &#34;sequential_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_2 (Flatten) (None, 150528) 0 dense_2 (Dense) (None, 4) 602116 dense_3 (Dense) (None, 4) 20 dense_4 (Dense) (None, 1) 5 ================================================================= Total params: 602,141 Trainable params: 602,141 Non-trainable params: 0 _________________________________________________________________ . Despite having 20 times more parameters than our CNN(model_1), model_2 performs poorly. . tf.random.set_seed(42) # Create the model(same as above but let&#39;s modify it for better) model_3 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (224,224,3)), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # 2. Compile the model model_3.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history_3 = model_3.fit(train_data, epochs = 5, steps_per_epoch = len(train_data), validation_data = valid_data, validation_steps = len(valid_data)) . Epoch 1/5 47/47 [==============================] - 10s 211ms/step - loss: 3.2629 - accuracy: 0.6147 - val_loss: 0.4939 - val_accuracy: 0.7760 Epoch 2/5 47/47 [==============================] - 9s 201ms/step - loss: 0.6015 - accuracy: 0.7340 - val_loss: 1.2933 - val_accuracy: 0.5620 Epoch 3/5 47/47 [==============================] - 10s 204ms/step - loss: 0.6752 - accuracy: 0.7227 - val_loss: 2.5866 - val_accuracy: 0.5020 Epoch 4/5 47/47 [==============================] - 9s 200ms/step - loss: 0.8349 - accuracy: 0.7020 - val_loss: 0.5200 - val_accuracy: 0.7460 Epoch 5/5 47/47 [==============================] - 10s 204ms/step - loss: 0.5538 - accuracy: 0.7507 - val_loss: 0.4485 - val_accuracy: 0.8020 . model_3.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_3 (Flatten) (None, 150528) 0 dense_5 (Dense) (None, 100) 15052900 dense_6 (Dense) (None, 100) 10100 dense_7 (Dense) (None, 100) 10100 dense_8 (Dense) (None, 1) 101 ================================================================= Total params: 15,073,201 Trainable params: 15,073,201 Non-trainable params: 0 _________________________________________________________________ . The model_3 has around 15 million parameters. Model_3 has around 500 times more parameters to train than the model_2 (the CNN model). And this shows the true power of convolution neural networks(CNN) . Note: You can think of trainable parameters as patterns a model can learn from data. Intuitively, you might think more is better. In several cases that will be true. But the difference here is the two different styles of model we are using. Convolutional neural network seeks to sort out and learn the most important pattterns in an image. So even though there are less number of parameters in our CNN these are often more helpful in deciphering from visual data. . Binary Classification: . Getting more familiar with our dataset | Preprocess the data(eg. scaling/Normalization) | Create a model (start with a basic one) | Fit the model | Evaluate the model | Adjust different paramters and improve the model (try to beat any benchmarks if exist or make your own benchmark) | Repeat the steps to get the best possible result(experiment, experiment, experiment) | 1. Getting more familiar with our data . import os import matplotlib.pyplot as plt plt.figure() plt.subplot(1,2,1) steak_img = view_random_image(&quot;pizza_steak/train/&quot;, &quot;steak&quot;) plt.subplot(1,2,2) pizza_img = view_random_image(&quot;pizza_steak/train/&quot; , &quot;pizza&quot;) . Image shape: (288, 512, 3) Image shape: (512, 512, 3) . Preprocess the data . train_dir = &quot;pizza_steak/train/&quot; test_dir = &quot;pizza_steak/test/&quot; . Our next step is to turn our data into batches . A batch is a small subset of data. Rather than look at all 10,000 images at one time, a model might only look at 32 at a time. It does this for a couple or reasons: . 10,000 images(or more) might not fit into the memory of your processor(GPU) | Trying to learn the patterns in 10,000 images in one hit could result in thd model not begin able to learn very well. | Why a mini-batch size of 32? Training with large minibatches is bad for your health.More importantly, it&#39;s bad for your test error.Friends dont let friends use minibatches larger than 32. https://t.co/hxx2rGhIG1 . &mdash; Yann LeCun (@ylecun) April 26, 2018 | !nvidia-smi . Thu Feb 17 16:18:35 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 73C P0 75W / 149W | 1305MiB / 11441MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ . from tensorflow.keras.preprocessing.image import ImageDataGenerator train_datagen = ImageDataGenerator(rescale = 1/255.) test_datagen = ImageDataGenerator(rescale = 1/255.) . train_data = train_datagen.flow_from_directory(directory = train_dir, # Target directory of images target_size = (224,224), # Target size of images(height,width) class_mode = &quot;binary&quot;, # type of data you are working with batch_size = 32) # size of mini-batches to load data into test_data = test_datagen.flow_from_directory(directory = test_dir, target_size = (224,224), class_mode = &quot;binary&quot;, batch_size = 32) . Found 1500 images belonging to 2 classes. Found 500 images belonging to 2 classes. . images, labels = train_data.next() # get the &quot;next&quot; batch of images/labels in the batch len(images), len(labels) . (32, 32) . len(train_data) # No. of total images/ mini-batch size . 47 . # Get the first two images images[:2], images[0].shape # we get the output as array of pixel values . (array([[[[0.47058827, 0.40784317, 0.34509805], [0.4784314 , 0.427451 , 0.3647059 ], [0.48627454, 0.43529415, 0.37254903], ..., [0.8313726 , 0.70980394, 0.48627454], [0.8431373 , 0.73333335, 0.5372549 ], [0.87843144, 0.7725491 , 0.5882353 ]], [[0.50980395, 0.427451 , 0.36078432], [0.5058824 , 0.42352945, 0.35686275], [0.5137255 , 0.4431373 , 0.3647059 ], ..., [0.82745105, 0.7058824 , 0.48235297], [0.82745105, 0.70980394, 0.5058824 ], [0.8431373 , 0.73333335, 0.5372549 ]], [[0.5254902 , 0.427451 , 0.34901962], [0.5372549 , 0.43921572, 0.36078432], [0.5372549 , 0.45098042, 0.36078432], ..., [0.82745105, 0.7019608 , 0.4784314 ], [0.82745105, 0.7058824 , 0.49411768], [0.8352942 , 0.7176471 , 0.5137255 ]], ..., [[0.77647066, 0.5647059 , 0.2901961 ], [0.7803922 , 0.53333336, 0.22352943], [0.79215693, 0.5176471 , 0.18039216], ..., [0.30588236, 0.2784314 , 0.24705884], [0.24705884, 0.23137257, 0.19607845], [0.2784314 , 0.27450982, 0.25490198]], [[0.7843138 , 0.57254905, 0.29803923], [0.79215693, 0.54509807, 0.24313727], [0.8000001 , 0.5254902 , 0.18823531], ..., [0.2627451 , 0.23529413, 0.20392159], [0.24313727, 0.227451 , 0.19215688], [0.26666668, 0.2627451 , 0.24313727]], [[0.7960785 , 0.59607846, 0.3372549 ], [0.7960785 , 0.5647059 , 0.26666668], [0.81568635, 0.54901963, 0.22352943], ..., [0.23529413, 0.19607845, 0.16078432], [0.3019608 , 0.26666668, 0.24705884], [0.26666668, 0.2509804 , 0.24705884]]], [[[0.38823533, 0.4666667 , 0.36078432], [0.3921569 , 0.46274513, 0.36078432], [0.38431376, 0.454902 , 0.36078432], ..., [0.5294118 , 0.627451 , 0.54509807], [0.5294118 , 0.627451 , 0.54509807], [0.5411765 , 0.6392157 , 0.5568628 ]], [[0.38431376, 0.454902 , 0.3529412 ], [0.3921569 , 0.46274513, 0.36078432], [0.39607847, 0.4666667 , 0.37254903], ..., [0.54509807, 0.6431373 , 0.5686275 ], [0.5529412 , 0.6509804 , 0.5764706 ], [0.5647059 , 0.6627451 , 0.5882353 ]], [[0.3921569 , 0.46274513, 0.36078432], [0.38431376, 0.454902 , 0.3529412 ], [0.4039216 , 0.47450984, 0.3803922 ], ..., [0.5764706 , 0.67058825, 0.6156863 ], [0.5647059 , 0.6666667 , 0.6156863 ], [0.5647059 , 0.6666667 , 0.6156863 ]], ..., [[0.47058827, 0.5647059 , 0.4784314 ], [0.4784314 , 0.5764706 , 0.4901961 ], [0.48235297, 0.5803922 , 0.49803925], ..., [0.39607847, 0.42352945, 0.3019608 ], [0.37647063, 0.40000004, 0.2901961 ], [0.3803922 , 0.4039216 , 0.3019608 ]], [[0.45098042, 0.5529412 , 0.454902 ], [0.46274513, 0.5647059 , 0.4666667 ], [0.47058827, 0.57254905, 0.47450984], ..., [0.40784317, 0.43529415, 0.3137255 ], [0.39607847, 0.41960788, 0.31764707], [0.38823533, 0.40784317, 0.31764707]], [[0.47450984, 0.5764706 , 0.47058827], [0.47058827, 0.57254905, 0.4666667 ], [0.46274513, 0.5647059 , 0.4666667 ], ..., [0.4039216 , 0.427451 , 0.31764707], [0.3921569 , 0.4156863 , 0.3137255 ], [0.4039216 , 0.42352945, 0.3372549 ]]]], dtype=float32), (224, 224, 3)) . . labels # we get to know which label by crosschecking the labels with the input data . array([1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.], dtype=float32) . 2. Create a CNN Model (start with a baseline) . A baseline is a relatively simple model or existing result that you setup when beginning a machine learning experiment, as you keep experimenting we try to do more better than our basline. . Note: In deep learning, there is almost an infinite amount of architectures you could create. So one of the best ways to get started is to start with something simple and see if it works on your data and then introduce complexity as required(e.g. look at which current model is performing best in the field for your problem) . Hyperparameter name What does it do? Typical valuse . Filters | Decides how many filters should pass over an input tensor (e.g. sliding windows over an image) | 10, 32, 64, 128 (higher values lead to more complex models) | . Kernel size(also called filter size) | Determines the shape of the filters(sliding windows) over the outputs | 3,5, 7 (lower values learn smaller features, higher values learn larger features) | . Padding | Pads the target tensor with zeros (if &quot;same&quot;) to preserver input shape. Or leaves in the target tensor as is (if &quot;valid&quot;), loweing the output shape. | &quot;same&quot; or &quot;valid&quot; | . Strides | The number of steps a filter takes across an image at a time(eg. if strides = 1, a fileter moves across an image 1 pixel at a time) | 1 (default), 2 | . Resource: CNN-Explainer . from tensorflow.keras.optimizers import Adam from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Activation from tensorflow.keras import Sequential . import tensorflow as tf # Create the model model_4 = Sequential([ Conv2D(filters = 10, # filters is the number of sliding windows going across an input kernel_size = 3, # the size of the sliding window going across an input strides =1, # the size of the step the sliding window takes across an input padding = &quot;valid&quot;, # If &quot;same&quot; padding output is the same shape as input but &quot;valid&quot; output shape gets compressed. activation = &quot;relu&quot;, input_shape = (224,224,3)), Conv2D(10,2, activation = &quot;relu&quot;), Conv2D(10,3 , activation = &quot;relu&quot;), Flatten(), Dense(1, activation = &quot;sigmoid&quot;) # Output layer(working with binary classification so only 1 output neuron) ]) # Compile the model model_4.compile(loss = &quot;binary_crossentropy&quot;, optimizer = Adam(), metrics = [&quot;accuracy&quot;]) . model_4.summary() . Model: &#34;sequential_4&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_12 (Conv2D) (None, 222, 222, 10) 280 conv2d_13 (Conv2D) (None, 221, 221, 10) 410 conv2d_14 (Conv2D) (None, 219, 219, 10) 910 flatten_4 (Flatten) (None, 479610) 0 dense_9 (Dense) (None, 1) 479611 ================================================================= Total params: 481,211 Trainable params: 481,211 Non-trainable params: 0 _________________________________________________________________ . 4. Fit the Model . len(train_data), len(test_data) . (47, 16) . history_4 = model_4.fit(train_data, epochs = 5, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = len(test_data)) . Epoch 1/5 47/47 [==============================] - 13s 251ms/step - loss: 1.0757 - accuracy: 0.6960 - val_loss: 0.4364 - val_accuracy: 0.7920 Epoch 2/5 47/47 [==============================] - 11s 230ms/step - loss: 0.4187 - accuracy: 0.8147 - val_loss: 0.4065 - val_accuracy: 0.8380 Epoch 3/5 47/47 [==============================] - 11s 230ms/step - loss: 0.3305 - accuracy: 0.8633 - val_loss: 0.3746 - val_accuracy: 0.8280 Epoch 4/5 47/47 [==============================] - 15s 313ms/step - loss: 0.1696 - accuracy: 0.9420 - val_loss: 0.4955 - val_accuracy: 0.7780 Epoch 5/5 47/47 [==============================] - 20s 424ms/step - loss: 0.0709 - accuracy: 0.9807 - val_loss: 0.4630 - val_accuracy: 0.8080 . model_4.evaluate(test_data) . 16/16 [==============================] - 4s 258ms/step - loss: 0.4630 - accuracy: 0.8080 . [0.4629989564418793, 0.8080000281333923] . 5. Evaluating our model . It looks like our model is learning something, let&#39;s evaluate it. . import pandas as pd pd.DataFrame(history_4.history).plot(figsize = (10,7)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb5ad1601d0&gt; . def plot_loss_curves(history): &quot;&quot;&quot; Returns separate loss curves for training and validation metrics &quot;&quot;&quot; loss = history.history[&quot;loss&quot;] val_loss = history.history[&quot;val_loss&quot;] accuracy = history.history[&quot;accuracy&quot;] val_accuracy = history.history[&quot;val_accuracy&quot;] epochs = range(len(history.history[&quot;loss&quot;])) # how many epochs we run for? # Plot the loss plt.plot(epochs, loss, label = &quot;training_loss&quot;) plt.plot(epochs, val_loss, label = &quot;val_loss&quot;) plt.title(&quot;loss&quot;) plt.xlabel(&quot;epochs&quot;) plt.legend() # Plot the accuracy plt.figure() plt.plot(epochs, accuracy, label = &quot;training_accuracy&quot;) plt.plot(epochs, val_accuracy, label = &quot;val_accuracy&quot;) plt.title(&quot;accuracy&quot;) plt.xlabel(&quot;epochs&quot;) plt.legend() . Note: When a model&#39;s validation loss starts to increase, it&#39;s likely that the model is overfitting tht training dataset. This means, it&#39;s learning the patterns in the training dataset too well and thus the model&#39;s ability to generalize to unseen data will be diminished. . plot_loss_curves(history_4) . Note: Ideally the two loss curves (training and validation) will be very similar to each other (training loss and validation loss decreasing at similar rates), when there are large differences your model may be overfitting. . 6. Adjust the model parameters . Fitting a machine learning model comes in 3 steps: . Create a baseline | Beat the baseline by overfitting a larger model. | Reduce the overfitting | Ways to induce overfitting: . Increase the number of conv layers | Increase the number of conv filters | . Reduce overfitting: . Add data augmentation | Add regularization (such as MaxPool2D) | Add more data... | . Note: Reducing overfitting also known as regularization. . model_5 = Sequential([ Conv2D(10,3, activation = &quot;relu&quot;, input_shape=(224,224,3)), MaxPool2D(pool_size = 2), Conv2D(10,3, activation = &quot;relu&quot;), MaxPool2D(), Conv2D(10,3, activation= &quot;relu&quot;), MaxPool2D(), Flatten(), Dense(1, activation = &quot;sigmoid&quot;) ]) . model_5.compile(loss = &quot;binary_crossentropy&quot;, optimizer = Adam(), metrics = [&quot;accuracy&quot;]) . history_5 = model_5.fit(train_data, epochs =5, steps_per_epoch = len(train_data), validation_data = test_data, validation_steps = len(valid_data)) . Epoch 1/5 47/47 [==============================] - 11s 220ms/step - loss: 0.6293 - accuracy: 0.6480 - val_loss: 0.4676 - val_accuracy: 0.7800 Epoch 2/5 47/47 [==============================] - 10s 212ms/step - loss: 0.4690 - accuracy: 0.7820 - val_loss: 0.4022 - val_accuracy: 0.8220 Epoch 3/5 47/47 [==============================] - 10s 215ms/step - loss: 0.4438 - accuracy: 0.8033 - val_loss: 0.3693 - val_accuracy: 0.8480 Epoch 4/5 47/47 [==============================] - 10s 216ms/step - loss: 0.3932 - accuracy: 0.8340 - val_loss: 0.3502 - val_accuracy: 0.8400 Epoch 5/5 47/47 [==============================] - 11s 237ms/step - loss: 0.4066 - accuracy: 0.8187 - val_loss: 0.3302 - val_accuracy: 0.8660 . model_5.summary() . Model: &#34;sequential_5&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_15 (Conv2D) (None, 222, 222, 10) 280 max_pooling2d_5 (MaxPooling (None, 111, 111, 10) 0 2D) conv2d_16 (Conv2D) (None, 109, 109, 10) 910 max_pooling2d_6 (MaxPooling (None, 54, 54, 10) 0 2D) conv2d_17 (Conv2D) (None, 52, 52, 10) 910 max_pooling2d_7 (MaxPooling (None, 26, 26, 10) 0 2D) flatten_5 (Flatten) (None, 6760) 0 dense_10 (Dense) (None, 1) 6761 ================================================================= Total params: 8,861 Trainable params: 8,861 Non-trainable params: 0 _________________________________________________________________ . plot_loss_curves(history_5) . Finding Data Augmentation . train_datagen_augmented = ImageDataGenerator(rescale = 1/255., rotation_range= 0.2, # how much do you want to rotate an image? shear_range= 0.2, # how much do you want to shear an image? zoom_range= 0.2, # zoom in randomly on an image width_shift_range = 0.2, height_shift_range=0.3, horizontal_flip= True) # flipping the image # Create ImageDataGenerator without data augmentation train_datagen = ImageDataGenerator(rescale = 1/255.) # Create ImageDataGenerator without datat augmentation for the test dataset test_datagen = ImageDataGenerator(rescale =1/255.) . Question: What is data augmentation? . Data augmentation is the process of altering our training data, leading it to have more diversity and in turn allowing our modesl to learn more genearalizable patterns. Altering means adjusting the rotation of an image, flipping it, cropping it or something similar. . Improving a model(from data perspective) . Method to improve a model(reduce overfitting) What does it do? . More data | Gives a model more of a chance to learn patterns between samples(e.g. if model is performing poorly on images of pizza, show it more images of pizza) | . Data augmentation | Increase the diversity of your training dataset without collecting more data(e.g. take your photos of pizza and randomly rotate them 30 deg.). Increased diversity forces a model to learn more generalizable patterns | . Better data | Not all data samples are created equally. Removing poor samples from or adding better samples to your dataset can improve your model&#39;s performance | . Use transfer learning | Take a model&#39;s pre-trained patterns from one problem and tweak them to suit your own problem. For example, take a model trained on pictures of cars to recognise pictures of trucks | . Let&#39;s write some code to visualize data augmentation. . print(&quot;Augmented training images:&quot;) train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;, shuffle=False) # Don&#39;t shuffle for demonstration purposes, usually a good thing to shuffle # Create non-augmented data batches print(&quot;Non-augmented training images:&quot;) train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;, shuffle=False) # Don&#39;t shuffle for demonstration purposes print(&quot;Unchanged test images:&quot;) test_data = test_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;) . Augmented training images: Found 1500 images belonging to 2 classes. Non-augmented training images: Found 1500 images belonging to 2 classes. Unchanged test images: Found 500 images belonging to 2 classes. . Note: Data augmentation usually only performed on the training data. Usually ImageDataGenerator built-in data augmentation parameters our images are left as they are in the directories but are modified as they&#39;re loaded into the model. Let&#39;s visualize some augmented data! . images, labels = train_data.next() augmented_images, augmented_labels = train_data_augmented.next() # Note: labels aren&#39;t augmented, they stay the same . random_number = random.randint(0, 32) # we&#39;re making batches of size 32, so we&#39;ll get a random instance print(f&quot;showing image number: {random_number}&quot;) plt.imshow(images[random_number]) plt.title(f&quot;Original image&quot;) plt.axis(False) plt.figure() plt.imshow(augmented_images[random_number]) plt.title(f&quot;Augmented image&quot;) plt.axis(False); . showing image number: 6 . Let&#39;s build a model and see how it learns on augmented data. . model_6 = Sequential([ Conv2D(10,3, activation = &quot;relu&quot;), MaxPool2D(pool_size = 2), Conv2D(10,3 , activation = &quot;relu&quot;), MaxPool2D(), Conv2D(10,3, activation = &quot;relu&quot;), MaxPool2D(), Flatten(), Dense(1, activation = &quot;sigmoid&quot;) ]) # Compile the model model_6.compile(loss = &quot;binary_crossentropy&quot;, optimizer = Adam(), metrics = [&quot;accuracy&quot;]) # Fit the model history_6 = model_6.fit(train_data_augmented, # fitting model_6 on augmented data epochs = 5, steps_per_epoch = len(train_data_augmented), validation_data = test_data, validation_steps = len(test_data)) . Epoch 1/5 47/47 [==============================] - 26s 547ms/step - loss: 0.7067 - accuracy: 0.5567 - val_loss: 0.6788 - val_accuracy: 0.5000 Epoch 2/5 47/47 [==============================] - 26s 549ms/step - loss: 0.7515 - accuracy: 0.5227 - val_loss: 0.6899 - val_accuracy: 0.5000 Epoch 3/5 47/47 [==============================] - 26s 548ms/step - loss: 0.6917 - accuracy: 0.5067 - val_loss: 0.6834 - val_accuracy: 0.5600 Epoch 4/5 47/47 [==============================] - 26s 546ms/step - loss: 0.6841 - accuracy: 0.5347 - val_loss: 0.6643 - val_accuracy: 0.5560 Epoch 5/5 47/47 [==============================] - 25s 540ms/step - loss: 0.6629 - accuracy: 0.5740 - val_loss: 0.6350 - val_accuracy: 0.6780 . plot_loss_curves(history_6) . Let&#39;s shuffle our augmented training data and train another model(the same as before) . # Import data and augment it from training directory print(&quot;Augmented training images:&quot;) train_data_augmented_shuffled = train_datagen_augmented.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode=&#39;binary&#39;, shuffle=True) # Don&#39;t shuffle for demonstration purposes, usually a good thing to shuffle . Augmented training images: Found 1500 images belonging to 2 classes. . model_7 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), # same input shape as our images Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(1, activation=&#39;sigmoid&#39;) ]) # Compile the model model_7.compile(loss=&quot;binary_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) # Fit the model history_7 = model_7.fit(train_data_augmented_shuffled, epochs=5, steps_per_epoch=len(train_data_augmented_shuffled), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 47/47 [==============================] - 30s 612ms/step - loss: 0.6597 - accuracy: 0.5953 - val_loss: 0.5916 - val_accuracy: 0.6720 Epoch 2/5 47/47 [==============================] - 26s 552ms/step - loss: 0.5980 - accuracy: 0.6833 - val_loss: 0.6378 - val_accuracy: 0.6280 Epoch 3/5 47/47 [==============================] - 26s 546ms/step - loss: 0.5257 - accuracy: 0.7380 - val_loss: 0.4047 - val_accuracy: 0.8260 Epoch 4/5 47/47 [==============================] - 35s 744ms/step - loss: 0.5384 - accuracy: 0.7367 - val_loss: 0.3878 - val_accuracy: 0.8420 Epoch 5/5 47/47 [==============================] - 34s 709ms/step - loss: 0.5049 - accuracy: 0.7607 - val_loss: 0.4293 - val_accuracy: 0.7920 . We have hit an accuracy of around 77%!! . plot_loss_curves(history_7) . Note: When shuffling training data, the model gets exposed to all different kinds of data during training, thus enabling it to learn features across a wide array of images(in our case, pizza &amp; steak at the same time instead of just pizza then steak) . 7. Repeat the steps to get the best possible result(experiment, experiment, experiment) . Since we&#39;ve already improved a long way from our baseline, there are a few things we can try to continue to improve the model: . Increase the number of model layers (e.g. add more Conv2D/MaxPool2D layers) | Increase the number of filters in each convolutional layers (e.g. from 10 to 32 or even 64) | Train for longer (more epochs) | Find an ideal learning rate | Get more data(give the model more oppurtunites to learn) | Use transfer learning to leverage what another image model has learn and adjust it for our own use case. | . Making a prediction with our trained model on our custom data . print(class_names) . [&#39;pizza&#39; &#39;steak&#39;] . import matplotlib.image as mpimg import matplotlib.pyplot as plt !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg steak = mpimg.imread(&quot;03-steak.jpeg&quot;) plt.imshow(steak) plt.axis(False); . --2022-02-17 17:40:54-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1978213 (1.9M) [image/jpeg] Saving to: ‘03-steak.jpeg.2’ 03-steak.jpeg.2 100%[===================&gt;] 1.89M --.-KB/s in 0.07s 2022-02-17 17:40:54 (26.7 MB/s) - ‘03-steak.jpeg.2’ saved [1978213/1978213] . steak.shape . (4032, 3024, 3) . steak . array([[[162, 158, 149], [163, 159, 150], [166, 162, 153], ..., [136, 17, 23], [140, 21, 27], [140, 21, 27]], [[164, 160, 151], [164, 160, 151], [164, 160, 151], ..., [133, 14, 20], [134, 15, 21], [137, 18, 24]], [[166, 162, 153], [165, 161, 152], [163, 159, 150], ..., [136, 17, 23], [134, 15, 21], [136, 17, 23]], ..., [[154, 132, 109], [142, 122, 98], [101, 80, 59], ..., [165, 154, 148], [152, 141, 135], [140, 129, 123]], [[141, 119, 95], [139, 117, 93], [106, 86, 62], ..., [158, 147, 141], [142, 131, 125], [138, 127, 121]], [[148, 127, 100], [149, 127, 103], [114, 94, 70], ..., [153, 142, 136], [138, 127, 121], [145, 132, 126]]], dtype=uint8) . . Note: We need to preprocess our custom data before predicting with our model. It is important that our custom data is preprocessed into the same format as the data our model was trained on. . def load_and_prep_image(filename, img_shape=224): &quot;&quot;&quot; Reads an image from filename, turns it into a tensor and reshapes it to (img_shape, colour_channels) &quot;&quot;&quot; img = tf.io.read_file(filename) # Decode the read file into tensor img = tf.image.decode_image(img, channels =3) # Resize the image img = tf.image.resize(img, size = [img_shape, img_shape]) # Rescale the image and get all values between 0 and 1 img = img/255. return img . steak = load_and_prep_image(&quot;03-steak.jpeg&quot;,img_shape = 224) steak . &lt;tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy= array([[[0.6377451 , 0.6220588 , 0.57892156], [0.6504902 , 0.63186276, 0.5897059 ], [0.63186276, 0.60833335, 0.5612745 ], ..., [0.52156866, 0.05098039, 0.09019608], [0.49509802, 0.04215686, 0.07058824], [0.52843136, 0.07745098, 0.10490196]], [[0.6617647 , 0.6460784 , 0.6107843 ], [0.6387255 , 0.6230392 , 0.57598037], [0.65588236, 0.63235295, 0.5852941 ], ..., [0.5352941 , 0.06862745, 0.09215686], [0.529902 , 0.05931373, 0.09460784], [0.5142157 , 0.05539216, 0.08676471]], [[0.6519608 , 0.6362745 , 0.5892157 ], [0.6392157 , 0.6137255 , 0.56764704], [0.65637255, 0.6269608 , 0.5828431 ], ..., [0.53137255, 0.06470589, 0.08039216], [0.527451 , 0.06862745, 0.1 ], [0.52254903, 0.05196078, 0.0872549 ]], ..., [[0.49313724, 0.42745098, 0.31029412], [0.05441177, 0.01911765, 0. ], [0.2127451 , 0.16176471, 0.09509804], ..., [0.6132353 , 0.59362745, 0.57009804], [0.65294117, 0.6333333 , 0.6098039 ], [0.64166665, 0.62990195, 0.59460783]], [[0.65392154, 0.5715686 , 0.45 ], [0.6367647 , 0.54656863, 0.425 ], [0.04656863, 0.01372549, 0. ], ..., [0.6372549 , 0.61764705, 0.59411764], [0.63529414, 0.6215686 , 0.5892157 ], [0.6401961 , 0.62058824, 0.59705883]], [[0.1 , 0.05539216, 0. ], [0.48333332, 0.40882352, 0.29117647], [0.65 , 0.5686275 , 0.44019607], ..., [0.6308824 , 0.6161765 , 0.5808824 ], [0.6519608 , 0.63186276, 0.5901961 ], [0.6338235 , 0.6259804 , 0.57892156]]], dtype=float32)&gt; . . print(f&quot;Shape before new dimension: {steak.shape}&quot;) steak = tf.expand_dims(steak, axis=0) # add an extra dimension at axis 0 #steak = steak[tf.newaxis, ...] # alternative to the above, &#39;...&#39; is short for &#39;every other dimension&#39; print(f&quot;Shape after new dimension: {steak.shape}&quot;) steak . Shape before new dimension: (224, 224, 3) Shape after new dimension: (1, 224, 224, 3) . &lt;tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy= array([[[[0.6377451 , 0.6220588 , 0.57892156], [0.6504902 , 0.63186276, 0.5897059 ], [0.63186276, 0.60833335, 0.5612745 ], ..., [0.52156866, 0.05098039, 0.09019608], [0.49509802, 0.04215686, 0.07058824], [0.52843136, 0.07745098, 0.10490196]], [[0.6617647 , 0.6460784 , 0.6107843 ], [0.6387255 , 0.6230392 , 0.57598037], [0.65588236, 0.63235295, 0.5852941 ], ..., [0.5352941 , 0.06862745, 0.09215686], [0.529902 , 0.05931373, 0.09460784], [0.5142157 , 0.05539216, 0.08676471]], [[0.6519608 , 0.6362745 , 0.5892157 ], [0.6392157 , 0.6137255 , 0.56764704], [0.65637255, 0.6269608 , 0.5828431 ], ..., [0.53137255, 0.06470589, 0.08039216], [0.527451 , 0.06862745, 0.1 ], [0.52254903, 0.05196078, 0.0872549 ]], ..., [[0.49313724, 0.42745098, 0.31029412], [0.05441177, 0.01911765, 0. ], [0.2127451 , 0.16176471, 0.09509804], ..., [0.6132353 , 0.59362745, 0.57009804], [0.65294117, 0.6333333 , 0.6098039 ], [0.64166665, 0.62990195, 0.59460783]], [[0.65392154, 0.5715686 , 0.45 ], [0.6367647 , 0.54656863, 0.425 ], [0.04656863, 0.01372549, 0. ], ..., [0.6372549 , 0.61764705, 0.59411764], [0.63529414, 0.6215686 , 0.5892157 ], [0.6401961 , 0.62058824, 0.59705883]], [[0.1 , 0.05539216, 0. ], [0.48333332, 0.40882352, 0.29117647], [0.65 , 0.5686275 , 0.44019607], ..., [0.6308824 , 0.6161765 , 0.5808824 ], [0.6519608 , 0.63186276, 0.5901961 ], [0.6338235 , 0.6259804 , 0.57892156]]]], dtype=float32)&gt; . pred = model_7.predict(steak) pred . array([[0.90207344]], dtype=float32) . Looks like our custom image is begin put through our model, however it currently outputs a prediction probability. . class_names . array([&#39;pizza&#39;, &#39;steak&#39;], dtype=&#39;&lt;U5&#39;) . pred_class = class_names[int(tf.round(pred))] pred_class . &#39;steak&#39; . def pred_and_plot(model,filename, class_names= class_names): &quot;&quot;&quot; Import an image located at filename, makes a prediction with model and plots the image with predicted class as the title &quot;&quot;&quot; # Import the target image and preprocess it img = load_and_prep_image(filename) # Make a prediction pred = model.predict(tf.expand_dims(img,axis=0)) # Get the predicted class pred_class = class_names[int(tf.round(pred))] # PLot the image and predicted class plt.imshow(img) plt.title(f&quot;Prediction: {pred_class}&quot;) plt.axis(False); . pred_and_plot(model_7, &quot;03-steak.jpeg&quot;) . Wow!! our model works on a custom image data. That is so cool!! . !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg pred_and_plot(model_7, &quot;03-pizza-dad.jpeg&quot;) . --2022-02-17 18:11:56-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2874848 (2.7M) [image/jpeg] Saving to: ‘03-pizza-dad.jpeg’ 03-pizza-dad.jpeg 100%[===================&gt;] 2.74M --.-KB/s in 0.08s 2022-02-17 18:11:57 (35.1 MB/s) - ‘03-pizza-dad.jpeg’ saved [2874848/2874848] . Let&#39;s test on another random image pulled from the internet . !wget https://raw.githubusercontent.com/sandeshkatakam/My-Machine_learning-Blog/master/images/pizza-internet.jpg pred_and_plot(model_7, &quot;pizza-internet.jpg&quot;) . --2022-02-17 18:15:06-- https://raw.githubusercontent.com/sandeshkatakam/My-Machine_learning-Blog/master/images/pizza-internet.jpg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 62115 (61K) [image/jpeg] Saving to: ‘pizza-internet.jpg’ pizza-internet.jpg 100%[===================&gt;] 60.66K --.-KB/s in 0.01s 2022-02-17 18:15:06 (4.30 MB/s) - ‘pizza-internet.jpg’ saved [62115/62115] . Yayyy! Our model is very successful for images of pizza/steak from outside the dataset too. . model_7.save(&#39;pizza_steak_detect_model.h5&#39;) . Multi-class Image Classification . we&#39;ve have already seen binary classification now we are going to level up with 10 classes of food. . Become one with the data | Preprocess the data(get it ready for model) | Create a model(start with a baseline model) | Fit the model(overfit it to make sure it works) | Evaluate the model | Adjust different hyperparameters and improve the model(try to beat baseline/reduce overfitting) | Repeat until satisified. | 1. Import data . import zipfile !wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip # Unzip our data zip_ref = zipfile.ZipFile(&quot;10_food_classes_all_data.zip&quot;,&quot;r&quot;) zip_ref.extractall() zip_ref.close() . --2022-02-17 18:24:02-- https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_all_data.zip Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.203.128, 108.177.97.128, 108.177.125.128, ... Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.203.128|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 519183241 (495M) [application/zip] Saving to: ‘10_food_classes_all_data.zip’ 10_food_classes_all 100%[===================&gt;] 495.13M 149MB/s in 3.3s 2022-02-17 18:24:06 (148 MB/s) - ‘10_food_classes_all_data.zip’ saved [519183241/519183241] . import os # walk through 10 classes of food image data for dirpath, dirnames, filenames, in os.walk(&quot;10_food_classes_all_data&quot;): print(f&quot;There are {len(dirnames)} directories and {len(filenames)} images in &#39;{dirpath}&#39;.&quot;) . There are 2 directories and 0 images in &#39;10_food_classes_all_data&#39;. There are 10 directories and 0 images in &#39;10_food_classes_all_data/test&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/chicken_wings&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/hamburger&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/chicken_curry&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/fried_rice&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/sushi&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/steak&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/pizza&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/ramen&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/grilled_salmon&#39;. There are 0 directories and 250 images in &#39;10_food_classes_all_data/test/ice_cream&#39;. There are 10 directories and 0 images in &#39;10_food_classes_all_data/train&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/chicken_wings&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/hamburger&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/chicken_curry&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/fried_rice&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/sushi&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/steak&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/pizza&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/ramen&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/grilled_salmon&#39;. There are 0 directories and 750 images in &#39;10_food_classes_all_data/train/ice_cream&#39;. . !ls -la 10_food_classes_all_data/ . total 16 drwxr-xr-x 4 root root 4096 Feb 17 18:24 . drwxr-xr-x 1 root root 4096 Feb 17 18:24 .. drwxr-xr-x 12 root root 4096 Feb 17 18:24 test drwxr-xr-x 12 root root 4096 Feb 17 18:24 train . train_dir = &quot;10_food_classes_all_data/train/&quot; test_dir = &quot;10_food_classes_all_data/test/&quot; . import pathlib import numpy as np data_dir = pathlib.Path(train_dir) class_names = np.array(sorted([item.name for item in data_dir.glob(&#39;*&#39;)])) print(class_names) . [&#39;chicken_curry&#39; &#39;chicken_wings&#39; &#39;fried_rice&#39; &#39;grilled_salmon&#39; &#39;hamburger&#39; &#39;ice_cream&#39; &#39;pizza&#39; &#39;ramen&#39; &#39;steak&#39; &#39;sushi&#39;] . import random img = view_random_image(target_dir = train_dir, target_class = random.choice(class_names)) . Image shape: (512, 512, 3) . 2. Preprocess the data(prepare it for a model) . from tensorflow.keras.preprocessing.image import ImageDataGenerator # Rescale train_datagen = ImageDataGenerator(rescale=1/255.) test_datagen = ImageDataGenerator(rescale = 1/255.) # Load data in from directories and turn it into batches train_data = train_datagen.flow_from_directory(train_dir, target_size=(224,224), batch_size= 32, class_mode =&quot;categorical&quot;) test_data = test_datagen.flow_from_directory(test_dir, target_size=(224,224), batch_size= 32, class_mode =&quot;categorical&quot;) . Found 7500 images belonging to 10 classes. Found 2500 images belonging to 10 classes. . 3. Create a Model(start with a baseline) . import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Activation . IMG_SIZE = (224,224) # Set as global variable for reuse model_8 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), # same input shape as our images Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(10, activation=&#39;softmax&#39;) # changed to have 10 output neurons and used softmax ]) # Compile the model model_8.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) . 4. Fit the Model . history_8 = model_8.fit(train_data, #now 10 different classes epochs=5, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 235/235 [==============================] - 67s 281ms/step - loss: 2.1563 - accuracy: 0.2025 - val_loss: 2.0711 - val_accuracy: 0.2468 Epoch 2/5 235/235 [==============================] - 59s 250ms/step - loss: 1.9543 - accuracy: 0.3121 - val_loss: 2.0008 - val_accuracy: 0.2944 Epoch 3/5 235/235 [==============================] - 76s 322ms/step - loss: 1.5549 - accuracy: 0.4741 - val_loss: 2.0152 - val_accuracy: 0.3340 Epoch 4/5 235/235 [==============================] - 72s 306ms/step - loss: 0.9483 - accuracy: 0.6879 - val_loss: 2.4873 - val_accuracy: 0.2820 Epoch 5/5 235/235 [==============================] - 57s 242ms/step - loss: 0.4002 - accuracy: 0.8753 - val_loss: 3.6105 - val_accuracy: 0.2640 . 5. Evaluate the Model . model_8.evaluate(test_data) . 79/79 [==============================] - 13s 165ms/step - loss: 3.6105 - accuracy: 0.2640 . [3.6105382442474365, 0.2639999985694885] . plot_loss_curves(history_8) . What do these loss curves tell us? . It seem our model is overfitting the training set badly.. in ohter words, it&#39;s getting great results on the training data but fails to genearlize well to unseen data and performs poorly on the test dataset. . 6. Adjust the Hyperparamaters and Improve the model(to beat baseline/reduce overfitting) . Due to its performance on the training data, it&#39;s clear our model is learning something.. . However, it&#39;s not generalizing well to unseen data(overfitting) . So, let&#39;s try and fix overfitting by... . Get more data - having more data gives a model more oppurtunity to learn diverse patterns... | Simplify the model - if our current model is overfitting the data, it may be too complicated of a model, one way to simplify a model is to: reduce # of layers or reduce # hidden units in layers. Since hidden units try to find more complex relationships in the train-data they may cause overfitting more for the train data. | Use data augmentation - data augmentation manipulates the training data in such a way to add more diversity to it (without altering the original data) | Use transfer learning - transfer learning leverages the patterns another model has learned on a similar data set and use those patterns on your own dataset. | . # Let&#39;s try to remove 2 Conv layers model_9 = Sequential([ Conv2D(10, 3, activation=&#39;relu&#39;, input_shape=(224, 224, 3)), MaxPool2D(), Conv2D(10, 3, activation=&#39;relu&#39;), MaxPool2D(), Flatten(), Dense(10, activation=&#39;softmax&#39;) ]) # Compile the model model_9.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(), metrics=[&quot;accuracy&quot;]) . model_9.summary() . Model: &#34;sequential_13&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_42 (Conv2D) (None, 222, 222, 10) 280 max_pooling2d_26 (MaxPoolin (None, 111, 111, 10) 0 g2D) conv2d_43 (Conv2D) (None, 109, 109, 10) 910 max_pooling2d_27 (MaxPoolin (None, 54, 54, 10) 0 g2D) flatten_13 (Flatten) (None, 29160) 0 dense_18 (Dense) (None, 10) 291610 ================================================================= Total params: 292,800 Trainable params: 292,800 Non-trainable params: 0 _________________________________________________________________ . history_9 = model_9.fit(train_data, epochs=5, steps_per_epoch=len(train_data), validation_data=test_data, validation_steps=len(test_data)) . Epoch 1/5 235/235 [==============================] - 69s 289ms/step - loss: 2.1796 - accuracy: 0.2367 - val_loss: 1.9681 - val_accuracy: 0.3112 Epoch 2/5 235/235 [==============================] - 57s 240ms/step - loss: 1.7394 - accuracy: 0.4091 - val_loss: 1.8815 - val_accuracy: 0.3416 Epoch 3/5 235/235 [==============================] - 64s 271ms/step - loss: 1.3053 - accuracy: 0.5753 - val_loss: 2.0208 - val_accuracy: 0.3304 Epoch 4/5 235/235 [==============================] - 60s 257ms/step - loss: 0.8062 - accuracy: 0.7487 - val_loss: 2.4421 - val_accuracy: 0.3068 Epoch 5/5 235/235 [==============================] - 62s 264ms/step - loss: 0.4266 - accuracy: 0.8795 - val_loss: 2.8362 - val_accuracy: 0.3040 . plot_loss_curves(history_9) . Looks like our simplifying the model experiment did not work.. the accuracy went down and overfitting continued.. . Next, we try data augmentation. . Trying to reduce overfitting with data augmentation . Let&#39;s try and improve our model&#39;s results by using augmented training data... Ideally we want to: . Reduce overfitting (get the train and validation loss curves closer) | Improve the validation accuracy | . train_datagen_augmented = ImageDataGenerator(rescale = 1/255., rotation_range= 0.2, # how much do you want to rotate an image? shear_range= 0.2, # how much do you want to shear an image? zoom_range= 0.2, # zoom in randomly on an image width_shift_range = 0.2, height_shift_range=0.3, horizontal_flip= True) # flipping the image train_data_augmented = train_datagen_augmented.flow_from_directory(train_dir, target_size =(224,224), batch_size = 32, class_mode = &quot;categorical&quot;) # Create ImageDataGenerator without datat augmentation for the test dataset test_datagen = ImageDataGenerator(rescale =1/255.) test_data = test_datagen.flow_from_directory(test_dir, target_size=(224,224), batch_size = 32, class_mode = &quot;categorical&quot;) . Found 7500 images belonging to 10 classes. Found 2500 images belonging to 10 classes. . model_10 = tf.keras.models.clone_model(model_9) # replicate the same model # Compile the model model_10.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) . history_10 = model_10.fit(train_data_augmented, epochs=5, steps_per_epoch=len(train_data_augmented), validation_data=test_data, validation_steps= len(test_data)) . Epoch 1/5 235/235 [==============================] - 158s 670ms/step - loss: 2.2273 - accuracy: 0.1820 - val_loss: 2.1040 - val_accuracy: 0.2480 Epoch 2/5 235/235 [==============================] - 137s 584ms/step - loss: 2.0844 - accuracy: 0.2659 - val_loss: 2.0185 - val_accuracy: 0.2932 Epoch 3/5 235/235 [==============================] - 125s 534ms/step - loss: 2.0446 - accuracy: 0.2843 - val_loss: 1.8922 - val_accuracy: 0.3428 Epoch 4/5 235/235 [==============================] - 134s 571ms/step - loss: 1.9889 - accuracy: 0.3055 - val_loss: 1.8494 - val_accuracy: 0.3808 Epoch 5/5 235/235 [==============================] - 148s 631ms/step - loss: 1.9679 - accuracy: 0.3211 - val_loss: 1.7955 - val_accuracy: 0.4072 . model_8.evaluate(test_data) . 79/79 [==============================] - 16s 201ms/step - loss: 3.6105 - accuracy: 0.2640 . [3.610537528991699, 0.2639999985694885] . model_10.evaluate(test_data) . 79/79 [==============================] - 22s 274ms/step - loss: 1.7955 - accuracy: 0.4072 . [1.795534610748291, 0.40720000863075256] . plot_loss_curves(history_10) . Woah!! That result is much better, the loss curves are much closer to each ohter than the baseline model and they look line they&#39;rea heading in the right direction. So, if we were to train for longer, we might see further improvements. . 7. Repeat until satisfied . we can still try to bring our loss curves closer together and trying to improve the validation/test accuracy. . By running lots of experiments: . restructuring our model&#39;s architecture (increasing layers/hidden units) | try different methods of data augmentation (adjust the hyperparameters in our ImageDataGenerator instance) | training for longer (e.g. 10 epochs or more) | try transfer learning | . Making a prediction with our trained model on custom data . Let&#39;s use our trained model to make some predictions on our own custom images. . class_names . array([&#39;chicken_curry&#39;, &#39;chicken_wings&#39;, &#39;fried_rice&#39;, &#39;grilled_salmon&#39;, &#39;hamburger&#39;, &#39;ice_cream&#39;, &#39;pizza&#39;, &#39;ramen&#39;, &#39;steak&#39;, &#39;sushi&#39;], dtype=&#39;&lt;U14&#39;) . # Download some custom images !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg . --2022-02-17 19:43:17-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-hamburger.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 3564285 (3.4M) [image/jpeg] Saving to: ‘03-hamburger.jpeg’ 03-hamburger.jpeg 100%[===================&gt;] 3.40M --.-KB/s in 0.08s 2022-02-17 19:43:18 (43.1 MB/s) - ‘03-hamburger.jpeg’ saved [3564285/3564285] --2022-02-17 19:43:19-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-pizza-dad.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 2874848 (2.7M) [image/jpeg] Saving to: ‘03-pizza-dad.jpeg.1’ 03-pizza-dad.jpeg.1 100%[===================&gt;] 2.74M --.-KB/s in 0.08s 2022-02-17 19:43:19 (34.5 MB/s) - ‘03-pizza-dad.jpeg.1’ saved [2874848/2874848] --2022-02-17 19:43:19-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-sushi.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1725178 (1.6M) [image/jpeg] Saving to: ‘03-sushi.jpeg’ 03-sushi.jpeg 100%[===================&gt;] 1.64M --.-KB/s in 0.07s 2022-02-17 19:43:20 (25.0 MB/s) - ‘03-sushi.jpeg’ saved [1725178/1725178] --2022-02-17 19:43:21-- https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/03-steak.jpeg Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1978213 (1.9M) [image/jpeg] Saving to: ‘03-steak.jpeg.3’ 03-steak.jpeg.3 100%[===================&gt;] 1.89M --.-KB/s in 0.07s 2022-02-17 19:43:21 (28.0 MB/s) - ‘03-steak.jpeg.3’ saved [1978213/1978213] . . def pred_and_plot(model,filename, class_names= class_names): &quot;&quot;&quot; Import an image located at filename, makes a prediction with model and plots the image with predicted class as the title &quot;&quot;&quot; # Import the target image and preprocess it img = load_and_prep_image(filename) # Make a prediction pred = model.predict(tf.expand_dims(img,axis=0)) # Add in logic for mulit-class if len(pred[0]) &gt; 1: pred_class = class_names[tf.argmax(pred[0])] else: pred_class = class_names[int(tf.round(pred[0]))] # PLot the image and predicted class plt.imshow(img) plt.title(f&quot;Prediction: {pred_class}&quot;) plt.axis(False); . pred_and_plot(model = model_10, filename = &quot;03-pizza-dad.jpeg&quot;, class_names = class_names) . This our model got it right! . pred_and_plot(model = model_10, filename = &quot;03-sushi.jpeg&quot;, class_names = class_names) . Looks, like our model got this wrong! This is because it only achieved ~39% accuracy on the test data. So we can expect it to function quite poorly on other unseen data. . Saving and Loading our Model . model_10.save(&quot;save_trained_model_10&quot;) . INFO:tensorflow:Assets written to: save_trained_model_10/assets . loaded_model_10 = tf.keras.models.load_model(&quot;save_trained_model_10&quot;) loaded_model_10.evaluate(test_data) . 79/79 [==============================] - 13s 155ms/step - loss: 1.7955 - accuracy: 0.4072 . [1.795534372329712, 0.40720000863075256] . Bibliography: . CNN Explainer | TensorFlow Developer Certificate Udemy Course | TensorFlow Playground | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deeplearning/neuralnetworks/tensorflow/python/2022/02/17/Convolutional-Networks-and-Computer-vision-with-TensorFlow.html",
            "relUrl": "/deeplearning/neuralnetworks/tensorflow/python/2022/02/17/Convolutional-Networks-and-Computer-vision-with-TensorFlow.html",
            "date": " • Feb 17, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Neural Networks for Classification with TensorFlow",
            "content": "Neural Network for Classification with TensorFlow . This Notebook is an account of working for the Udemy course by Daniel Bourke: TensorFlow Developer Certificate in 2022: Zero to Mastery . Classification problems are those where our motive is to classify given data by predicting the labels for the input data after the networks looks at a large sample of examples . Example Classification Problems: . Is this email spam or not spam? (Binary Classification) . | Is this a photo of sushi, steak or pizza? (Multi-Class Classification problem) . | What tags should this article have?(Multi-label Classification) . | . This Notebook covers: . Architecture of a neural network classification model. . | Input shapes and output shapes of a classification model(features and labels) . | Creating custom data to view and fit. . | Steps in modelling . Creating a model | compiling a model | fitting a model | evaluating a model. | . | Different classification evaluation methods. . | Saving and Loading models. . | . Classification Inputs and Outputs: . Input and Output Shapes . [batch_size, width, height, colour_channels] gets represented as a tensor. Example shape of the tensor encoded from an image of resolution 224X224 pixels. . Shape = [None,224,224,3] or . Shape = [32,224,224,3] . we have set the batch_size as 32 in the second one because it is the common batch to take for a model. It&#39;s actually a default batch_size in tensorflow. we mostly take batches of 32 so that we don&#39;t run out of memory during our training. The batch_size can be configured according to the memory of the computer we are working on. some of the other examples of batch sizes are: 64, 128, 256 etc.. . Examples of output shape: Shape = [n], where n is the number of classes we have in our classification model. . Architecture of a Classification model . Example architecture of a classification model: . # 1. Create a model (specified to your problem) model = tf.keras.Sequential([ tf.keras.Input(shape= (224,224,3)), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(3, activation = &quot;softmax&quot;) ]) # 2. Compile the Model model.compile(loss = tf.keras.losses.CategoricalCrossentropy(), optimizer = tf.keras.Adam(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model model.fit(X_train, y_train, epochs = 5) # 4. Evaluate the model model.evaluate(X_test, y_test) . Hyperparameter Binary Classification Mutliclass Classification . Input layer shape | Same as number of features | Same as Binary Classification | . Hidden Layer(s) | Problem Specific | Same as Binary Classification | . Neurons per hidden layer | Problem Specific(genearlly 10 to 100) | Same as Binary Classification | . Output layer shape | 1(one class or the other) | 1 per class | . Hidden Activation | Usually ReLU(rectified linear unit) | Same as Binary Classification | . Output Activation | Sigmoid | Softmax | . Loss function | Cross Entropy(tf.keras.losses.BinaryCrossentropy) | Cross entropy(tf.keras.losses.CategoricalCrossentropy) | . Optimizer | SGD(Stochastic gradient Descent), Adam | Same as Binary Classification | . Creating data to view and fit . from sklearn.datasets import make_circles # Make 1000 examples n_samples = 1000 # Create circles X, y = make_circles(n_samples, noise = 0.03, random_state = 42) . X . array([[ 0.75424625, 0.23148074], [-0.75615888, 0.15325888], [-0.81539193, 0.17328203], ..., [-0.13690036, -0.81001183], [ 0.67036156, -0.76750154], [ 0.28105665, 0.96382443]]) . y[:10] . array([1, 1, 1, 1, 0, 1, 1, 1, 1, 0]) . our data is hard to understand right now. Let&#39;s visualize it! . import pandas as pd import numpy as np circles = pd.DataFrame({&quot;X0&quot;: X[:,0], &quot;X1&quot;: X[:,1], &quot;lablel&quot;: y}) circles . X0 X1 lablel . 0 0.754246 | 0.231481 | 1 | . 1 -0.756159 | 0.153259 | 1 | . 2 -0.815392 | 0.173282 | 1 | . 3 -0.393731 | 0.692883 | 1 | . 4 0.442208 | -0.896723 | 0 | . ... ... | ... | ... | . 995 0.244054 | 0.944125 | 0 | . 996 -0.978655 | -0.272373 | 0 | . 997 -0.136900 | -0.810012 | 1 | . 998 0.670362 | -0.767502 | 0 | . 999 0.281057 | 0.963824 | 0 | . 1000 rows × 3 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; import matplotlib.pyplot as plt from matplotlib import style style.use(&#39;dark_background&#39;) plt.scatter(X[:,0], X[:,1], c= y, cmap = plt.cm.RdYlBu ); . Resources: . Neural Networks Playground You can tweak the hyperparameters and visualize the results in a much more interactive way. | . X.shape, y.shape . ((1000, 2), (1000,)) . len(X), len(y) . (1000, 1000) . X[0], y[0] . (array([0.75424625, 0.23148074]), 1) . Steps in Modelling . The steps in modelling with TensorFlow are typically: . Create or import a model | Compile the model | Fit the model | Evaluate the model | Tweak | Evaluate.... | import tensorflow as tf print(tf.__version__) . 2.7.0 . tf.random.set_seed(42) # 1. Create the model using Sequential API model_1 = tf.keras.Sequential([ tf.keras.layers.Dense(1) ]) # 2. Compile the model model_1.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model model_1.fit(X,y, epochs = 5) . Epoch 1/5 32/32 [==============================] - 1s 1ms/step - loss: 2.8544 - accuracy: 0.4600 Epoch 2/5 32/32 [==============================] - 0s 1ms/step - loss: 0.7131 - accuracy: 0.5430 Epoch 3/5 32/32 [==============================] - 0s 1ms/step - loss: 0.6973 - accuracy: 0.5090 Epoch 4/5 32/32 [==============================] - 0s 1ms/step - loss: 0.6950 - accuracy: 0.5010 Epoch 5/5 32/32 [==============================] - 0s 1ms/step - loss: 0.6942 - accuracy: 0.4830 . &lt;keras.callbacks.History at 0x7f848eb26310&gt; . model_1.fit(X,y ,epochs = 200, verbose = 0) model_1.evaluate(X,y) . 32/32 [==============================] - 0s 1ms/step - loss: 0.6935 - accuracy: 0.5000 . [0.6934829950332642, 0.5] . Since we are working on a binary classification problem and our model is getting around ~50% accuracy so, it&#39;s practically guessing . # Add another layer and train for longer # Set the random seed tf.random.set_seed(42) # 1. Create a model, this time with 2 layers model_2 = tf.keras.Sequential([ tf.keras.layers.Dense(1), tf.keras.layers.Dense(1) ]) # 2. Compile the model model_2.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model model_2.fit(X,y, epochs = 100, verbose = 0) . &lt;keras.callbacks.History at 0x7f848eb26190&gt; . model_2.evaluate(X,y) . 32/32 [==============================] - 0s 1ms/step - loss: 0.6933 - accuracy: 0.5000 . [0.6933314800262451, 0.5] . Improving our model . Let&#39;s see what we change in each of the modelling steps: . Create a model - we might add more layers or increase the number of hidden units within a layer. . | Compiling a model - we can choose different optimization function like Adam, instead of SGD. . | Fitting a model - we can fit our model for more epochs(train for longer) . | # Set the random seed tf.random.set_seed(42) # 1. Create the model (this time 3 layers) model_3 = tf.keras.Sequential([ tf.keras.layers.Dense(100), # add 100 dense neurons tf.keras.layers.Dense(10), tf.keras.layers.Dense(1) # add another layer with 10 neurons ]) # 2. Compile the model model_3.compile(loss = tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # 3. Fit the model model_3.fit(X,y, epochs=100, verbose = 0) . &lt;keras.callbacks.History at 0x7f848e9b8650&gt; . model_3.evaluate(X,y) . 32/32 [==============================] - 0s 1ms/step - loss: 0.6980 - accuracy: 0.5080 . [0.6980254650115967, 0.5080000162124634] . still ~50% accuracy ..really bad result for us . To visualize our model&#39;s predictions let&#39;s create a function plot_decision_boundary, this will: . Take in a trained model, features(X) and labels(y) . | Create a meshgrid of te different X values. . | Make predictions across the meshgird . | Plot the predictions as well as line between zones(where each unique class falls) | . def plot_decision_boundary(model, X,y): &quot;&quot;&quot; plots the decision boundary created by a model prediction on X &quot;&quot;&quot; # Define the axis boundaries of the plot and create a meshgrid x_min, x_max = X[:,0].min() -0.1, X[:,0].max() + 0.1 y_min, y_max = X[:,1].min() - 0.1, X[:,1].max() + 0.1 xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min,y_max,100)) # Create X value(you are goind to make predictions on these) x_in = np.c_[xx.ravel(), yy.ravel()] # stack 2D arrays together # Make predictions y_pred = model.predict(x_in) # Check for mutli-class if len(y_pred[0]) &gt; 1 : print(&quot;doing multi class classification&quot;) # We have to reshape our prediction to get them ready for plotting y_pred = np.argmax(y_pred, axis = 1).reshape(xx.shape) else: print(&quot;doing binary classification&quot;) y_pred = np.round(y_pred).reshape(xx.shape) # Plot the decision boundary plt.contourf(xx, yy, y_pred, cmap = plt.cm.RdYlBu, alpha = 0.7) plt.scatter(X[:,0], X[:,1], c= y, s= 40, cmap = plt.cm.RdYlBu) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) . . plot_decision_boundary(model_3, X, y) . doing binary classification . This looks like model is fitting a linear boundary. That is the reason why our model is performing poorly on our metrics. . # Let&#39;s see if our model can be used for a regression problem.. tf.random.set_seed(42) # Create some regression data X_regression = tf.range(0,1000,5) y_regression = tf.range(100,1100, 5) # # y = X +100 # Let&#39;s split our training data into training and test splits X_reg_train = X_regression[:150] X_reg_test = X_regression[150:] y_reg_train = y_regression[:150] y_reg_test = y_regression[150:] . We compiled our model for a binary classification problem, Now we are working on a regression problem, let&#39;s change the model to suit our data. . tf.random.set_seed(42) # 1. Create teh model model_3 = tf.keras.Sequential([ tf.keras.layers.Dense(100), tf.keras.layers.Dense(10), tf.keras.layers.Dense(1) ]) # 2. Compile the model, this time with a regression-specific loss function model_3.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;mae&quot;]) # 3. Fit the model model_3.fit(tf.expand_dims(X_reg_train, axis=-1), y_reg_train, epochs =100, verbose = 0) . &lt;keras.callbacks.History at 0x7f848e28fa10&gt; . y_reg_preds = model_3.predict(X_reg_test) # Plot the model&#39;s predictoins against our regression data plt.figure(figsize = (10,7)) plt.scatter(X_reg_train, y_reg_train, c= &#39;b&#39;, label = &quot;Training data&quot;) plt.scatter(X_reg_test, y_reg_test, c = &#39;g&#39;, label =&quot;Test data&quot;) plt.scatter(X_reg_test, y_reg_preds, c = &#39;r&#39;, label = &quot;Predictions&quot;) plt.legend(); . We missed to capture the Non-linearity ! . The Missing Piece: Non-linearity: . # Set the seed tf.random.set_seed(42) # 1. Create the model model_4 = tf.keras.Sequential([ tf.keras.layers.Dense(1, activation = tf.keras.activations.linear) ]) # 2. Compile the model model_4.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.001), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history = model_4.fit(X, y, epochs = 100) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Epoch 1/100 32/32 [==============================] - 0s 1ms/step - loss: 4.2979 - accuracy: 0.4670 Epoch 2/100 32/32 [==============================] - 0s 1ms/step - loss: 4.2317 - accuracy: 0.4400 Epoch 3/100 32/32 [==============================] - 0s 1ms/step - loss: 4.1610 - accuracy: 0.4310 Epoch 4/100 32/32 [==============================] - 0s 1ms/step - loss: 4.1183 - accuracy: 0.4270 Epoch 5/100 32/32 [==============================] - 0s 1ms/step - loss: 4.0784 - accuracy: 0.4240 Epoch 6/100 32/32 [==============================] - 0s 1ms/step - loss: 3.9604 - accuracy: 0.4170 Epoch 7/100 32/32 [==============================] - 0s 1ms/step - loss: 3.8936 - accuracy: 0.4110 Epoch 8/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7818 - accuracy: 0.4140 Epoch 9/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7564 - accuracy: 0.4140 Epoch 10/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7547 - accuracy: 0.4150 Epoch 11/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7537 - accuracy: 0.4140 Epoch 12/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7526 - accuracy: 0.4140 Epoch 13/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7518 - accuracy: 0.4140 Epoch 14/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7510 - accuracy: 0.4140 Epoch 15/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7503 - accuracy: 0.4130 Epoch 16/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7497 - accuracy: 0.4130 Epoch 17/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7490 - accuracy: 0.4130 Epoch 18/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7484 - accuracy: 0.4130 Epoch 19/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7478 - accuracy: 0.4120 Epoch 20/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7472 - accuracy: 0.4120 Epoch 21/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7466 - accuracy: 0.4110 Epoch 22/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7461 - accuracy: 0.4110 Epoch 23/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7455 - accuracy: 0.4110 Epoch 24/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7450 - accuracy: 0.4110 Epoch 25/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7366 - accuracy: 0.4120 Epoch 26/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7256 - accuracy: 0.4120 Epoch 27/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7230 - accuracy: 0.4140 Epoch 28/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7215 - accuracy: 0.4140 Epoch 29/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7204 - accuracy: 0.4150 Epoch 30/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7196 - accuracy: 0.4160 Epoch 31/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7188 - accuracy: 0.4170 Epoch 32/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7181 - accuracy: 0.4170 Epoch 33/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7174 - accuracy: 0.4170 Epoch 34/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7167 - accuracy: 0.4170 Epoch 35/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7161 - accuracy: 0.4160 Epoch 36/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7153 - accuracy: 0.4150 Epoch 37/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7147 - accuracy: 0.4150 Epoch 38/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7141 - accuracy: 0.4150 Epoch 39/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7135 - accuracy: 0.4150 Epoch 40/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7129 - accuracy: 0.4150 Epoch 41/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7123 - accuracy: 0.4150 Epoch 42/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7118 - accuracy: 0.4160 Epoch 43/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7112 - accuracy: 0.4160 Epoch 44/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7107 - accuracy: 0.4160 Epoch 45/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7101 - accuracy: 0.4150 Epoch 46/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7096 - accuracy: 0.4170 Epoch 47/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7091 - accuracy: 0.4170 Epoch 48/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7014 - accuracy: 0.4170 Epoch 49/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6892 - accuracy: 0.4190 Epoch 50/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6856 - accuracy: 0.4190 Epoch 51/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6846 - accuracy: 0.4190 Epoch 52/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6837 - accuracy: 0.4190 Epoch 53/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6829 - accuracy: 0.4190 Epoch 54/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6821 - accuracy: 0.4190 Epoch 55/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6814 - accuracy: 0.4190 Epoch 56/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6807 - accuracy: 0.4200 Epoch 57/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6727 - accuracy: 0.4210 Epoch 58/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6690 - accuracy: 0.4190 Epoch 59/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6677 - accuracy: 0.4190 Epoch 60/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6585 - accuracy: 0.4190 Epoch 61/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6489 - accuracy: 0.4200 Epoch 62/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6330 - accuracy: 0.4190 Epoch 63/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6282 - accuracy: 0.4200 Epoch 64/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6162 - accuracy: 0.4220 Epoch 65/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6062 - accuracy: 0.4240 Epoch 66/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6031 - accuracy: 0.4260 Epoch 67/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6012 - accuracy: 0.4280 Epoch 68/100 32/32 [==============================] - 0s 2ms/step - loss: 3.5999 - accuracy: 0.4280 Epoch 69/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5904 - accuracy: 0.4290 Epoch 70/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5858 - accuracy: 0.4300 Epoch 71/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5846 - accuracy: 0.4300 Epoch 72/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5836 - accuracy: 0.4300 Epoch 73/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5826 - accuracy: 0.4310 Epoch 74/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5817 - accuracy: 0.4330 Epoch 75/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5656 - accuracy: 0.4360 Epoch 76/100 32/32 [==============================] - 0s 2ms/step - loss: 3.5355 - accuracy: 0.4410 Epoch 77/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5335 - accuracy: 0.4410 Epoch 78/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5319 - accuracy: 0.4420 Epoch 79/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5306 - accuracy: 0.4420 Epoch 80/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5208 - accuracy: 0.4420 Epoch 81/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5172 - accuracy: 0.4420 Epoch 82/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5157 - accuracy: 0.4430 Epoch 83/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5144 - accuracy: 0.4450 Epoch 84/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5132 - accuracy: 0.4450 Epoch 85/100 32/32 [==============================] - 0s 2ms/step - loss: 3.5043 - accuracy: 0.4460 Epoch 86/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5002 - accuracy: 0.4470 Epoch 87/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4905 - accuracy: 0.4470 Epoch 88/100 32/32 [==============================] - 0s 2ms/step - loss: 3.4788 - accuracy: 0.4470 Epoch 89/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4661 - accuracy: 0.4470 Epoch 90/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4452 - accuracy: 0.4490 Epoch 91/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4198 - accuracy: 0.4560 Epoch 92/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4176 - accuracy: 0.4550 Epoch 93/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4072 - accuracy: 0.4580 Epoch 94/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4036 - accuracy: 0.4580 Epoch 95/100 32/32 [==============================] - 0s 1ms/step - loss: 3.3944 - accuracy: 0.4590 Epoch 96/100 32/32 [==============================] - 0s 1ms/step - loss: 3.3290 - accuracy: 0.4590 Epoch 97/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2727 - accuracy: 0.4600 Epoch 98/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2636 - accuracy: 0.4580 Epoch 99/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2509 - accuracy: 0.4570 Epoch 100/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2459 - accuracy: 0.4570 . . plt.scatter(X[:,0], X[:,1], c = y, cmap = plt.cm.RdYlBu); . plot_decision_boundary(model_4, X, y) . doing binary classification . Let&#39;s try build our first neural network with a non-linear activation function . # Set random seed tf.random.set_seed(42) # 1. Create a model with non-linear activation model_5 = tf.keras.Sequential([ tf.keras.layers.Dense(1, activation = tf.keras.activations.relu) ]) # 2. Compile the model model_5.compile(loss= tf.keras.losses.BinaryCrossentropy(), optimizer = tf.keras.optimizers.Adam(lr = 0.001), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history = model_5.fit(X,y,epochs= 100) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Epoch 1/100 32/32 [==============================] - 0s 2ms/step - loss: 4.2979 - accuracy: 0.4670 Epoch 2/100 32/32 [==============================] - 0s 1ms/step - loss: 4.2317 - accuracy: 0.4400 Epoch 3/100 32/32 [==============================] - 0s 1ms/step - loss: 4.1610 - accuracy: 0.4310 Epoch 4/100 32/32 [==============================] - 0s 2ms/step - loss: 4.1183 - accuracy: 0.4270 Epoch 5/100 32/32 [==============================] - 0s 1ms/step - loss: 4.0784 - accuracy: 0.4240 Epoch 6/100 32/32 [==============================] - 0s 1ms/step - loss: 3.9604 - accuracy: 0.4170 Epoch 7/100 32/32 [==============================] - 0s 1ms/step - loss: 3.8936 - accuracy: 0.4110 Epoch 8/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7818 - accuracy: 0.4140 Epoch 9/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7564 - accuracy: 0.4140 Epoch 10/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7547 - accuracy: 0.4150 Epoch 11/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7537 - accuracy: 0.4140 Epoch 12/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7526 - accuracy: 0.4140 Epoch 13/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7518 - accuracy: 0.4140 Epoch 14/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7510 - accuracy: 0.4140 Epoch 15/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7503 - accuracy: 0.4130 Epoch 16/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7497 - accuracy: 0.4130 Epoch 17/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7490 - accuracy: 0.4130 Epoch 18/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7484 - accuracy: 0.4130 Epoch 19/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7478 - accuracy: 0.4120 Epoch 20/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7472 - accuracy: 0.4120 Epoch 21/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7466 - accuracy: 0.4110 Epoch 22/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7461 - accuracy: 0.4110 Epoch 23/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7455 - accuracy: 0.4110 Epoch 24/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7450 - accuracy: 0.4110 Epoch 25/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7366 - accuracy: 0.4120 Epoch 26/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7256 - accuracy: 0.4120 Epoch 27/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7230 - accuracy: 0.4140 Epoch 28/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7215 - accuracy: 0.4140 Epoch 29/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7204 - accuracy: 0.4150 Epoch 30/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7196 - accuracy: 0.4160 Epoch 31/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7188 - accuracy: 0.4170 Epoch 32/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7181 - accuracy: 0.4170 Epoch 33/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7174 - accuracy: 0.4170 Epoch 34/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7167 - accuracy: 0.4170 Epoch 35/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7161 - accuracy: 0.4160 Epoch 36/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7153 - accuracy: 0.4150 Epoch 37/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7147 - accuracy: 0.4150 Epoch 38/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7141 - accuracy: 0.4150 Epoch 39/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7135 - accuracy: 0.4150 Epoch 40/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7129 - accuracy: 0.4150 Epoch 41/100 32/32 [==============================] - 0s 2ms/step - loss: 3.7123 - accuracy: 0.4150 Epoch 42/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7118 - accuracy: 0.4160 Epoch 43/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7112 - accuracy: 0.4160 Epoch 44/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7107 - accuracy: 0.4160 Epoch 45/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7101 - accuracy: 0.4150 Epoch 46/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7096 - accuracy: 0.4170 Epoch 47/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7091 - accuracy: 0.4170 Epoch 48/100 32/32 [==============================] - 0s 1ms/step - loss: 3.7014 - accuracy: 0.4170 Epoch 49/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6892 - accuracy: 0.4190 Epoch 50/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6856 - accuracy: 0.4190 Epoch 51/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6846 - accuracy: 0.4190 Epoch 52/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6837 - accuracy: 0.4190 Epoch 53/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6829 - accuracy: 0.4190 Epoch 54/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6821 - accuracy: 0.4190 Epoch 55/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6814 - accuracy: 0.4190 Epoch 56/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6807 - accuracy: 0.4200 Epoch 57/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6727 - accuracy: 0.4210 Epoch 58/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6690 - accuracy: 0.4190 Epoch 59/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6677 - accuracy: 0.4190 Epoch 60/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6585 - accuracy: 0.4190 Epoch 61/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6489 - accuracy: 0.4200 Epoch 62/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6330 - accuracy: 0.4190 Epoch 63/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6282 - accuracy: 0.4200 Epoch 64/100 32/32 [==============================] - 0s 2ms/step - loss: 3.6162 - accuracy: 0.4220 Epoch 65/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6062 - accuracy: 0.4240 Epoch 66/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6031 - accuracy: 0.4260 Epoch 67/100 32/32 [==============================] - 0s 1ms/step - loss: 3.6012 - accuracy: 0.4280 Epoch 68/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5999 - accuracy: 0.4280 Epoch 69/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5904 - accuracy: 0.4290 Epoch 70/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5858 - accuracy: 0.4300 Epoch 71/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5846 - accuracy: 0.4300 Epoch 72/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5836 - accuracy: 0.4300 Epoch 73/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5826 - accuracy: 0.4310 Epoch 74/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5817 - accuracy: 0.4330 Epoch 75/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5656 - accuracy: 0.4360 Epoch 76/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5355 - accuracy: 0.4410 Epoch 77/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5335 - accuracy: 0.4410 Epoch 78/100 32/32 [==============================] - 0s 2ms/step - loss: 3.5319 - accuracy: 0.4420 Epoch 79/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5306 - accuracy: 0.4420 Epoch 80/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5208 - accuracy: 0.4420 Epoch 81/100 32/32 [==============================] - 0s 2ms/step - loss: 3.5172 - accuracy: 0.4420 Epoch 82/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5157 - accuracy: 0.4430 Epoch 83/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5144 - accuracy: 0.4450 Epoch 84/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5132 - accuracy: 0.4450 Epoch 85/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5043 - accuracy: 0.4460 Epoch 86/100 32/32 [==============================] - 0s 1ms/step - loss: 3.5002 - accuracy: 0.4470 Epoch 87/100 32/32 [==============================] - 0s 2ms/step - loss: 3.4905 - accuracy: 0.4470 Epoch 88/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4788 - accuracy: 0.4470 Epoch 89/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4661 - accuracy: 0.4470 Epoch 90/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4452 - accuracy: 0.4490 Epoch 91/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4198 - accuracy: 0.4560 Epoch 92/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4176 - accuracy: 0.4550 Epoch 93/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4072 - accuracy: 0.4580 Epoch 94/100 32/32 [==============================] - 0s 1ms/step - loss: 3.4036 - accuracy: 0.4580 Epoch 95/100 32/32 [==============================] - 0s 1ms/step - loss: 3.3944 - accuracy: 0.4590 Epoch 96/100 32/32 [==============================] - 0s 1ms/step - loss: 3.3290 - accuracy: 0.4590 Epoch 97/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2727 - accuracy: 0.4600 Epoch 98/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2636 - accuracy: 0.4580 Epoch 99/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2509 - accuracy: 0.4570 Epoch 100/100 32/32 [==============================] - 0s 1ms/step - loss: 3.2459 - accuracy: 0.4570 . . Still the result looks bad. Our model is performing worse than guessing. . #set the random seed tf.random.set_seed(42) # 1. Create the model model_6 = tf.keras.Sequential([ tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &#39;relu&#39;), tf.keras.layers.Dense(1) ]) # 2. Compile the model model_6.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.001), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history = model_6.fit(X,y, epochs = 250) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Epoch 1/250 32/32 [==============================] - 1s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 2/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 3/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 4/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 5/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 6/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 7/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 8/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 9/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 10/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 11/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 12/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 13/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 14/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 15/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 16/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 17/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 18/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 19/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 20/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 21/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 22/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 23/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 24/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 25/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 26/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 27/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 28/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 29/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 30/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 31/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 32/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 33/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 34/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 35/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 36/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 37/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 38/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 39/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 40/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 41/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 42/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 43/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 44/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 45/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 46/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 47/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 48/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 49/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 50/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 51/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 52/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 53/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 54/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 55/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 56/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 57/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 58/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 59/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 60/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 61/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 62/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 63/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 64/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 65/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 66/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 67/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 68/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 69/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 70/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 71/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 72/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 73/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 74/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 75/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 76/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 77/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 78/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 79/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 80/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 81/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 82/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 83/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 84/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 85/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 86/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 87/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 88/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 89/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 90/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 91/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 92/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 93/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 94/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 95/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 96/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 97/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 98/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 99/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 100/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 101/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 102/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 103/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 104/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 105/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 106/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 107/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 108/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 109/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 110/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 111/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 112/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 113/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 114/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 115/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 116/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 117/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 118/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 119/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 120/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 121/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 122/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 123/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 124/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 125/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 126/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 127/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 128/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 129/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 130/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 131/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 132/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 133/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 134/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 135/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 136/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 137/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 138/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 139/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 140/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 141/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 142/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 143/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 144/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 145/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 146/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 147/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 148/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 149/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 150/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 151/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 152/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 153/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 154/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 155/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 156/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 157/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 158/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 159/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 160/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 161/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 162/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 163/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 164/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 165/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 166/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 167/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 168/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 169/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 170/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 171/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 172/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 173/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 174/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 175/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 176/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 177/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 178/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 179/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 180/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 181/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 182/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 183/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 184/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 185/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 186/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 187/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 188/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 189/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 190/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 191/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 192/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 193/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 194/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 195/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 196/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 197/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 198/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 199/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 200/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 201/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 202/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 203/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 204/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 205/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 206/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 207/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 208/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 209/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 210/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 211/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 212/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 213/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 214/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 215/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 216/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 217/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 218/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 219/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 220/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 221/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 222/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 223/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 224/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 225/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 226/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 227/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 228/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 229/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 230/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 231/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 232/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 233/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 234/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 235/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 236/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 237/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 238/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 239/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 240/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 241/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 242/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 243/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 244/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 245/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 246/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 247/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 248/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 249/250 32/32 [==============================] - 0s 2ms/step - loss: 7.7125 - accuracy: 0.5000 Epoch 250/250 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 . . Let&#39;s evaluate the model to see how it is doing . model_6.evaluate(X,y) . 32/32 [==============================] - 0s 1ms/step - loss: 7.7125 - accuracy: 0.5000 . [7.712474346160889, 0.5] . It&#39;s performing much worse than guessing now. . plot_decision_boundary(model_6, X, y) . doing binary classification . It&#39;s time to fix the model and provide the missing piece and that is . #set the random seed tf.random.set_seed(42) # 1. Create the model model_7 = tf.keras.Sequential([ tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # 2. Compile the model model_7.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.001), metrics = [&quot;accuracy&quot;]) # 3. Fit the model history = model_7.fit(X,y, epochs = 250) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Epoch 1/250 32/32 [==============================] - 1s 2ms/step - loss: 0.6891 - accuracy: 0.5020 Epoch 2/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6848 - accuracy: 0.5170 Epoch 3/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5160 Epoch 4/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.5300 Epoch 5/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6802 - accuracy: 0.5390 Epoch 6/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6794 - accuracy: 0.5380 Epoch 7/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.5500 Epoch 8/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.5480 Epoch 9/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6772 - accuracy: 0.5530 Epoch 10/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6766 - accuracy: 0.5510 Epoch 11/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6760 - accuracy: 0.5500 Epoch 12/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6754 - accuracy: 0.5550 Epoch 13/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6747 - accuracy: 0.5560 Epoch 14/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6741 - accuracy: 0.5540 Epoch 15/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6734 - accuracy: 0.5580 Epoch 16/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6727 - accuracy: 0.5550 Epoch 17/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6722 - accuracy: 0.5550 Epoch 18/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6716 - accuracy: 0.5550 Epoch 19/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6710 - accuracy: 0.5590 Epoch 20/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6702 - accuracy: 0.5600 Epoch 21/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.5640 Epoch 22/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.5600 Epoch 23/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6682 - accuracy: 0.5700 Epoch 24/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6679 - accuracy: 0.5570 Epoch 25/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6671 - accuracy: 0.5670 Epoch 26/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6662 - accuracy: 0.5750 Epoch 27/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6657 - accuracy: 0.5810 Epoch 28/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6645 - accuracy: 0.5700 Epoch 29/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6642 - accuracy: 0.5700 Epoch 30/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6634 - accuracy: 0.5790 Epoch 31/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6627 - accuracy: 0.5760 Epoch 32/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6619 - accuracy: 0.5760 Epoch 33/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6610 - accuracy: 0.5760 Epoch 34/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6606 - accuracy: 0.5760 Epoch 35/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6597 - accuracy: 0.5790 Epoch 36/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6586 - accuracy: 0.5780 Epoch 37/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6580 - accuracy: 0.5780 Epoch 38/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6573 - accuracy: 0.5790 Epoch 39/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6564 - accuracy: 0.5790 Epoch 40/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6556 - accuracy: 0.5760 Epoch 41/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6548 - accuracy: 0.5780 Epoch 42/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6540 - accuracy: 0.5750 Epoch 43/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6530 - accuracy: 0.5780 Epoch 44/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6521 - accuracy: 0.5810 Epoch 45/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6496 - accuracy: 0.5810 Epoch 46/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6481 - accuracy: 0.5830 Epoch 47/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6472 - accuracy: 0.5850 Epoch 48/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6451 - accuracy: 0.5880 Epoch 49/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6426 - accuracy: 0.5880 Epoch 50/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6404 - accuracy: 0.5940 Epoch 51/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6384 - accuracy: 0.6010 Epoch 52/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6364 - accuracy: 0.6130 Epoch 53/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6344 - accuracy: 0.6110 Epoch 54/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6312 - accuracy: 0.6280 Epoch 55/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6287 - accuracy: 0.6380 Epoch 56/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6259 - accuracy: 0.6840 Epoch 57/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6227 - accuracy: 0.6950 Epoch 58/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6200 - accuracy: 0.6990 Epoch 59/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6168 - accuracy: 0.6950 Epoch 60/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6133 - accuracy: 0.7240 Epoch 61/250 32/32 [==============================] - 0s 2ms/step - loss: 0.6101 - accuracy: 0.7200 Epoch 62/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6059 - accuracy: 0.7330 Epoch 63/250 32/32 [==============================] - 0s 1ms/step - loss: 0.6014 - accuracy: 0.7400 Epoch 64/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5966 - accuracy: 0.7460 Epoch 65/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5905 - accuracy: 0.7440 Epoch 66/250 32/32 [==============================] - 0s 1ms/step - loss: 0.5830 - accuracy: 0.7450 Epoch 67/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5757 - accuracy: 0.7460 Epoch 68/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5683 - accuracy: 0.7800 Epoch 69/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5614 - accuracy: 0.8010 Epoch 70/250 32/32 [==============================] - 0s 1ms/step - loss: 0.5548 - accuracy: 0.8010 Epoch 71/250 32/32 [==============================] - 0s 1ms/step - loss: 0.5474 - accuracy: 0.8240 Epoch 72/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5406 - accuracy: 0.8460 Epoch 73/250 32/32 [==============================] - 0s 1ms/step - loss: 0.5327 - accuracy: 0.8470 Epoch 74/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.8620 Epoch 75/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.8870 Epoch 76/250 32/32 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.8820 Epoch 77/250 32/32 [==============================] - 0s 1ms/step - loss: 0.4927 - accuracy: 0.9300 Epoch 78/250 32/32 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.9090 Epoch 79/250 32/32 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.9460 Epoch 80/250 32/32 [==============================] - 0s 2ms/step - loss: 0.4632 - accuracy: 0.9600 Epoch 81/250 32/32 [==============================] - 0s 1ms/step - loss: 0.4546 - accuracy: 0.9600 Epoch 82/250 32/32 [==============================] - 0s 2ms/step - loss: 0.4456 - accuracy: 0.9580 Epoch 83/250 32/32 [==============================] - 0s 1ms/step - loss: 0.4360 - accuracy: 0.9630 Epoch 84/250 32/32 [==============================] - 0s 2ms/step - loss: 0.4260 - accuracy: 0.9690 Epoch 85/250 32/32 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.9700 Epoch 86/250 32/32 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.9740 Epoch 87/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.9750 Epoch 88/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.9720 Epoch 89/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.9750 Epoch 90/250 32/32 [==============================] - 0s 2ms/step - loss: 0.3716 - accuracy: 0.9750 Epoch 91/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.9840 Epoch 92/250 32/32 [==============================] - 0s 2ms/step - loss: 0.3554 - accuracy: 0.9830 Epoch 93/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3482 - accuracy: 0.9800 Epoch 94/250 32/32 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.9820 Epoch 95/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.9880 Epoch 96/250 32/32 [==============================] - 0s 2ms/step - loss: 0.3268 - accuracy: 0.9870 Epoch 97/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.9870 Epoch 98/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.9890 Epoch 99/250 32/32 [==============================] - 0s 1ms/step - loss: 0.3059 - accuracy: 0.9880 Epoch 100/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2993 - accuracy: 0.9890 Epoch 101/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2931 - accuracy: 0.9900 Epoch 102/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.9900 Epoch 103/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.9900 Epoch 104/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.9920 Epoch 105/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2698 - accuracy: 0.9910 Epoch 106/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.9910 Epoch 107/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2589 - accuracy: 0.9900 Epoch 108/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2539 - accuracy: 0.9890 Epoch 109/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2483 - accuracy: 0.9920 Epoch 110/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2433 - accuracy: 0.9920 Epoch 111/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2390 - accuracy: 0.9900 Epoch 112/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2354 - accuracy: 0.9900 Epoch 113/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9910 Epoch 114/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2251 - accuracy: 0.9910 Epoch 115/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9920 Epoch 116/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9920 Epoch 117/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2130 - accuracy: 0.9900 Epoch 118/250 32/32 [==============================] - 0s 2ms/step - loss: 0.2092 - accuracy: 0.9910 Epoch 119/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9910 Epoch 120/250 32/32 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9890 Epoch 121/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9920 Epoch 122/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9910 Epoch 123/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.9920 Epoch 124/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9910 Epoch 125/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1839 - accuracy: 0.9900 Epoch 126/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1809 - accuracy: 0.9920 Epoch 127/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1771 - accuracy: 0.9910 Epoch 128/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9920 Epoch 129/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1712 - accuracy: 0.9930 Epoch 130/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9930 Epoch 131/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9910 Epoch 132/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1630 - accuracy: 0.9930 Epoch 133/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9910 Epoch 134/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9910 Epoch 135/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9940 Epoch 136/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9920 Epoch 137/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9920 Epoch 138/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9930 Epoch 139/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1459 - accuracy: 0.9920 Epoch 140/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9910 Epoch 141/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9930 Epoch 142/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9940 Epoch 143/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9920 Epoch 144/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9930 Epoch 145/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1332 - accuracy: 0.9920 Epoch 146/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1314 - accuracy: 0.9910 Epoch 147/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9900 Epoch 148/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9930 Epoch 149/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9930 Epoch 150/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9920 Epoch 151/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1229 - accuracy: 0.9910 Epoch 152/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9920 Epoch 153/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.9930 Epoch 154/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9920 Epoch 155/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9940 Epoch 156/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.9930 Epoch 157/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.9920 Epoch 158/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9930 Epoch 159/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1099 - accuracy: 0.9920 Epoch 160/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9900 Epoch 161/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1079 - accuracy: 0.9930 Epoch 162/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9930 Epoch 163/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1046 - accuracy: 0.9940 Epoch 164/250 32/32 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9930 Epoch 165/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1027 - accuracy: 0.9890 Epoch 166/250 32/32 [==============================] - 0s 2ms/step - loss: 0.1010 - accuracy: 0.9930 Epoch 167/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0996 - accuracy: 0.9930 Epoch 168/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9920 Epoch 169/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9930 Epoch 170/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9920 Epoch 171/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9920 Epoch 172/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9920 Epoch 173/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9930 Epoch 174/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0915 - accuracy: 0.9920 Epoch 175/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9910 Epoch 176/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9930 Epoch 177/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9940 Epoch 178/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9900 Epoch 179/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0869 - accuracy: 0.9920 Epoch 180/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9930 Epoch 181/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9930 Epoch 182/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9920 Epoch 183/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9920 Epoch 184/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0823 - accuracy: 0.9920 Epoch 185/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9940 Epoch 186/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9930 Epoch 187/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 0.9940 Epoch 188/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9920 Epoch 189/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9930 Epoch 190/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9930 Epoch 191/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9920 Epoch 192/250 32/32 [==============================] - 0s 5ms/step - loss: 0.0759 - accuracy: 0.9910 Epoch 193/250 32/32 [==============================] - 0s 7ms/step - loss: 0.0748 - accuracy: 0.9910 Epoch 194/250 32/32 [==============================] - 0s 6ms/step - loss: 0.0742 - accuracy: 0.9920 Epoch 195/250 32/32 [==============================] - 0s 3ms/step - loss: 0.0735 - accuracy: 0.9930 Epoch 196/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9920 Epoch 197/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9920 Epoch 198/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0716 - accuracy: 0.9930 Epoch 199/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9920 Epoch 200/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9920 Epoch 201/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9930 Epoch 202/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0686 - accuracy: 0.9930 Epoch 203/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0682 - accuracy: 0.9930 Epoch 204/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9920 Epoch 205/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0668 - accuracy: 0.9930 Epoch 206/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9930 Epoch 207/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9920 Epoch 208/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0654 - accuracy: 0.9930 Epoch 209/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9920 Epoch 210/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9920 Epoch 211/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9920 Epoch 212/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.9920 Epoch 213/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0623 - accuracy: 0.9920 Epoch 214/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9930 Epoch 215/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9920 Epoch 216/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9910 Epoch 217/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9930 Epoch 218/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9910 Epoch 219/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9930 Epoch 220/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.9920 Epoch 221/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9930 Epoch 222/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9920 Epoch 223/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9930 Epoch 224/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9930 Epoch 225/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9950 Epoch 226/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0560 - accuracy: 0.9920 Epoch 227/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9950 Epoch 228/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9920 Epoch 229/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9920 Epoch 230/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9930 Epoch 231/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9930 Epoch 232/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9920 Epoch 233/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0529 - accuracy: 0.9930 Epoch 234/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 0.9930 Epoch 235/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0519 - accuracy: 0.9930 Epoch 236/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0515 - accuracy: 0.9930 Epoch 237/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.9900 Epoch 238/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 0.9920 Epoch 239/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9930 Epoch 240/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9940 Epoch 241/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9920 Epoch 242/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0494 - accuracy: 0.9930 Epoch 243/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0491 - accuracy: 0.9930 Epoch 244/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9920 Epoch 245/250 32/32 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9940 Epoch 246/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0483 - accuracy: 0.9930 Epoch 247/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9950 Epoch 248/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9930 Epoch 249/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0471 - accuracy: 0.9950 Epoch 250/250 32/32 [==============================] - 0s 2ms/step - loss: 0.0471 - accuracy: 0.9940 . . model_7.evaluate(X,y) . 32/32 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9920 . [0.04593363776803017, 0.9919999837875366] . plot_decision_boundary(model_7,X,y) . doing binary classification . Wow, we are very close our model segregated the data points almost all the blue ones are inside the blue decision boundary. . Note: The combination of linear(straight lines) and non-linear(non-straight lines) functions is one of they key fundamentals of neural networks. . A = tf.cast(tf.range(-10,10), tf.float32) A . &lt;tf.Tensor: shape=(20,), dtype=float32, numpy= array([-10., -9., -8., -7., -6., -5., -4., -3., -2., -1., 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt; . plt.plot(A); . It&#39;s a straight line!! . def sigmoid(x): return 1/(1+tf.exp(-x)) # Use the sigmoid function on our toy tensor sigmoid(A) . &lt;tf.Tensor: shape=(20,), dtype=float32, numpy= array([4.5397872e-05, 1.2339458e-04, 3.3535014e-04, 9.1105117e-04, 2.4726233e-03, 6.6928510e-03, 1.7986210e-02, 4.7425874e-02, 1.1920292e-01, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01, 8.8079703e-01, 9.5257413e-01, 9.8201376e-01, 9.9330717e-01, 9.9752742e-01, 9.9908900e-01, 9.9966466e-01, 9.9987662e-01], dtype=float32)&gt; . plt.plot(sigmoid(A)); . The sigmoid function transformed the linear graph into non-linear plot . def relu(x): return tf.maximum(0,x) relu(A) . &lt;tf.Tensor: shape=(20,), dtype=float32, numpy= array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt; . plt.plot(relu(A)); . tf.keras.activations.linear(A) . &lt;tf.Tensor: shape=(20,), dtype=float32, numpy= array([-10., -9., -8., -7., -6., -5., -4., -3., -2., -1., 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)&gt; . plt.plot(tf.keras.activations.linear(A)); . So, a linear function doesn&#39;t do anything for our input data. So for our data which had non-linear relation it failed to plot the best decision boundary. . A brief review of standard activation functions: . Linear: tf.keras.activations.linear(A) | Sigmoid: tf.keras.activations.sigmoid(A) | ReLU: tf.keras.activations.relu(A) | . Evaluating and improving our classification model . so far we have been training and testing on the same data set. But we risk the model overfitting more to training data and achieve high accuracy on train set but fail to give us correct predictions on test set. And that is disastrous for an ML application. Three sets of data: . Training set | Validation set | Test set | . X_train, y_train = X[:800], y[:800] X_test, y_test = X[800:], y[800:] X_train.shape, X_test.shape, y_train.shape, y_test.shape . ((800, 2), (200, 2), (800,), (200,)) . # Let&#39;s create a model to fit on the training data and evaluate on the testing data # Set the random seed tf.random.set_seed(42) # 1. Create the model (same as model_7) model_8 = tf.keras.Sequential([ tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # 2. Compile the Modle model_8.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.01), metrics = [&quot;accuracy&quot;]) # 3. Fit the Model history = model_8.fit(X_train, y_train, epochs = 25) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Epoch 1/25 25/25 [==============================] - 1s 2ms/step - loss: 0.6847 - accuracy: 0.5425 Epoch 2/25 25/25 [==============================] - 0s 2ms/step - loss: 0.6777 - accuracy: 0.5525 Epoch 3/25 25/25 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.5512 Epoch 4/25 25/25 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.5775 Epoch 5/25 25/25 [==============================] - 0s 3ms/step - loss: 0.6633 - accuracy: 0.5850 Epoch 6/25 25/25 [==============================] - 0s 2ms/step - loss: 0.6546 - accuracy: 0.5838 Epoch 7/25 25/25 [==============================] - 0s 3ms/step - loss: 0.6413 - accuracy: 0.6750 Epoch 8/25 25/25 [==============================] - 0s 3ms/step - loss: 0.6264 - accuracy: 0.7013 Epoch 9/25 25/25 [==============================] - 0s 3ms/step - loss: 0.6038 - accuracy: 0.7487 Epoch 10/25 25/25 [==============================] - 0s 2ms/step - loss: 0.5714 - accuracy: 0.7738 Epoch 11/25 25/25 [==============================] - 0s 3ms/step - loss: 0.5404 - accuracy: 0.7650 Epoch 12/25 25/25 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7837 Epoch 13/25 25/25 [==============================] - 0s 3ms/step - loss: 0.4683 - accuracy: 0.7975 Epoch 14/25 25/25 [==============================] - 0s 3ms/step - loss: 0.4113 - accuracy: 0.8450 Epoch 15/25 25/25 [==============================] - 0s 3ms/step - loss: 0.3625 - accuracy: 0.9125 Epoch 16/25 25/25 [==============================] - 0s 3ms/step - loss: 0.3209 - accuracy: 0.9312 Epoch 17/25 25/25 [==============================] - 0s 3ms/step - loss: 0.2847 - accuracy: 0.9488 Epoch 18/25 25/25 [==============================] - 0s 3ms/step - loss: 0.2597 - accuracy: 0.9525 Epoch 19/25 25/25 [==============================] - 0s 3ms/step - loss: 0.2375 - accuracy: 0.9563 Epoch 20/25 25/25 [==============================] - 0s 2ms/step - loss: 0.2135 - accuracy: 0.9663 Epoch 21/25 25/25 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9775 Epoch 22/25 25/25 [==============================] - 0s 3ms/step - loss: 0.1752 - accuracy: 0.9737 Epoch 23/25 25/25 [==============================] - 0s 3ms/step - loss: 0.1619 - accuracy: 0.9787 Epoch 24/25 25/25 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.9775 Epoch 25/25 25/25 [==============================] - 0s 3ms/step - loss: 0.1490 - accuracy: 0.9762 . . model_8.evaluate(X_test, y_test) . 7/7 [==============================] - 0s 6ms/step - loss: 0.1247 - accuracy: 1.0000 . [0.1246885135769844, 1.0] . plt.figure(figsize = (12,6)) plt.subplot(1,2,1) plt.title(&quot;Train&quot;) plot_decision_boundary(model_8, X_train, y_train) plt.subplot(1,2,2) plt.title(&quot;Test&quot;) plot_decision_boundary(model_8, X_test, y_test) . doing binary classification doing binary classification . Excellent!! our model performed with 100% accuracy on the test set and that too with 25 epochs (because we have turned up the learning rate by 10X) . model_8.summary() . Model: &#34;sequential_8&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_17 (Dense) (32, 4) 12 dense_18 (Dense) (32, 4) 20 dense_19 (Dense) (32, 1) 5 ================================================================= Total params: 37 Trainable params: 37 Non-trainable params: 0 _________________________________________________________________ . Plot the Loss or Training Curve: . history.history . {&#39;accuracy&#39;: [0.5425000190734863, 0.5525000095367432, 0.5512499809265137, 0.5774999856948853, 0.5849999785423279, 0.5837500095367432, 0.675000011920929, 0.7012500166893005, 0.7487499713897705, 0.7737500071525574, 0.7649999856948853, 0.7837499976158142, 0.7975000143051147, 0.8450000286102295, 0.9125000238418579, 0.9312499761581421, 0.9487500190734863, 0.9524999856948853, 0.956250011920929, 0.9662500023841858, 0.9775000214576721, 0.9737499952316284, 0.9787499904632568, 0.9775000214576721, 0.9762499928474426], &#39;loss&#39;: [0.6846511960029602, 0.6777209639549255, 0.6735945343971252, 0.6681485772132874, 0.6632686853408813, 0.6545672416687012, 0.6412575244903564, 0.6264281272888184, 0.6038310527801514, 0.5714036226272583, 0.540442943572998, 0.5015039443969727, 0.46833187341690063, 0.4113016128540039, 0.3625059425830841, 0.32090437412261963, 0.2847079932689667, 0.25971999764442444, 0.23746901750564575, 0.21351958811283112, 0.1938202977180481, 0.17524366080760956, 0.16189303994178772, 0.1549890637397766, 0.14897282421588898]} . . # Convert the history object into a DataFrame pd.DataFrame(history.history) . loss accuracy . 0 0.684651 | 0.54250 | . 1 0.677721 | 0.55250 | . 2 0.673595 | 0.55125 | . 3 0.668149 | 0.57750 | . 4 0.663269 | 0.58500 | . 5 0.654567 | 0.58375 | . 6 0.641258 | 0.67500 | . 7 0.626428 | 0.70125 | . 8 0.603831 | 0.74875 | . 9 0.571404 | 0.77375 | . 10 0.540443 | 0.76500 | . 11 0.501504 | 0.78375 | . 12 0.468332 | 0.79750 | . 13 0.411302 | 0.84500 | . 14 0.362506 | 0.91250 | . 15 0.320904 | 0.93125 | . 16 0.284708 | 0.94875 | . 17 0.259720 | 0.95250 | . 18 0.237469 | 0.95625 | . 19 0.213520 | 0.96625 | . 20 0.193820 | 0.97750 | . 21 0.175244 | 0.97375 | . 22 0.161893 | 0.97875 | . 23 0.154989 | 0.97750 | . 24 0.148973 | 0.97625 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . pd.DataFrame(history.history).plot() plt.title(&quot;Loss curve of Model_8&quot;) . Text(0.5, 1.0, &#39;Loss curve of Model_8&#39;) . Note: For many problems, the loss function going down means the model is improving (the predictions it&#39;s maksing are getting closer to the ground truth labels) . Finding the best learning rate . To find the ideal learning rate (the learning rate where the loss decreases the most during training) we&#39;re going to use the following steps: . A learning rate callback - you can think of a call back as an extra piece of functionality, you can add to your model while training | Create another model | A modified loss curve plot | . # Set the random seed tf.random.set_seed(42) # Create a model (same as model_8) model_9 = tf.keras.Sequential([ tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # Compile the Model model_9.compile(loss = &quot;binary_crossentropy&quot;, optimizer = &quot;Adam&quot;, metrics = [&quot;accuracy&quot;]) # Create a learning rate call back lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20)) # Fit the model history_9 = model_9.fit(X_train, y_train, epochs = 100, callbacks = [lr_scheduler]) . Epoch 1/100 25/25 [==============================] - 1s 3ms/step - loss: 0.6945 - accuracy: 0.4988 - lr: 1.0000e-04 Epoch 2/100 25/25 [==============================] - 0s 4ms/step - loss: 0.6938 - accuracy: 0.4975 - lr: 1.1220e-04 Epoch 3/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6930 - accuracy: 0.4963 - lr: 1.2589e-04 Epoch 4/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6922 - accuracy: 0.4975 - lr: 1.4125e-04 Epoch 5/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6914 - accuracy: 0.5063 - lr: 1.5849e-04 Epoch 6/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6906 - accuracy: 0.5013 - lr: 1.7783e-04 Epoch 7/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6898 - accuracy: 0.4950 - lr: 1.9953e-04 Epoch 8/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6889 - accuracy: 0.5038 - lr: 2.2387e-04 Epoch 9/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6880 - accuracy: 0.5013 - lr: 2.5119e-04 Epoch 10/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6871 - accuracy: 0.5050 - lr: 2.8184e-04 Epoch 11/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6863 - accuracy: 0.5200 - lr: 3.1623e-04 Epoch 12/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6856 - accuracy: 0.5163 - lr: 3.5481e-04 Epoch 13/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5175 - lr: 3.9811e-04 Epoch 14/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6842 - accuracy: 0.5200 - lr: 4.4668e-04 Epoch 15/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.5213 - lr: 5.0119e-04 Epoch 16/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.5213 - lr: 5.6234e-04 Epoch 17/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.5225 - lr: 6.3096e-04 Epoch 18/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6819 - accuracy: 0.5300 - lr: 7.0795e-04 Epoch 19/100 25/25 [==============================] - 0s 3ms/step - loss: 0.6816 - accuracy: 0.5312 - lr: 7.9433e-04 Epoch 20/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6811 - accuracy: 0.5387 - lr: 8.9125e-04 Epoch 21/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6806 - accuracy: 0.5400 - lr: 0.0010 Epoch 22/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6801 - accuracy: 0.5412 - lr: 0.0011 Epoch 23/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6796 - accuracy: 0.5400 - lr: 0.0013 Epoch 24/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6790 - accuracy: 0.5425 - lr: 0.0014 Epoch 25/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6784 - accuracy: 0.5450 - lr: 0.0016 Epoch 26/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6778 - accuracy: 0.5387 - lr: 0.0018 Epoch 27/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6770 - accuracy: 0.5425 - lr: 0.0020 Epoch 28/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6760 - accuracy: 0.5537 - lr: 0.0022 Epoch 29/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6754 - accuracy: 0.5512 - lr: 0.0025 Epoch 30/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6739 - accuracy: 0.5575 - lr: 0.0028 Epoch 31/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6726 - accuracy: 0.5500 - lr: 0.0032 Epoch 32/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6711 - accuracy: 0.5512 - lr: 0.0035 Epoch 33/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6688 - accuracy: 0.5562 - lr: 0.0040 Epoch 34/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6672 - accuracy: 0.5612 - lr: 0.0045 Epoch 35/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6660 - accuracy: 0.5888 - lr: 0.0050 Epoch 36/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6625 - accuracy: 0.5625 - lr: 0.0056 Epoch 37/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.5813 - lr: 0.0063 Epoch 38/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6521 - accuracy: 0.6025 - lr: 0.0071 Epoch 39/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.7088 - lr: 0.0079 Epoch 40/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6210 - accuracy: 0.7113 - lr: 0.0089 Epoch 41/100 25/25 [==============================] - 0s 1ms/step - loss: 0.5904 - accuracy: 0.7487 - lr: 0.0100 Epoch 42/100 25/25 [==============================] - 0s 2ms/step - loss: 0.5688 - accuracy: 0.7312 - lr: 0.0112 Epoch 43/100 25/25 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7563 - lr: 0.0126 Epoch 44/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4533 - accuracy: 0.8150 - lr: 0.0141 Epoch 45/100 25/25 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.9112 - lr: 0.0158 Epoch 46/100 25/25 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.9463 - lr: 0.0178 Epoch 47/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1968 - accuracy: 0.9575 - lr: 0.0200 Epoch 48/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.9700 - lr: 0.0224 Epoch 49/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9613 - lr: 0.0251 Epoch 50/100 25/25 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9700 - lr: 0.0282 Epoch 51/100 25/25 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9638 - lr: 0.0316 Epoch 52/100 25/25 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9513 - lr: 0.0355 Epoch 53/100 25/25 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9787 - lr: 0.0398 Epoch 54/100 25/25 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9588 - lr: 0.0447 Epoch 55/100 25/25 [==============================] - 0s 2ms/step - loss: 0.0733 - accuracy: 0.9712 - lr: 0.0501 Epoch 56/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9550 - lr: 0.0562 Epoch 57/100 25/25 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9613 - lr: 0.0631 Epoch 58/100 25/25 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9750 - lr: 0.0708 Epoch 59/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1898 - accuracy: 0.9275 - lr: 0.0794 Epoch 60/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1895 - accuracy: 0.9312 - lr: 0.0891 Epoch 61/100 25/25 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8612 - lr: 0.1000 Epoch 62/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9725 - lr: 0.1122 Epoch 63/100 25/25 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9937 - lr: 0.1259 Epoch 64/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9638 - lr: 0.1413 Epoch 65/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1323 - accuracy: 0.9488 - lr: 0.1585 Epoch 66/100 25/25 [==============================] - 0s 2ms/step - loss: 0.1819 - accuracy: 0.9375 - lr: 0.1778 Epoch 67/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6672 - accuracy: 0.7613 - lr: 0.1995 Epoch 68/100 25/25 [==============================] - 0s 1ms/step - loss: 0.5301 - accuracy: 0.6687 - lr: 0.2239 Epoch 69/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4140 - accuracy: 0.7925 - lr: 0.2512 Epoch 70/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.7412 - lr: 0.2818 Epoch 71/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4759 - accuracy: 0.7262 - lr: 0.3162 Epoch 72/100 25/25 [==============================] - 0s 2ms/step - loss: 0.3748 - accuracy: 0.8112 - lr: 0.3548 Epoch 73/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8150 - lr: 0.3981 Epoch 74/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4143 - accuracy: 0.8087 - lr: 0.4467 Epoch 75/100 25/25 [==============================] - 0s 2ms/step - loss: 0.5961 - accuracy: 0.7412 - lr: 0.5012 Epoch 76/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7713 - lr: 0.5623 Epoch 77/100 25/25 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.7113 - lr: 0.6310 Epoch 78/100 25/25 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.8675 - lr: 0.7079 Epoch 79/100 25/25 [==============================] - 0s 2ms/step - loss: 1.1824 - accuracy: 0.6275 - lr: 0.7943 Epoch 80/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6873 - accuracy: 0.5425 - lr: 0.8913 Epoch 81/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7068 - accuracy: 0.5575 - lr: 1.0000 Epoch 82/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6879 - accuracy: 0.5838 - lr: 1.1220 Epoch 83/100 25/25 [==============================] - 0s 2ms/step - loss: 0.6996 - accuracy: 0.5700 - lr: 1.2589 Epoch 84/100 25/25 [==============================] - 0s 1ms/step - loss: 0.6471 - accuracy: 0.5863 - lr: 1.4125 Epoch 85/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7457 - accuracy: 0.5312 - lr: 1.5849 Epoch 86/100 25/25 [==============================] - 0s 1ms/step - loss: 0.7546 - accuracy: 0.5038 - lr: 1.7783 Epoch 87/100 25/25 [==============================] - 0s 1ms/step - loss: 0.7681 - accuracy: 0.5063 - lr: 1.9953 Epoch 88/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7596 - accuracy: 0.4963 - lr: 2.2387 Epoch 89/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7778 - accuracy: 0.5063 - lr: 2.5119 Epoch 90/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7741 - accuracy: 0.4787 - lr: 2.8184 Epoch 91/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7851 - accuracy: 0.5163 - lr: 3.1623 Epoch 92/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7441 - accuracy: 0.4888 - lr: 3.5481 Epoch 93/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7354 - accuracy: 0.5163 - lr: 3.9811 Epoch 94/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7548 - accuracy: 0.4938 - lr: 4.4668 Epoch 95/100 25/25 [==============================] - 0s 2ms/step - loss: 0.8087 - accuracy: 0.4863 - lr: 5.0119 Epoch 96/100 25/25 [==============================] - 0s 2ms/step - loss: 0.7714 - accuracy: 0.4638 - lr: 5.6234 Epoch 97/100 25/25 [==============================] - 0s 2ms/step - loss: 0.8001 - accuracy: 0.5013 - lr: 6.3096 Epoch 98/100 25/25 [==============================] - 0s 1ms/step - loss: 0.9554 - accuracy: 0.4963 - lr: 7.0795 Epoch 99/100 25/25 [==============================] - 0s 1ms/step - loss: 0.9268 - accuracy: 0.4913 - lr: 7.9433 Epoch 100/100 25/25 [==============================] - 0s 2ms/step - loss: 0.8563 - accuracy: 0.4663 - lr: 8.9125 . . pd.DataFrame(history_9.history).plot(figsize=(10,7), xlabel = &quot;epochs&quot;); . # Plot the learning rate versus the loss lrs = 1e-4 * (10**(tf.range(100)/20)) plt.figure(figsize=(10,7)) plt.semilogx(lrs, history_9.history[&quot;loss&quot;]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;Loss&quot;) plt.title(&quot;learning rate vs. Loss&quot;); . . Let&#39;s analyze the plot and pick the best learning rate. You can use this plot to pick the learning rate when you are trying to improve the performance of your model. genearally the default learning rates that come with the pre-built functions works well too. . In this case, pick the learning rate around the value where the the loss decreases faster. . 10**0, 10**-1, 10**-2, 10**-3, 10**-4 . (1, 0.1, 0.01, 0.001, 0.0001) . Pick the value 0.02 where the loss is still decreasing steadily. . 10**-2 . 0.01 . # Let&#39;s try using a higher *ideal* learning rate with the same model # Set the random seed tf.random.set_seed(42) # Create a model (same as model_8) model_10 = tf.keras.Sequential([ tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = &quot;sigmoid&quot;) ]) # Compile the Model model_10.compile(loss = &quot;binary_crossentropy&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.02), metrics = [&quot;accuracy&quot;]) # Fit the model with 20 epochs(5 less than before) history_10 = model_10.fit(X_train, y_train, epochs = 20) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . Epoch 1/20 25/25 [==============================] - 0s 1ms/step - loss: 0.6837 - accuracy: 0.5600 Epoch 2/20 25/25 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.5750 Epoch 3/20 25/25 [==============================] - 0s 1ms/step - loss: 0.6626 - accuracy: 0.5875 Epoch 4/20 25/25 [==============================] - 0s 1ms/step - loss: 0.6332 - accuracy: 0.6388 Epoch 5/20 25/25 [==============================] - 0s 2ms/step - loss: 0.5830 - accuracy: 0.7563 Epoch 6/20 25/25 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.8313 Epoch 7/20 25/25 [==============================] - 0s 2ms/step - loss: 0.4251 - accuracy: 0.8450 Epoch 8/20 25/25 [==============================] - 0s 1ms/step - loss: 0.3596 - accuracy: 0.8875 Epoch 9/20 25/25 [==============================] - 0s 2ms/step - loss: 0.3152 - accuracy: 0.9100 Epoch 10/20 25/25 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9500 Epoch 11/20 25/25 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9500 Epoch 12/20 25/25 [==============================] - 0s 2ms/step - loss: 0.1721 - accuracy: 0.9750 Epoch 13/20 25/25 [==============================] - 0s 2ms/step - loss: 0.1443 - accuracy: 0.9837 Epoch 14/20 25/25 [==============================] - 0s 2ms/step - loss: 0.1232 - accuracy: 0.9862 Epoch 15/20 25/25 [==============================] - 0s 2ms/step - loss: 0.1085 - accuracy: 0.9850 Epoch 16/20 25/25 [==============================] - 0s 2ms/step - loss: 0.0940 - accuracy: 0.9937 Epoch 17/20 25/25 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9962 Epoch 18/20 25/25 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9937 Epoch 19/20 25/25 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9875 Epoch 20/20 25/25 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9887 . . model_10.evaluate(X_test,y_test) . 7/7 [==============================] - 0s 3ms/step - loss: 0.0574 - accuracy: 0.9900 . [0.05740184709429741, 0.9900000095367432] . model_8.evaluate(X_test,y_test) . 7/7 [==============================] - 0s 3ms/step - loss: 0.1247 - accuracy: 1.0000 . [0.1246885135769844, 1.0] . Which model should we chooose? one has got the best accuracy and the other has very low loss value. . plt.figure(figsize = (12,6)) plt.subplot(1,2,1) plt.title(&quot;Train&quot;) plot_decision_boundary(model_10, X_train, y_train) plt.subplot(1,2,2) plt.title(&quot;Test&quot;) plot_decision_boundary(model_10, X_test,y_test) plt.show(); . doing binary classification doing binary classification . More classification evaluation methods . Alongside visualizing our models results as much as possible, there are a handful of other classification evaluation methods &amp; metrics should be familiar with: . Classification Evaluation methods: . Keys: tp = true-positive, tn = true-negative , fp = false-positive, fn = false-negative. . Metric Name Metric Formula Code When to use . Accuracy | Accuracy = $ frac{tp=tn}{tp+tn+fp+fn}$ | tf.keras.metrics.Accuracy() or sklearn.metrics.accuracy_score() | Default metric for classification problems. Not the bset for imbalanced cases | . Precision | Precision = $ frac{tp}{tp+fp}$ | tf.keras.metrics.Precision() or sklearn.metrics.precision_score() | High precision lead to less false positives | . Recall | Recall = $ frac{tp}{tp+fn}$ | tf.keras.metrics.Recall() or sklearn.metrics.recall_score() | Higher recall leads to less false negatives | . F1-Score | F1-Score = $2 frac{precision cdot recall}{precision + recall}$ | sklearn.metrics.f1_score() | Combination of precision and recall, usually a good overall metric for a classification model | . Confusion matrix | NA | custom function or sklearn.metrics.confusion_matrix() | When comparing predictions to truth labels to see where model gets confused. Can be hard to use with large number of classes | . loss, accuracy = model_10.evaluate(X_test, y_test) print(f&quot;Model loss on the test set: {loss}&quot;) print(f&quot;Model accuracy on the test set: {(accuracy*100) : .2f}% &quot;) . 7/7 [==============================] - 0s 4ms/step - loss: 0.0574 - accuracy: 0.9900 Model loss on the test set: 0.05740184709429741 Model accuracy on the test set: 99.00% . Anatomy of confusion matrix . True positive = model predicts 1 when truth is 1 | True negative = model predicts 0 when truth is 0 | False positive = model predicts 1 when truth is 0 | False negative = model predicts 0 when truth is 1 | . In the confusion matrix the correct predictions are along the diagonals.(i.e. True positives, True negatives) . from sklearn.metrics import confusion_matrix # Make predictions y_preds = model_10.predict(X_test) # Create confusion matrix confusion_matrix(y_test, y_preds) . ValueError Traceback (most recent call last) &lt;ipython-input-68-c579ca7d54a1&gt; in &lt;module&gt;() 6 7 # Create confusion matrix -&gt; 8 confusion_matrix(y_test,y_preds) /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in confusion_matrix(y_true, y_pred, labels, sample_weight, normalize) 305 (0, 2, 1, 1) 306 &#34;&#34;&#34; --&gt; 307 y_type, y_true, y_pred = _check_targets(y_true, y_pred) 308 if y_type not in (&#34;binary&#34;, &#34;multiclass&#34;): 309 raise ValueError(&#34;%s is not supported&#34; % y_type) /usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py in _check_targets(y_true, y_pred) 93 raise ValueError( 94 &#34;Classification metrics can&#39;t handle a mix of {0} and {1} targets&#34;.format( &gt; 95 type_true, type_pred 96 ) 97 ) ValueError: Classification metrics can&#39;t handle a mix of binary and continuous targets . Let&#39;s check the values of y_preds we passed in . y_preds[1:10] . array([[9.9923790e-01], [9.9032348e-01], [9.9706942e-01], [3.9622977e-01], [1.8126935e-02], [9.6829069e-01], [1.9746721e-02], [9.9967170e-01], [5.6460500e-04]], dtype=float32) . Looks like our predictions array has come out in prediction probability form. The standard output from the sigmoid(or softmax) . tf.round(y_preds)[:10] . &lt;tf.Tensor: shape=(10, 1), dtype=float32, numpy= array([[1.], [1.], [1.], [1.], [0.], [0.], [1.], [0.], [1.], [0.]], dtype=float32)&gt; . confusion_matrix(y_test, tf.round(y_preds)) . array([[99, 2], [ 0, 99]]) . Now, we got our confusion matrix let&#39;s evaluate and make the confusion matrix in much more understandable form . import itertools figsize = (10,10) # Create the confusion matrix cm = confusion_matrix(y_test, tf.round(y_preds)) cm_norm = cm.astype(&quot;float&quot;)/ cm.sum(axis = 1)[:, np.newaxis] # Noramlize our confusion matrix n_classes = cm.shape[0] # Let&#39;s make it neat and clear fig, ax = plt.subplots(figsize = figsize) # Create a matrix plot cax = ax.matshow(cm, cmap = plt.cm.Blues) fig.colorbar(cax) # Create classes classes = False if classes: labels = classes else: labels = np.arange(cm.shape[0]) # Let&#39;s label the axis ax.set(title = &quot;Confusion Matrix&quot;, xlabel = &quot;Predicted Label&quot;, ylabel = &quot;True Label&quot;, xticks = np.arange(n_classes), yticks = np.arange(n_classes), xticklabels = labels, yticklabels = labels) # Set x-axis labels to the bottom ax.xaxis.set_label_position(&quot;bottom&quot;) ax.xaxis.tick_bottom() # ADjust label size ax.yaxis.label.set_size(20) ax.xaxis.label.set_size(20) ax.title.set_size(20) # Set threshold for different colors threshold = (cm.max() + cm.min())/2. # Plot the text on each cell for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, f&quot;{cm[i,j]} ({cm_norm[i,j]*100:.1f}% )&quot;, horizontalalignment = &quot;center&quot;, color = &quot;white&quot; if cm[i,j] &gt; threshold else &quot;black&quot;, size = 15) . We will try to code up a function to reuse the plotting of confusion matrix for another time . def plot_confusion_matrix(y_test,y_preds): import itertools figsize = (10,10) # Create the confusion matrix cm = confusion_matrix(y_test, tf.round(y_preds)) cm_norm = cm.astype(&quot;float&quot;)/ cm.sum(axis = 1)[:, np.newaxis] # Noramlize our confusion matrix n_classes = cm.shape[0] # Let&#39;s make it neat and clear fig, ax = plt.subplots(figsize = figsize) # Create a matrix plot cax = ax.matshow(cm, cmap = plt.cm.Blues) fig.colorbar(cax) # Create classes classes = False if classes: labels = classes else: labels = np.arange(cm.shape[0]) # Let&#39;s label the axis ax.set(title = &quot;Confusion Matrix&quot;, xlabel = &quot;Predicted Label&quot;, ylabel = &quot;True Label&quot;, xticks = np.arange(n_classes), yticks = np.arange(n_classes), xticklabels = labels, yticklabels = labels) # Set x-axis labels to the bottom ax.xaxis.set_label_position(&quot;bottom&quot;) ax.xaxis.tick_bottom() # Adjust label size ax.yaxis.label.set_size(20) ax.xaxis.label.set_size(20) ax.title.set_size(20) # Set threshold for different colors threshold = (cm.max() + cm.min())/2. # Plot the text on each cell for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, f&quot;{cm[i,j]} ({cm_norm[i,j]*100:.1f}% )&quot;, horizontalalignment = &quot;center&quot;, color = &quot;white&quot; if cm[i,j] &gt; threshold else &quot;black&quot;, size = 15) . plot_confusion_matrix(y_test,y_preds) . Great, our function is working and we can reuse the function for other models from now on so that we can save time rewriting all the code for the above plot. . Working with larger examples (Multi-class Classification) . When you have more than two classes as an option, it is known as multi-class classification. . This means if you have 3 different classes, it&#39;s multi-class classification | It aslo means if we have hundred different classes, it&#39;s a multi-class classification. | . To practice mutli-class classification, we are going to build a neural netowrk to classify images of different items of clothing. . import tensorflow as tf from tensorflow.keras.datasets import fashion_mnist fashion_mnist = tf.keras.datasets.fashion_mnist # The data has already been sorted into training and test sets for us (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data() . train_data[2] . array([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 118, 24, 0, 0, 0, 0, 0, 48, 88, 5, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 12, 100, 212, 205, 185, 179, 173, 186, 193, 221, 142, 85, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 85, 76, 199, 225, 248, 255, 238, 226, 157, 68, 80, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 91, 69, 91, 201, 218, 225, 209, 158, 61, 93, 72, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 79, 89, 61, 59, 87, 108, 75, 56, 76, 97, 73, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 75, 89, 80, 80, 67, 63, 73, 83, 80, 96, 72, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 77, 88, 77, 80, 83, 83, 83, 83, 81, 95, 76, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 89, 96, 80, 83, 81, 84, 85, 85, 85, 97, 84, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 93, 97, 81, 85, 84, 85, 87, 88, 84, 99, 87, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 95, 87, 84, 87, 88, 85, 87, 87, 84, 92, 87, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 97, 87, 87, 85, 88, 87, 87, 87, 88, 85, 107, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 17, 100, 88, 87, 87, 88, 87, 87, 85, 89, 77, 118, 8, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 10, 93, 87, 87, 87, 87, 87, 88, 87, 89, 80, 103, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 9, 96, 87, 87, 87, 87, 87, 88, 87, 88, 87, 103, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 12, 96, 85, 87, 87, 87, 85, 87, 87, 88, 89, 100, 2, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 20, 95, 84, 88, 85, 87, 88, 88, 88, 89, 88, 99, 8, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 21, 96, 85, 87, 85, 88, 88, 88, 88, 89, 89, 99, 10, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 24, 96, 85, 87, 85, 87, 88, 88, 89, 88, 91, 102, 14, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 25, 93, 84, 88, 87, 87, 87, 87, 87, 89, 91, 103, 29, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 30, 95, 85, 88, 88, 87, 87, 87, 87, 89, 88, 102, 37, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 34, 96, 88, 87, 87, 87, 87, 87, 87, 85, 85, 97, 38, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 40, 96, 87, 85, 87, 87, 87, 87, 87, 85, 84, 92, 49, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 46, 95, 83, 84, 87, 87, 87, 87, 87, 87, 84, 87, 84, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 72, 95, 85, 84, 85, 88, 87, 87, 89, 87, 85, 83, 63, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 64, 100, 84, 87, 88, 85, 88, 88, 84, 87, 83, 95, 53, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 10, 102, 100, 91, 91, 89, 85, 84, 84, 87, 108, 106, 14, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 73, 93, 104, 107, 103, 103, 106, 102, 75, 10, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 18, 42, 57, 56, 32, 8, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8) . . train_labels[0] . 9 . # Show the first training example print(f&quot;Training sample: n {train_data[0]} n&quot;) print(f&quot;Training label: n {train_labels[0]} n&quot;) . Training sample: [[ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 13 73 0 0 1 4 0 0 0 0 1 1 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 3 0 36 136 127 62 54 0 0 0 1 3 4 0 0 3] [ 0 0 0 0 0 0 0 0 0 0 0 0 6 0 102 204 176 134 144 123 23 0 0 0 0 12 10 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 155 236 207 178 107 156 161 109 64 23 77 130 72 15] [ 0 0 0 0 0 0 0 0 0 0 0 1 0 69 207 223 218 216 216 163 127 121 122 146 141 88 172 66] [ 0 0 0 0 0 0 0 0 0 1 1 1 0 200 232 232 233 229 223 223 215 213 164 127 123 196 229 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 183 225 216 223 228 235 227 224 222 224 221 223 245 173 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 193 228 218 213 198 180 212 210 211 213 223 220 243 202 0] [ 0 0 0 0 0 0 0 0 0 1 3 0 12 219 220 212 218 192 169 227 208 218 224 212 226 197 209 52] [ 0 0 0 0 0 0 0 0 0 0 6 0 99 244 222 220 218 203 198 221 215 213 222 220 245 119 167 56] [ 0 0 0 0 0 0 0 0 0 4 0 0 55 236 228 230 228 240 232 213 218 223 234 217 217 209 92 0] [ 0 0 1 4 6 7 2 0 0 0 0 0 237 226 217 223 222 219 222 221 216 223 229 215 218 255 77 0] [ 0 3 0 0 0 0 0 0 0 62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159 0] [ 0 0 0 0 18 44 82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215 0] [ 0 57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246 0] [ 3 202 228 224 221 211 211 214 205 205 205 220 240 80 150 255 229 221 188 154 191 210 204 209 222 228 225 0] [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241 65 73 106 117 168 219 221 215 217 223 223 224 229 29] [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230 67] [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115] [ 0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210 92] [ 0 0 74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170 0] [ 2 0 0 0 66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168 99 58 0 0] [ 0 0 0 0 0 0 0 40 61 44 72 41 35 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] Training label: 9 . . train_data[0].shape, train_labels[0].shape . ((28, 28), ()) . import matplotlib.pyplot as plt plt.imshow(train_data[7]); . train_labels[7] . 2 . Indexing of labels . Label Description . 0 | T-shirt/top | . 1 | Trouser | . 2 | Pullover | . 3 | Dress | . 4 | Coat | . 5 | Sandal | . 6 | Shirt | . 7 | Sneaker | . 8 | Bag | . 9 | Ankle boot | . class_names = [&quot;T-shirt/top&quot;, &quot;Trouser&quot;, &quot;Pullover&quot;, &quot;Dress&quot;, &quot;Coat&quot;, &quot;Sandal&quot;, &quot;Shirt&quot;, &quot;Sneaker&quot;, &quot;Bag&quot;, &quot;Ankle boot&quot;] len(class_names) . 10 . index_of_choice = 100 plt.imshow(train_data[17], cmap = plt.cm.binary) plt.title(class_names[train_labels[index_of_choice]]) . Text(0.5, 1.0, &#39;Bag&#39;) . import random plt.figure(figsize = (7,7)) for i in range(4): ax = plt.subplot(2,2,i+1) rand_index = random.choice(range(len(train_data))) plt.imshow(train_data[rand_index], cmap = plt.cm.binary) plt.title(class_names[train_labels[rand_index]]) plt.axis(False) . Building a Multi-class classification model . For our multi-class classification model, we can use similar architecture to our binary classifier, however we have to tweak few things: . Input shape = 28 x 28 (the shape of one image) | Output shape = 10(one per class of clothing) | Loss function = tf.keras.losses.CategoricalCrossentropy() If your labels are one-hot encoded, use CategoricalCrossentropy() | If your labels are integer form use SparseCategoricalentropy() | . | Output layer activation = softmax not sigmoid | . flatten_model = tf.keras.Sequential([tf.keras.layers.Flatten(input_shape=(28,28))]) flatten_model.output_shape . (None, 784) . The dimension 784 is the total number of pixels in a 28 x 28 greyscale image . train_labels[0:10] . array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8) . our train labels are in the form of integers we need to transform into one-hot encoded vector. If we change the loss to sparse categorical entropy it does this task by itself. . tf.one_hot(train_labels[:10], depth=10), tf.one_hot(test_labels[:10], depth=10) . (&lt;tf.Tensor: shape=(10, 10), dtype=float32, numpy= array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(10, 10), dtype=float32, numpy= array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]], dtype=float32)&gt;) . # Set the random seed tf.random.set_seed(42) # Create the model model_11 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (28,28)), # To avoid shape mismatch errors we flatten the model tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(10, activation = tf.keras.activations.softmax) # Changed to softmax for mutli-class detection ]) # compile the model model_11.compile(loss = tf.keras.losses.CategoricalCrossentropy(), # changed the loss function optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit the model non_norm_history = model_11.fit(train_data, tf.one_hot(train_labels,depth=10), epochs =10, validation_data = (test_data, tf.one_hot(test_labels, depth=10))) . Epoch 1/10 1875/1875 [==============================] - 4s 2ms/step - loss: 2.1671 - accuracy: 0.1606 - val_loss: 1.7959 - val_accuracy: 0.2046 Epoch 2/10 1875/1875 [==============================] - 4s 2ms/step - loss: 1.7066 - accuracy: 0.2509 - val_loss: 1.6567 - val_accuracy: 0.2805 Epoch 3/10 1875/1875 [==============================] - 4s 2ms/step - loss: 1.6321 - accuracy: 0.2806 - val_loss: 1.6094 - val_accuracy: 0.2857 Epoch 4/10 1875/1875 [==============================] - 3s 2ms/step - loss: 1.6052 - accuracy: 0.2833 - val_loss: 1.6041 - val_accuracy: 0.2859 Epoch 5/10 1875/1875 [==============================] - 3s 2ms/step - loss: 1.5975 - accuracy: 0.2862 - val_loss: 1.6064 - val_accuracy: 0.2756 Epoch 6/10 1875/1875 [==============================] - 4s 2ms/step - loss: 1.5950 - accuracy: 0.2920 - val_loss: 1.5747 - val_accuracy: 0.2994 Epoch 7/10 1875/1875 [==============================] - 4s 2ms/step - loss: 1.5775 - accuracy: 0.3040 - val_loss: 1.6030 - val_accuracy: 0.3000 Epoch 8/10 1875/1875 [==============================] - 4s 2ms/step - loss: 1.5708 - accuracy: 0.3175 - val_loss: 1.5635 - val_accuracy: 0.3315 Epoch 9/10 1875/1875 [==============================] - 3s 2ms/step - loss: 1.5638 - accuracy: 0.3280 - val_loss: 1.5534 - val_accuracy: 0.3334 Epoch 10/10 1875/1875 [==============================] - 3s 2ms/step - loss: 1.5432 - accuracy: 0.3346 - val_loss: 1.5390 - val_accuracy: 0.3549 . . model_11.summary() . Model: &#34;sequential_22&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_10 (Flatten) (None, 784) 0 dense_56 (Dense) (None, 4) 3140 dense_57 (Dense) (None, 4) 20 dense_58 (Dense) (None, 10) 50 ================================================================= Total params: 3,210 Trainable params: 3,210 Non-trainable params: 0 _________________________________________________________________ . train_data.min(), test_data.max() . (0, 255) . Neural Networks prefer data to be scaled or normalized this means, they like to have the numbers in the tensors between 0 &amp; 1 . train_data_norm = train_data/ 255.0 test_data_norm = test_data/ 255.0 # Check the min and max values of the scaled training data train_data_norm.min(), train_data_norm.max() . (0.0, 1.0) . tf.random.set_seed(42) # Create a model (same as model_11) model_12 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape= (28,28)), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(10, activation = &quot;softmax&quot;) ]) # Compile the model model_12.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit the Model norm_history = model_12.fit(train_data_norm, train_labels, epochs = 10, validation_data = (test_data_norm,test_labels)) . Epoch 1/10 1875/1875 [==============================] - 4s 2ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617 Epoch 2/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6376 - accuracy: 0.7757 - val_loss: 0.6400 - val_accuracy: 0.7820 Epoch 3/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5942 - accuracy: 0.7914 - val_loss: 0.6247 - val_accuracy: 0.7783 Epoch 4/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5750 - accuracy: 0.7979 - val_loss: 0.6078 - val_accuracy: 0.7881 Epoch 5/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5641 - accuracy: 0.8006 - val_loss: 0.6169 - val_accuracy: 0.7881 Epoch 6/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5544 - accuracy: 0.8043 - val_loss: 0.5855 - val_accuracy: 0.7951 Epoch 7/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5488 - accuracy: 0.8063 - val_loss: 0.6097 - val_accuracy: 0.7836 Epoch 8/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5428 - accuracy: 0.8077 - val_loss: 0.5787 - val_accuracy: 0.7971 Epoch 9/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5373 - accuracy: 0.8097 - val_loss: 0.5698 - val_accuracy: 0.7977 Epoch 10/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5360 - accuracy: 0.8124 - val_loss: 0.5658 - val_accuracy: 0.8014 . Note: Neural Networks tend to prefer data in numerical form as well as sclaed/normalized form. . import pandas as pd # Plot non-normalized data loss curves pd.DataFrame(non_norm_history.history).plot(title= &quot;Non-normalized data&quot;) # Plot normalized data loss curves pd.DataFrame(norm_history.history).plot(title=&quot;Normalized data&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f847ec5c750&gt; . Note: Same model with even slightly different data can produce dramatically different results. So, when you are comparing models, it&#39;s important to make sure you are comparing them on the same criteria(eg. same architecture but different data or same data but different architecture) . Brief description of steps in modelling: . Turn all the data into numbers (neural networks can&#39;t handle strings) | Make sure all of your tensors are the right shape | Scale features (normalize or standardize, neural networks tend to prefer normalization) | . Finding the ideal learning rate . # Set random seed tf.random.set_seed(42) # Create model model_13 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (28,28)), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(10 ,activation = &quot;softmax&quot;) ]) # Compile the model model_13.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Create a learning rate call back lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch/20)) # Fit the model find_lr_history = model_13.fit(train_data_norm, train_labels, epochs= 40, validation_data = (test_data_norm, test_labels), callbacks = [lr_scheduler]) . Epoch 1/40 1875/1875 [==============================] - 8s 4ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617 - lr: 0.0010 Epoch 2/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.6366 - accuracy: 0.7759 - val_loss: 0.6400 - val_accuracy: 0.7808 - lr: 0.0011 Epoch 3/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5934 - accuracy: 0.7911 - val_loss: 0.6278 - val_accuracy: 0.7770 - lr: 0.0013 Epoch 4/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5749 - accuracy: 0.7969 - val_loss: 0.6122 - val_accuracy: 0.7871 - lr: 0.0014 Epoch 5/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5655 - accuracy: 0.7987 - val_loss: 0.6061 - val_accuracy: 0.7913 - lr: 0.0016 Epoch 6/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5569 - accuracy: 0.8022 - val_loss: 0.5917 - val_accuracy: 0.7940 - lr: 0.0018 Epoch 7/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5542 - accuracy: 0.8036 - val_loss: 0.5898 - val_accuracy: 0.7896 - lr: 0.0020 Epoch 8/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5509 - accuracy: 0.8039 - val_loss: 0.5829 - val_accuracy: 0.7949 - lr: 0.0022 Epoch 9/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5468 - accuracy: 0.8047 - val_loss: 0.6036 - val_accuracy: 0.7833 - lr: 0.0025 Epoch 10/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5478 - accuracy: 0.8058 - val_loss: 0.5736 - val_accuracy: 0.7974 - lr: 0.0028 Epoch 11/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5446 - accuracy: 0.8059 - val_loss: 0.5672 - val_accuracy: 0.8016 - lr: 0.0032 Epoch 12/40 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5432 - accuracy: 0.8067 - val_loss: 0.5773 - val_accuracy: 0.7950 - lr: 0.0035 Epoch 13/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5425 - accuracy: 0.8056 - val_loss: 0.5775 - val_accuracy: 0.7992 - lr: 0.0040 Epoch 14/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5407 - accuracy: 0.8078 - val_loss: 0.5616 - val_accuracy: 0.8075 - lr: 0.0045 Epoch 15/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5408 - accuracy: 0.8052 - val_loss: 0.5773 - val_accuracy: 0.8039 - lr: 0.0050 Epoch 16/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5437 - accuracy: 0.8058 - val_loss: 0.5682 - val_accuracy: 0.8015 - lr: 0.0056 Epoch 17/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5419 - accuracy: 0.8075 - val_loss: 0.5995 - val_accuracy: 0.7964 - lr: 0.0063 Epoch 18/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5488 - accuracy: 0.8058 - val_loss: 0.5544 - val_accuracy: 0.8087 - lr: 0.0071 Epoch 19/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5506 - accuracy: 0.8042 - val_loss: 0.6068 - val_accuracy: 0.7864 - lr: 0.0079 Epoch 20/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5537 - accuracy: 0.8030 - val_loss: 0.5597 - val_accuracy: 0.8076 - lr: 0.0089 Epoch 21/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5572 - accuracy: 0.8036 - val_loss: 0.5998 - val_accuracy: 0.7934 - lr: 0.0100 Epoch 22/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5615 - accuracy: 0.8013 - val_loss: 0.5756 - val_accuracy: 0.8034 - lr: 0.0112 Epoch 23/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5655 - accuracy: 0.8017 - val_loss: 0.6386 - val_accuracy: 0.7668 - lr: 0.0126 Epoch 24/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5819 - accuracy: 0.7963 - val_loss: 0.6356 - val_accuracy: 0.7869 - lr: 0.0141 Epoch 25/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5810 - accuracy: 0.7977 - val_loss: 0.6481 - val_accuracy: 0.7865 - lr: 0.0158 Epoch 26/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5960 - accuracy: 0.7901 - val_loss: 0.6997 - val_accuracy: 0.7802 - lr: 0.0178 Epoch 27/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6101 - accuracy: 0.7870 - val_loss: 0.6124 - val_accuracy: 0.7917 - lr: 0.0200 Epoch 28/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6178 - accuracy: 0.7846 - val_loss: 0.6137 - val_accuracy: 0.7962 - lr: 0.0224 Epoch 29/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6357 - accuracy: 0.7771 - val_loss: 0.6655 - val_accuracy: 0.7621 - lr: 0.0251 Epoch 30/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6671 - accuracy: 0.7678 - val_loss: 0.7597 - val_accuracy: 0.7194 - lr: 0.0282 Epoch 31/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6836 - accuracy: 0.7585 - val_loss: 0.6958 - val_accuracy: 0.7342 - lr: 0.0316 Epoch 32/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.7062 - accuracy: 0.7553 - val_loss: 0.7015 - val_accuracy: 0.7732 - lr: 0.0355 Epoch 33/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.7383 - accuracy: 0.7500 - val_loss: 0.7146 - val_accuracy: 0.7706 - lr: 0.0398 Epoch 34/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.8033 - accuracy: 0.7300 - val_loss: 0.8987 - val_accuracy: 0.6848 - lr: 0.0447 Epoch 35/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.8429 - accuracy: 0.7110 - val_loss: 0.8750 - val_accuracy: 0.7053 - lr: 0.0501 Epoch 36/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.8651 - accuracy: 0.7033 - val_loss: 0.8176 - val_accuracy: 0.6989 - lr: 0.0562 Epoch 37/40 1875/1875 [==============================] - 4s 2ms/step - loss: 0.9203 - accuracy: 0.6837 - val_loss: 0.7876 - val_accuracy: 0.7333 - lr: 0.0631 Epoch 38/40 1875/1875 [==============================] - 4s 2ms/step - loss: 1.2374 - accuracy: 0.5191 - val_loss: 1.3699 - val_accuracy: 0.4902 - lr: 0.0708 Epoch 39/40 1875/1875 [==============================] - 4s 2ms/step - loss: 1.1828 - accuracy: 0.5311 - val_loss: 1.1010 - val_accuracy: 0.5819 - lr: 0.0794 Epoch 40/40 1875/1875 [==============================] - 4s 2ms/step - loss: 1.6640 - accuracy: 0.3303 - val_loss: 1.8528 - val_accuracy: 0.2779 - lr: 0.0891 . . import numpy as np import matplotlib.pyplot as plt lrs = 1e-3 * (10**(tf.range(40)/20)) plt.semilogx(lrs, find_lr_history.history[&quot;loss&quot;]) plt.xlabel(&quot;Learning rate&quot;) plt.ylabel(&quot;Loss&quot;) plt.title(&quot;Finding the ideal learning rate&quot;) . Text(0.5, 1.0, &#39;Finding the ideal learning rate&#39;) . How do we find ideal learing rate? . Just take a look at the graph and see at what value the above graph the curve is lowest and go backwards a little. | . SO, it looks like $10^{-3}$ seems to be good . # Let&#39;s refit a model with the ideal learning rate # Set the random seed tf.random.set_seed(42) # Create the model model_14 = tf.keras.Sequential([ tf.keras.layers.Flatten(input_shape = (28,28)), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(4, activation = &quot;relu&quot;), tf.keras.layers.Dense(10, activation = &quot;softmax&quot;) ]) # Compile the model model_14.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(), optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;accuracy&quot;]) # Fit the model history_14 = model_14.fit(train_data_norm, train_labels, epochs = 20, validation_data = (test_data_norm,test_labels)) . Epoch 1/20 1875/1875 [==============================] - 4s 2ms/step - loss: 1.0348 - accuracy: 0.6474 - val_loss: 0.6937 - val_accuracy: 0.7617 Epoch 2/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.6376 - accuracy: 0.7757 - val_loss: 0.6400 - val_accuracy: 0.7820 Epoch 3/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5942 - accuracy: 0.7914 - val_loss: 0.6247 - val_accuracy: 0.7783 Epoch 4/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5750 - accuracy: 0.7979 - val_loss: 0.6078 - val_accuracy: 0.7881 Epoch 5/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5641 - accuracy: 0.8006 - val_loss: 0.6169 - val_accuracy: 0.7881 Epoch 6/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5544 - accuracy: 0.8043 - val_loss: 0.5855 - val_accuracy: 0.7951 Epoch 7/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5488 - accuracy: 0.8063 - val_loss: 0.6097 - val_accuracy: 0.7836 Epoch 8/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5428 - accuracy: 0.8077 - val_loss: 0.5787 - val_accuracy: 0.7971 Epoch 9/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5373 - accuracy: 0.8097 - val_loss: 0.5698 - val_accuracy: 0.7977 Epoch 10/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5360 - accuracy: 0.8124 - val_loss: 0.5658 - val_accuracy: 0.8014 Epoch 11/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5311 - accuracy: 0.8130 - val_loss: 0.5714 - val_accuracy: 0.8002 Epoch 12/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5284 - accuracy: 0.8132 - val_loss: 0.5626 - val_accuracy: 0.8027 Epoch 13/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5271 - accuracy: 0.8138 - val_loss: 0.5619 - val_accuracy: 0.8041 Epoch 14/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5249 - accuracy: 0.8143 - val_loss: 0.5718 - val_accuracy: 0.7991 Epoch 15/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5231 - accuracy: 0.8148 - val_loss: 0.5706 - val_accuracy: 0.8024 Epoch 16/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5203 - accuracy: 0.8162 - val_loss: 0.5731 - val_accuracy: 0.8023 Epoch 17/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5191 - accuracy: 0.8176 - val_loss: 0.5594 - val_accuracy: 0.8030 Epoch 18/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5176 - accuracy: 0.8157 - val_loss: 0.5582 - val_accuracy: 0.8053 Epoch 19/20 1875/1875 [==============================] - 3s 2ms/step - loss: 0.5156 - accuracy: 0.8169 - val_loss: 0.5644 - val_accuracy: 0.8007 Epoch 20/20 1875/1875 [==============================] - 4s 2ms/step - loss: 0.5146 - accuracy: 0.8177 - val_loss: 0.5660 - val_accuracy: 0.8075 . . Evaluating our multi-class classification model . To evaluate our multi-class classification model we could: . Evaluate our multi-class classification metrics(such as confusion matrix) | Assess some of its predictions (through visualizations) | Improve its results (by training it for longer or changing the architecture) | Save and export it for use in application. Let&#39;s go through the first two steps for evaluation process | . import itertools from sklearn.metrics import confusion_matrix figsize = (10,10) def make_confusion_matrix(y_true, y_pred, classes=None, figsize= (15,15), text_size= 10 ): # Create the confusion matrix cm = confusion_matrix(y_true, tf.round(y_pred)) cm_norm = cm.astype(&quot;float&quot;)/ cm.sum(axis = 1)[:, np.newaxis] # Noramlize our confusion matrix n_classes = cm.shape[0] # Let&#39;s make it neat and clear fig, ax = plt.subplots(figsize = figsize) # Create a matrix plot cax = ax.matshow(cm, cmap = plt.cm.Blues) fig.colorbar(cax) # set labels to be classes if classes: labels = classes else: labels = np.arange(cm.shape[0]) # Let&#39;s label the axis ax.set(title = &quot;Confusion Matrix&quot;, xlabel = &quot;Predicted Label&quot;, ylabel = &quot;True Label&quot;, xticks = np.arange(n_classes), yticks = np.arange(n_classes), xticklabels = labels, yticklabels = labels) # Set x-axis labels to the bottom ax.xaxis.set_label_position(&quot;bottom&quot;) ax.xaxis.tick_bottom() # Adjust label size ax.yaxis.label.set_size(text_size) ax.xaxis.label.set_size(text_size) ax.title.set_size(text_size) # Set threshold for different colors threshold = (cm.max() + cm.min())/2. # Plot the text on each cell for i,j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, f&quot;{cm[i,j]} ({cm_norm[i,j]*100:.1f}% )&quot;, horizontalalignment = &quot;center&quot;, color = &quot;white&quot; if cm[i,j] &gt; threshold else &quot;black&quot;, size = 15) . class_names . [&#39;T-shirt/top&#39;, &#39;Trouser&#39;, &#39;Pullover&#39;, &#39;Dress&#39;, &#39;Coat&#39;, &#39;Sandal&#39;, &#39;Shirt&#39;, &#39;Sneaker&#39;, &#39;Bag&#39;, &#39;Ankle boot&#39;] . y_probs = model_14.predict(test_data_norm) # probs is short for prediction probability # View the first 5 predictions y_probs[:5] . array([[8.56299753e-11, 3.53615629e-13, 2.66337556e-05, 4.63562024e-08, 5.09498605e-05, 9.61192474e-02, 8.17780403e-08, 9.18688551e-02, 4.06052778e-03, 8.07873666e-01], [3.42785552e-06, 1.28992649e-16, 9.59891498e-01, 2.05162564e-07, 1.53292371e-02, 2.45320095e-13, 2.41428725e-02, 1.13834485e-28, 6.32718089e-04, 4.47896404e-08], [6.10630595e-05, 9.96576726e-01, 4.38669758e-08, 3.34058981e-03, 1.32494861e-05, 1.43831603e-21, 8.27906115e-06, 7.32374630e-18, 5.48116041e-08, 4.92251402e-14], [7.50314357e-05, 9.90536869e-01, 4.25285322e-07, 9.22318175e-03, 1.36231421e-04, 1.82760903e-18, 2.68082422e-05, 4.81248308e-14, 1.45215904e-06, 2.22114601e-11], [7.21899569e-02, 1.54957536e-06, 2.55668938e-01, 1.03631355e-02, 4.35413495e-02, 1.10693023e-13, 6.16930187e-01, 6.75438989e-23, 1.30491622e-03, 1.21404065e-09]], dtype=float32) . NOTE: Remember to make predictions on the same kind of data your model ws trained on(eg. if you trained your model on normalized data, you&#39;ll want to make predictions on normalized data) . y_probs[0], tf.argmax(y_probs[0]) , class_names[tf.argmax(y_probs[0])] . (array([8.5629975e-11, 3.5361563e-13, 2.6633756e-05, 4.6356202e-08, 5.0949860e-05, 9.6119247e-02, 8.1778040e-08, 9.1868855e-02, 4.0605278e-03, 8.0787367e-01], dtype=float32), &lt;tf.Tensor: shape=(), dtype=int64, numpy=9&gt;, &#39;Ankle boot&#39;) . y_preds = y_probs.argmax(axis = 1) # View the first 10 prediction labels y_preds[:10] . array([9, 2, 1, 1, 6, 1, 4, 6, 5, 7]) . from sklearn.metrics import confusion_matrix confusion_matrix(y_true = test_labels, y_pred = y_preds) . array([[696, 8, 25, 87, 9, 5, 160, 0, 10, 0], [ 2, 939, 2, 35, 9, 0, 13, 0, 0, 0], [ 19, 2, 656, 10, 188, 0, 110, 0, 15, 0], [ 39, 10, 10, 819, 55, 0, 47, 1, 19, 0], [ 0, 0, 95, 23, 800, 0, 73, 0, 7, 2], [ 0, 0, 1, 0, 0, 894, 0, 60, 7, 38], [106, 4, 158, 57, 159, 1, 499, 0, 16, 0], [ 0, 0, 0, 0, 0, 31, 0, 936, 0, 33], [ 4, 1, 38, 15, 8, 12, 9, 5, 906, 2], [ 0, 0, 1, 0, 2, 15, 0, 51, 1, 930]]) . make_confusion_matrix(y_true = test_labels, y_pred = y_preds, classes = class_names, figsize = (20,20), text_size = 10) . Note: Often when working with images and other forms of visual data, it&#39;s a good idea to visualize as much as possible to develop a further understanding of the data and the inputs and outputs of your models. . How about we create a function for: . Plot a random image | Make a prediction on said image | Label the plot with the truth label &amp; the predited label. | . import random def plot_random_image(model,images, true_labels, classes): &quot;&quot;&quot; Picks a random image , plots it and labels it with a prediction and truth value &quot;&quot;&quot; # Setup random integer i = random.randint(0,len(images)) # Create a prediction target_image = images[i] pred_probs = model.predict(target_image.reshape(1,28,28)) pred_label = classes[pred_probs.argmax()] true_label = classes[true_labels[i]] # Plot the image plt.imshow(target_image, cmap = plt.cm.binary) # Change the color of the titles depending on if the prediction is right or wrong if pred_label == true_label: color = &quot;green&quot; else: color = &quot;red&quot; # ADD xlabel infomation (predictions/true label) plt.xlabel(&quot;pred:{} {:2.0f}% (True: {})&quot;.format(pred_label, 100*tf.reduce_max(pred_probs), true_label, color = color)) # set color to green or red based on if prediction is correct or wrong. . plot_random_image(model= model_14, images = test_data_norm, # always make predictions on the same kind of data your model was trained on true_labels = test_labels, classes = class_names) . Yeah! we finally did it. We are getting the correct predictions for the images in the dataset. Run the cell to view predictions for random images in the dataset. . What patterns the model learning? . model_14.layers . [&lt;keras.layers.core.flatten.Flatten at 0x7f8480bf8590&gt;, &lt;keras.layers.core.dense.Dense at 0x7f8480bf8450&gt;, &lt;keras.layers.core.dense.Dense at 0x7f847eb5b950&gt;, &lt;keras.layers.core.dense.Dense at 0x7f8480bf6c50&gt;] . model_14.layers[1] . &lt;keras.layers.core.dense.Dense at 0x7f8480bf8450&gt; . weights, biases = model_14.layers[1].get_weights() #Shapes weights, weights.shape . (array([[ 0.7150263 , -0.06077093, -0.99763095, -1.0484313 ], [ 0.2773212 , -0.471554 , -0.52916455, 0.02329255], [ 0.7752433 , 0.5402759 , -1.128857 , -0.7426156 ], ..., [-0.3945342 , 0.47628632, -0.2264153 , 0.2550593 ], [-0.40515798, 0.61810046, 0.23928414, -0.50387603], [ 0.23884593, 0.11606961, -0.12131374, 0.04352392]], dtype=float32), (784, 4)) . In (784,4), 784 is the number of pixel values in the image from the dataset. and the number 4 is the number of nodes in that layer of the neural network . Reading resouce: kernel initializer and glorot_uniform in the tf.keras.layers.Dense . Now let&#39;s check out the bias vector . biases, biases.shape . (array([ 2.4485964e-02, -6.1471871e-04, -2.7230152e-01, 8.1124878e-01], dtype=float32), (4,)) . The difference between Weights matrix and bias vector is that the weight matrix has one value per data point. Whereas bias vector has one value per hidden unit of that layer. . Every neuron has a bias vector. Each of these is paired with a weights matrix. . The Bias vector get initialized as zeros(at least in the case of a TensorFlow Dense layer). . The Bias vector dictates how much the patterns within the corresponding weights matrix should influence the next layer. . from tensorflow.keras.utils import plot_model # See the inputs and outputs of each layer plot_model(model_14,show_shapes= True) . Bibliography: . TensorFlow Developer Certificate in 2022: Zero to Mastery | TensorFlow Fashion MNIST dataset | TensorFlow Documentation | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deeplearning/neuralnetworks/tensorflow/python/classification/2022/02/15/Neural-Networks-for-Classification-with-TensorFlow.html",
            "relUrl": "/deeplearning/neuralnetworks/tensorflow/python/classification/2022/02/15/Neural-Networks-for-Classification-with-TensorFlow.html",
            "date": " • Feb 15, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Neural Networks for Regression with TensorFlow",
            "content": "Neural Network Regression Model with TensorFlow . This notebook is continuation of the Blog post TensorFlow Fundamentals. The notebook is an account of my working for the Tensorflow tutorial by Daniel Bourke on Youtube. The Notebook will cover the following concepts: . Architecture of a neural network regression model. | Input shapes and output shapes of a regression model(features and labels). | Creating custom data to view and fit. | Steps in modelling Creating a model, compiling a model, fitting a model, evaluating a model. | . | Different evaluation methods. | Saving and loading models. | . Regression Problems: A regression problem is when the output variable is a real or continuous value, such as “salary” or “weight”. Many different models can be used, the simplest is the linear regression. It tries to fit data with the best hyper-plane which goes through the points. Examples: . How much will this house sell for? | How many people will buy this app? | How much will my health insurace be? | How much should I save each week for fuel? | . We can also use the regression model to try and predict where the bounding boxes should be in object detection problem. Object detection thus involves both regression and then classifying the image in the box(classification problem). . Regression Inputs and outputs . Architecture of a regression model: . Hyperparameters: Input Layer Shape : same as shape of number of features. | Hidden Layrer(s): Problem specific | Neurons per hidden layer : Problem specific. | Output layer shape: same as hape of desired prediction shape. | Hidden activation : Usually ReLU(rectified linear unit) sometimes sigmoid. | Output acitvation: None, ReLU, logistic/tanh. | Loss function : MSE(Mean squared error) or MAE(Mean absolute error) or combination of both. | Optimizer: SGD(Stochastic Gradient Descent), Adam optimizer. | . | . Source: Adapted from page 239 of Hands-On Machine learning with Scikit-Learn, Keras &amp; TensorFlow . Example of creating a sample regression model in TensorFlow: . # 1. Create a model(specific to your problem) model = tf.keras.Sequential([ tf.keras.Input(shape = (3,)), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(1, activation = None) ]) # 2. Compile the model model.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(lr = 0.0001), metrics = [&quot;mae&quot;]) # 3. Fit the model model.fit(X_train, Y_train, epochs = 100) . Introduction to Regression with Neural Networks in TensorFlow . import tensorflow as tf print(tf.__version__) . 2.7.0 . import numpy as np import matplotlib.pyplot as plt from matplotlib import style style.use(&#39;dark_background&#39;) # create features X = np.array([-7.0,-4.0,-1.0,2.0,5.0,8.0,11.0,14.0]) # Create labels y = np.array([3.0,6.0,9.0,12.0,15.0,18.0,21.0,24.0]) # Visualize it plt.scatter(X,y) . &lt;matplotlib.collections.PathCollection at 0x7f778c8e7f10&gt; . y == X + 10 . array([ True, True, True, True, True, True, True, True]) . Yayy.. we got the relation by just seeing the data. Since the data is small and the relation ship is just linear, it was easy to guess the relation. . Input and Output shapes . house_info = tf.constant([&quot;bedroom&quot;,&quot;bathroom&quot;, &quot;garage&quot;]) house_price = tf.constant([939700]) house_info, house_price . (&lt;tf.Tensor: shape=(3,), dtype=string, numpy=array([b&#39;bedroom&#39;, b&#39;bathroom&#39;, b&#39;garage&#39;], dtype=object)&gt;, &lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([939700], dtype=int32)&gt;) . X[0], y[0] . (-7.0, 3.0) . X[1], y[1] . (-4.0, 6.0) . input_shape = X[0].shape output_shape = y[0].shape input_shape, output_shape . ((), ()) . X[0].ndim . 0 . we are specifically looking at scalars here. Scalars have 0 dimension . X = tf.cast(tf.constant(X), dtype = tf.float32) y = tf.cast(tf.constant(y), dtype = tf.float32) X.shape, y.shape . (TensorShape([8]), TensorShape([8])) . input_shape = X[0].shape output_shape = y[0].shape input_shape, output_shape . (TensorShape([]), TensorShape([])) . plt.scatter(X,y) . &lt;matplotlib.collections.PathCollection at 0x7f778c89a290&gt; . Steps in modelling with Tensorflow . Creating a model - define the input and output layers, as well as the hidden layers of a deep learning model. . | Compiling a model - define the loss function(how wrong the prediction of our model is) and the optimizer (tells our model how to improve the partterns its learning) and evaluation metrics(what we can use to interpret the performance of our model). . | Fitting a model - letting the model try to find the patterns between X &amp; y (features and labels). . | X,y . (&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1., 2., 5., 8., 11., 14.], dtype=float32)&gt;, &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3., 6., 9., 12., 15., 18., 21., 24.], dtype=float32)&gt;) . X.shape . TensorShape([8]) . tf.random.set_seed(42) # Create a model using the Sequential API model = tf.keras.Sequential([ tf.keras.layers.Dense(1) ]) # Compile the model model.compile(loss=tf.keras.losses.mae, # mae is short for mean absolute error optimizer=tf.keras.optimizers.SGD(), # SGD is short for stochastic gradient descent metrics=[&quot;mae&quot;]) # Fit the model # model.fit(X, y, epochs=5) # this will break with TensorFlow 2.7.0+ model.fit(tf.expand_dims(X, axis=-1), y, epochs=5) . Epoch 1/5 1/1 [==============================] - 0s 295ms/step - loss: 11.5048 - mae: 11.5048 Epoch 2/5 1/1 [==============================] - 0s 5ms/step - loss: 11.3723 - mae: 11.3723 Epoch 3/5 1/1 [==============================] - 0s 5ms/step - loss: 11.2398 - mae: 11.2398 Epoch 4/5 1/1 [==============================] - 0s 5ms/step - loss: 11.1073 - mae: 11.1073 Epoch 5/5 1/1 [==============================] - 0s 5ms/step - loss: 10.9748 - mae: 10.9748 . &lt;keras.callbacks.History at 0x7f778c7c6bd0&gt; . X, y . (&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1., 2., 5., 8., 11., 14.], dtype=float32)&gt;, &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3., 6., 9., 12., 15., 18., 21., 24.], dtype=float32)&gt;) . y_pred = model.predict([17.0]) y_pred . array([[12.716021]], dtype=float32) . The output is very far off from the actual value. So, Our model is not working correctly. Let&#39;s go and improve our model in the next section. . Improving our Model . Let&#39;s take a look about the three steps when we created the above model. . We can improve the model by altering the steps we took to create a model. . Creating a model - here we might add more layers, increase the number of hidden units(all called neurons) within each of the hidden layers, change the activation function of each layer. . | Compiling a model - here we might change the optimization function or perhaps the learning rate of the optimization function. . | Fitting a model - here we might fit a model for more epochs (leave it for training longer) or on more data (give the model more examples to learn from) . | # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(1) ]) # 2. Compile the model model.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset model.fit(tf.expand_dims(X, axis=-1), y, epochs=100, verbose = 0) . &lt;keras.callbacks.History at 0x7f778c7e8850&gt; . X , y . (&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1., 2., 5., 8., 11., 14.], dtype=float32)&gt;, &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3., 6., 9., 12., 15., 18., 21., 24.], dtype=float32)&gt;) . model.predict([17.0]) . array([[29.739855]], dtype=float32) . We got so close the actual value is 27 we performed a better prediction than the last model we trained. But we need to improve much better. Let&#39;s see what more we change and how close can we get to our actual output . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(1) ]) # 2. Compile the model model.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(lr = 0.0001), # lr stands for learning rate metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset model.fit(tf.expand_dims(X, axis=-1), y, epochs=100, verbose = 0) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . &lt;keras.callbacks.History at 0x7f778bbb7a90&gt; . model.predict([17.0]) # we are going to predict for the same input value 17 . array([[14.394114]], dtype=float32) . Oh..god!! This result went really bad for us. . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &quot;relu&quot;), # only difference we made tf.keras.layers.Dense(1) ]) # 2. Compile the model model.compile(loss = &quot;mae&quot;, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset model.fit(tf.expand_dims(X, axis=-1), y, epochs=100, verbose = 0) # verbose will hide the output from epochs . &lt;keras.callbacks.History at 0x7f778babbbd0&gt; . X , y . (&lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([-7., -4., -1., 2., 5., 8., 11., 14.], dtype=float32)&gt;, &lt;tf.Tensor: shape=(8,), dtype=float32, numpy=array([ 3., 6., 9., 12., 15., 18., 21., 24.], dtype=float32)&gt;) . model.predict([17.0]) . array([[31.727652]], dtype=float32) . Oh, this should be 27 but this prediction is very far off from our previous prediction. It seems that our previous model did better than this. . Even though we find the values of our loss function are very low than that of our previous model. We still are far away from our label value. Why is that so?? The explanation is our model is overfitting the dataset. That means it is trying to map a function that just fits the already provided examples correctly but it cannot fit the new examples that we are giving. So, the mae and loss value if not the ultimate metric to check for improving the model. because we need to get less error for new examples that the model has not seen before. . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &quot;relu&quot;), # only difference we made tf.keras.layers.Dense(1) ]) # 2. Compile the model model.compile(loss = &quot;mae&quot;, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset model.fit(tf.expand_dims(X, axis=-1), y, epochs=100, verbose = 0)# verbose will hide the epochs output . &lt;keras.callbacks.History at 0x7f778b951790&gt; . model.predict([17.0]) . array([[25.852007]], dtype=float32) . Still not better!! . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;),# only difference we made tf.keras.layers.Dense(1) ]) # default value of lr is 0.001 # 2. Compile the model model.compile(loss = &quot;mae&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.01), # lr stands for learning rate metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset model.fit(tf.expand_dims(X, axis=-1), y, epochs=100, verbose = 0) # verbose will hide the epochs output . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. super(Adam, self).__init__(name, **kwargs) . &lt;keras.callbacks.History at 0x7f778cb41810&gt; . The learning rate is the most important hyperparameter for all the Neural Networks . Evaluating our model . In practice, a typical workflow you&#39;ll go through when building a neural network is: . Build a model -&gt; fit it -&gt; evaluate it -&gt; tweak a model -&gt; fit it -&gt; evaluate it -&gt; tweak it -&gt; fit it . Common ways to improve a deep model: . Adding Layers | Increase the number of hidden units | Change the activation functions | Change the optimization function | Change the learning rate | Fitting on more data | Train for longer (more epochs) | . Because we can alter each of these they are called hyperparameters . When it comes to evaluation.. there are 3 words you should memorize: . &quot;Visualize, Visualize, Visualize&quot; . It&#39;s a good idea to visualize: The data - what data are working with? What does it look like The model itself - What does our model look like? . The training of a model - how does a model perform while it learns? | The predictions of the model - how does the prediction of the model line up against the labels(original value) | . X_large = tf.range(-100,100,4) X_large . &lt;tf.Tensor: shape=(50,), dtype=int32, numpy= array([-100, -96, -92, -88, -84, -80, -76, -72, -68, -64, -60, -56, -52, -48, -44, -40, -36, -32, -28, -24, -20, -16, -12, -8, -4, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60, 64, 68, 72, 76, 80, 84, 88, 92, 96], dtype=int32)&gt; . y_large = X_large + 10 y_large . &lt;tf.Tensor: shape=(50,), dtype=int32, numpy= array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42, -38, -34, -30, -26, -22, -18, -14, -10, -6, -2, 2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66, 70, 74, 78, 82, 86, 90, 94, 98, 102, 106], dtype=int32)&gt; . import matplotlib.pyplot as plt plt.scatter(X_large,y_large) . &lt;matplotlib.collections.PathCollection at 0x7f778c8556d0&gt; . The 3 sets ... . Training set - The model learns from this data, which is typically 70-80% of the total data you have available. . | validation set - The model gets tuned on this data, which is typically 10-15% of the data avaialable. . | Test set - The model gets evaluated on this data to test what it has learned. This set is typically 10-15%. . | . len(X_large) . 50 . # since the dataset is small we can skip the valdation set X_train = X_large[:40] X_test = X_large[40:] y_train = y_large[:40] y_test = y_large[40:] len(X_train), len(X_test), len(y_train), len(y_test) . (40, 10, 40, 10) . Visualizing the data . Now we&#39;ve got our data in training and test sets. Let&#39;s visualize it. . plt.figure(figsize = (10,7)) # Plot the training data in blue plt.scatter(X_train, y_train, c= &#39;b&#39;, label = &quot;Training data&quot;) # Plot the test data in green plt.scatter(X_test, y_test, c = &quot;g&quot;, label = &quot;Training data&quot;) plt.legend(); . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(1) ]) # default value of lr is 0.001 # 2. Compile the model model.compile(loss = &quot;mae&quot;, optimizer = tf.keras.optimizers.SGD(), # lr stands for learning rate metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset #model.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100) . Let&#39;s visualize it before fitting the model . model.summary() . ValueError Traceback (most recent call last) &lt;ipython-input-254-5f15418b3570&gt; in &lt;module&gt;() -&gt; 1 model.summary() /usr/local/lib/python3.7/dist-packages/keras/engine/training.py in summary(self, line_length, positions, print_fn, expand_nested) 2578 if not self.built: 2579 raise ValueError( -&gt; 2580 &#39;This model has not yet been built. &#39; 2581 &#39;Build the model first by calling `build()` or by calling &#39; 2582 &#39;the model on a batch of data.&#39;) ValueError: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data. . model.summary() doesn&#39;t work without building the model or fitting the model . X[0], y[0] . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=-7.0&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.0&gt;) . tf.random.set_seed(42) # Create a model(same as above) model = tf.keras.Sequential([ tf.keras.layers.Dense(1, input_shape = [1]) # input_shape is 1 refer above code cell ]) # Compile the model model.compile(loss= &quot;mae&quot;, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;]) . model.summary() . Model: &#34;sequential_31&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_86 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . Total params - total number of parameters in the model. | Trainable parameters- these are the parameters (patterns) the model can update as it trains. | Non-Trainable parameters - these parameters aren&#39;t updated during training(this is typical when you have paramters from other models during transfer learning) | . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(10, input_shape = [1], name= &quot;input_layer&quot;), tf.keras.layers.Dense(1, name = &quot;output_layer&quot;) ], name = &quot;model_1&quot;) # 2. Compile the model model.compile(loss = &quot;mae&quot;, optimizer = tf.keras.optimizers.SGD(), # lr stands for learning rate metrics = [&quot;mae&quot;]) . model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (Dense) (None, 10) 20 output_layer (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . We have changed the layer names and added our custom model name. . from tensorflow.keras.utils import plot_model plot_model(model = model, to_file = &#39;model1.png&#39;, show_shapes = True) . # 1. Create the model model = tf.keras.Sequential([ tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;), tf.keras.layers.Dense(100, activation = &quot;relu&quot;),# only difference we made tf.keras.layers.Dense(1) ], name) # default value of lr is 0.001 # 2. Compile the model model.compile(loss = &quot;mae&quot;, optimizer = tf.keras.optimizers.Adam(lr = 0.01), # lr stands for learning rate metrics = [&quot;mae&quot;]) # 3. Fit the model to our dataset model.fit(tf.expand_dims(X_train, axis=-1), y_train, epochs=100, verbose = 0) . NameError Traceback (most recent call last) &lt;ipython-input-261-c283a288f289&gt; in &lt;module&gt;() 7 tf.keras.layers.Dense(100, activation = &#34;relu&#34;),# only difference we made 8 tf.keras.layers.Dense(1) -&gt; 9 ], name) 10 # default value of lr is 0.001 11 # 2. Compile the model NameError: name &#39;name&#39; is not defined . model.predict(X_test) . array([[-58.987164], [-62.91964 ], [-66.85213 ], [-70.7846 ], [-74.71708 ], [-78.64954 ], [-82.58203 ], [-86.51451 ], [-90.44698 ], [-94.37946 ]], dtype=float32) . wow, we are so close!!! . model.summary() . Model: &#34;model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (Dense) (None, 10) 20 output_layer (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . from tensorflow.keras.utils import plot_model plot_model(model = model, to_file = &#39;model.png&#39;, show_shapes = True) . Visualizing our model&#39;s predictions . To visualize predictions, it&#39;s a good idea to plot them against the ground truth labels. . Often you&#39;ll see this in the form of y_test or y_true versus y_pred . tf.random.set_seed(42) # Create a model (same as above) model = tf.keras.Sequential([ tf.keras.layers.Dense(10, input_shape = [1], name = &quot;input_layer&quot;), tf.keras.layers.Dense(1, name = &quot;output_layer&quot;) # define the input_shape to our model ], name = &quot;revised_model_1&quot;) # Compile model (same as above) model.compile(loss=tf.keras.losses.mae, optimizer=tf.keras.optimizers.SGD(), metrics=[&quot;mae&quot;]) . model.summary() . Model: &#34;revised_model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (Dense) (None, 10) 20 output_layer (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . model.fit(X_train, y_train, epochs=100, verbose=0) . &lt;keras.callbacks.History at 0x7f7788675350&gt; . model.summary() . Model: &#34;revised_model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_layer (Dense) (None, 10) 20 output_layer (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . y_pred = model.predict(X_test) tf.constant(y_pred) . &lt;tf.Tensor: shape=(10, 1), dtype=float32, numpy= array([[ 70.55218 ], [ 75.13991 ], [ 79.72763 ], [ 84.31535 ], [ 88.903076], [ 93.49081 ], [ 98.07853 ], [102.66625 ], [107.253975], [111.8417 ]], dtype=float32)&gt; . These are our predictions! . y_test . &lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 70, 74, 78, 82, 86, 90, 94, 98, 102, 106], dtype=int32)&gt; . These are the ground truth labels! . plot_model(model, show_shapes=True) . Note: IF you feel like you&#39;re going to reuse some kind of functionality in future, it&#39;s a good idea to define a function so that we can reuse it whenever we need. . def plot_predictions(train_data= X_train, train_labels = y_train, test_data = X_test, test_labels =y_test, predictions = y_pred): &quot;&quot;&quot; Plots training data, test data and compares predictions to ground truth labels &quot;&quot;&quot; plt.figure(figsize = (10,7)) # Plot training data in blue plt.scatter(train_data, train_labels, c= &quot;b&quot;, label = &quot;Training data&quot;) # Plot testing data in green plt.scatter(test_data, test_labels, c= &quot;g&quot;, label = &quot;Testing data&quot;) # Plot model&#39;s predictions in red plt.scatter(test_data, predictions, c= &quot;r&quot;, label = &quot;Predictions&quot;) # Show legends plt.legend(); . plot_predictions(train_data=X_train, train_labels=y_train, test_data=X_test, test_labels=y_test, predictions=y_pred) . We tuned our model very well this time. The predictions are really close to the actual values. . Evaluating our model&#39;s predictions with regression evaluation metrics . Depending on the problem you&#39;re working on, there will be different evaluation metrics to evaluate your model&#39;s performance. . Since, we&#39;re working on a regression, two of the main metrics: . MAE - mean absolute error, &quot;on average, how wrong id each of my model&#39;s predictions&quot; TensorFlow code: tf.keras.losses.MAE() | or tf.metrics.mean_absolute_error() $$ MAE = frac{Σ_{i=1}^{n} |y_i - x_i| }{n} $$ | . | MSE - mean square error, &quot;square of the average errors&quot; . tf.keras.losses.MSE() | tf.metrics.mean_square_error() $$ MSE = frac{1}{n} Σ_{i=1}^{n}(Y_i - hat{Y_i})^2$$ | . $ hat{Y_i}$ is the prediction our model makes. $Y_i$ is the label value. . | Huber - Combination of MSE and MAE, Less sensitive to outliers than MSE. . tf.keras.losses.Huber() | . | . model.evaluate(X_test, y_test) . 1/1 [==============================] - 0s 118ms/step - loss: 3.1969 - mae: 3.1969 . [3.196942090988159, 3.196942090988159] . mae = tf.metrics.mean_absolute_error(y_true = y_test, y_pred = tf.constant(y_pred)) mae . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([17.558258 , 14.1160555, 11.708948 , 10.336929 , 10. , 10.698161 , 12.447118 , 15.333002 , 19.253975 , 23.841698 ], dtype=float32)&gt; . We got the metric values wrong..why did this happen?? . tf.constant(y_pred) . &lt;tf.Tensor: shape=(10, 1), dtype=float32, numpy= array([[ 70.55218 ], [ 75.13991 ], [ 79.72763 ], [ 84.31535 ], [ 88.903076], [ 93.49081 ], [ 98.07853 ], [102.66625 ], [107.253975], [111.8417 ]], dtype=float32)&gt; . y_test . &lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 70, 74, 78, 82, 86, 90, 94, 98, 102, 106], dtype=int32)&gt; . Notice that the shape of y_pred is (10,1) and the shape of y_test is (10,) They might seem the same but they are not of the same shape. Let&#39;s reshape the tensor to make the shapes equal. . tf.squeeze(y_pred) . &lt;tf.Tensor: shape=(10,), dtype=float32, numpy= array([ 70.55218 , 75.13991 , 79.72763 , 84.31535 , 88.903076, 93.49081 , 98.07853 , 102.66625 , 107.253975, 111.8417 ], dtype=float32)&gt; . mae = tf.metrics.mean_absolute_error(y_true = y_test, y_pred = tf.squeeze(y_pred)) mae . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.1969407&gt; . Now,we got our metric value. The mean absolute error of our model is 3.1969407. Now, let&#39;s calculate the mean squared error and see how that goes. . mse = tf.metrics.mean_squared_error(y_true = y_test, y_pred = tf.squeeze(y_pred)) mse . &lt;tf.Tensor: shape=(), dtype=float32, numpy=13.070143&gt; . Our mean squared error is 13.070143. Remember, the mean squared error squares the error for every example in the test set and averages the values. So, generally, the mse is largeer than mae. When larger errors are more significant than smaller errors, then it is best to use mse. MAE can be used as a great starter metric for any regression problem. We can also try Huber and see how that goes. . huber_metric = tf.losses.huber(y_true = y_test, y_pred = tf.squeeze(y_pred)) huber_metric . &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.7069678&gt; . def mae(y_true, y_pred): return tf.metrics.mean_absolute_error(y_true = y_test, y_pred = tf.squeeze(y_pred)) def mse(y_true, y_pred): return tf.metrics.mean_squared_error(y_true = y_test, y_pred = tf.squeeze(y_pred)) def huber(y_true, y_pred): return tf.losses.huber(y_true = y_test, y_pred = tf.squeeze(y_pred)) . Running experiments to improve our model . Build a model -&gt; fit it -&gt; evaluate it -&gt; tweak a model -&gt; fit it -&gt; evaluate it -&gt; tweak it -&gt; fit it . Get more data - get more examples for your model to train on(more oppurtunities to learn patterns or relationships between features and labels). | Make your mode larger(using a more complex model) - this might come in the form of more layeres or more hidden unites in each layer. | Train for longer - give your model more of a chance to find patterns in the data. | Let&#39;s do a few modelling experiments: . model_1 - same as original model, 1 layer, trained for 100 epochs. | model_2 - 2 layers, trained for 100 epochs | model_3 - 2 layers, trained for 500 epochs. | You can design more experiments too to make the model more better . Build Model_1 . X_train, y_train . (&lt;tf.Tensor: shape=(40,), dtype=int32, numpy= array([-100, -96, -92, -88, -84, -80, -76, -72, -68, -64, -60, -56, -52, -48, -44, -40, -36, -32, -28, -24, -20, -16, -12, -8, -4, 0, 4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56], dtype=int32)&gt;, &lt;tf.Tensor: shape=(40,), dtype=int32, numpy= array([-90, -86, -82, -78, -74, -70, -66, -62, -58, -54, -50, -46, -42, -38, -34, -30, -26, -22, -18, -14, -10, -6, -2, 2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58, 62, 66], dtype=int32)&gt;) . tf.random.set_seed(42) # 1. Create the model model_1 = tf.keras.Sequential([ tf.keras.layers.Dense(1, input_shape = [1]) ], name = &quot;Model_1&quot;) # 2. Compile the model model_1.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;]) # 3. Fit the model model_1.fit(X_train, y_train ,epochs = 100, verbose = 0) . &lt;keras.callbacks.History at 0x7f778cd7f7d0&gt; . model_1.summary() . Model: &#34;Model_1&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_91 (Dense) (None, 1) 2 ================================================================= Total params: 2 Trainable params: 2 Non-trainable params: 0 _________________________________________________________________ . y_preds_1 = model_1.predict(X_test) plot_predictions(predictions = y_preds_1) . mae_1 = mae(y_test, y_preds_1) mse_1 = mse(y_test, y_preds_1) mae_1, mse_1 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=18.745327&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=353.57336&gt;) . Build Model_2 . 2 dense layers, trained for 100 epochs | . tf.random.set_seed(42) # 1. Create the model model_2 = tf.keras.Sequential([ tf.keras.layers.Dense(10, input_shape =[1]), tf.keras.layers.Dense(1) ], name = &quot;model_2&quot;) # 2. Compile the model model_2.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mse&quot;]) # Let&#39;s build this model with mse as eval metric. # 3. Fit the model model_2.fit(X_train, y_train ,epochs = 100, verbose = 0) . &lt;keras.callbacks.History at 0x7f778b7e57d0&gt; . model_2.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_92 (Dense) (None, 10) 20 dense_93 (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . y_preds_2 = model_2.predict(X_test) plot_predictions(predictions = y_preds_2) . Yeah,we improved this model very much than the previous one. If you want to compare with previous one..scroll up and see the plot_predictions of previous one and compare it with this one. . mae_2 = mae(y_test, y_preds_2) mse_2 = mse(y_test, y_preds_2) mae_2, mse_2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.1969407&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=13.070143&gt;) . Build Model_3 . 2 layers, trained for 500 epochs | . tf.random.set_seed(42) # 1. Create the model model_3 = tf.keras.Sequential([ tf.keras.layers.Dense(10, input_shape =[1]), tf.keras.layers.Dense(1) ], name = &quot;model_3&quot;) # 2. Compile the model model_3.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;]) # Let&#39;s build this model with mse as eval metric. # 3. Fit the model model_2.fit(X_train, y_train ,epochs = 500, verbose = 0) . &lt;keras.callbacks.History at 0x7f778c385950&gt; . y_preds_3 = model_3.predict(X_test) plot_predictions(predictions = y_preds_3) . This is even terrible performance than the first model. we have actually made the model worse. WHY?? . We, overfitted the model too much because we trained it for much longer than we are supposed to. . mae_3 = mae(y_test, y_preds_3) mse_3 = mse(y_test, y_preds_3) mae_3, mse_3 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=59.02484&gt;, &lt;tf.Tensor: shape=(), dtype=float32, numpy=3536.0774&gt;) . whoaa, the error is extremely high. I think the best of our models is model_2 . The Machine Learning practitioner&#39;s motto: . Experiment, experiment, experiment . Note: You want to start with small experiments(small models) and make sure they work and then increase their scale when neccessary. . Comparing the results of our experiments . We&#39;ve run a few experiments, let&#39;s compare the results now. . import pandas as pd model_results = [[&quot;model_1&quot;, mae_1.numpy(), mse_1.numpy()], [&quot;model_2&quot;, mae_2.numpy(), mse_2.numpy()], [&quot;model_3&quot;, mae_3.numpy(), mse_3.numpy()]] all_results = pd.DataFrame(model_results, columns =[&quot;model&quot;, &quot;mae&quot;, &quot;mse&quot;]) all_results . model mae mse . 0 model_1 | 18.745327 | 353.573364 | . 1 model_2 | 3.196941 | 13.070143 | . 2 model_3 | 59.024841 | 3536.077393 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; It looks like model_2 performed done the best. Let&#39;s look at what is model_2 . model_2.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_92 (Dense) (None, 10) 20 dense_93 (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . This is the model that has done the best on our dataset. . Note: One of your main goals should be to minimize the time between your experiments. The more experiments you do, the more things you will figure out which don&#39;t work and in turn, get closer to figuring out what does work. Remeber, the machine learning pracitioner&#39;s motto : &quot;experiment, experiment, experiment&quot;. . Tracking your experiments: . One really good habit of machine learning modelling is to track the results of your experiments. . And when doing so, it can be tedious if you are running lots of experiments. . Luckily, there are tools to help us! . Resources: As you build more models, you&#39;ll want to look into using: . TensorBoard - a component of TensorFlow library to help track modelling experiments. It is integrated into the TensorFlow library. . | Weights &amp; Biases - A tool for tracking all kinds of machine learning experiments (it plugs straight into tensorboard). . | . Saving our models . Saving our models allows us to use them outside of Google Colab(or wherever they were trained) such as in a web application or a mobile app. . There are two main formats we can save our model: . The SavedModel format | The HDF5 format | model.save() allows us to save the model and we can use it again to do add things to the model after reloading it. . model_2.save(&quot;best_model_SavedModel_format&quot;) . INFO:tensorflow:Assets written to: best_model_SavedModel_format/assets . If we are planning to use this model inside the tensorflow framework. we will be better off using the SavedModel format. But if we are planning to export the model else where and use it outside the tensorflow framework use the HDF5 format. . model_2.save(&quot;best_model_HDF5_format.h5&quot;) . Saving a model with SavedModel format will give us a folder with some files regarding our model. Saving a model with HDF5 format will give us just one file with our model. . Loading in a saved model . loaded_SavedModel_format = tf.keras.models.load_model(&quot;/content/best_model_SavedModel_format&quot;) loaded_SavedModel_format.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_92 (Dense) (None, 10) 20 dense_93 (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . model_2.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_92 (Dense) (None, 10) 20 dense_93 (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . model_2_preds = model_2.predict(X_test) loaded_SavedModel_format_preds = loaded_SavedModel_format.predict(X_test) model_2_preds == loaded_SavedModel_format_preds . array([[ True], [ True], [ True], [ True], [ True], [ True], [ True], [ True], [ True], [ True]]) . mae(y_true = y_test, y_pred = model_2_preds) == mae(y_true = y_test, y_pred = loaded_SavedModel_format_preds) . &lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt; . loaded_h5_model = tf.keras.models.load_model(&quot;/content/best_model_HDF5_format.h5&quot;) loaded_h5_model.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_92 (Dense) (None, 10) 20 dense_93 (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . model_2.summary() . Model: &#34;model_2&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_92 (Dense) (None, 10) 20 dense_93 (Dense) (None, 1) 11 ================================================================= Total params: 31 Trainable params: 31 Non-trainable params: 0 _________________________________________________________________ . Yeah the loading of .hf format model matched with our original mode_2 format. So, our model loading worked correctly. . model_2_preds = model_2.predict(X_test) loaded_h5_model_preds = loaded_h5_model.predict(X_test) model_2_preds == loaded_h5_model_preds . array([[ True], [ True], [ True], [ True], [ True], [ True], [ True], [ True], [ True], [ True]]) . Download a model(or any other file) from google colab . If you want to download your files from Google Colab: . you can go to the files tab and right click on the file you&#39;re after and click download. . | Use code(see the cell below). . | You can save it to google drive by connecting to google drive and copying it there. . | from google.colab import files files.download(&quot;/content/best_model_HDF5_format.h5&quot;) . !cp /content/best_model_HDF5_format.h5 /content/drive/MyDrive/tensor-flow-deep-learning . !ls /content/drive/MyDrive/tensor-flow-deep-learning . best_model_HDF5_format.h5 . We have saved our model to our google drive !!! . A larger example . We take a larger dataset to do create a regression model. The model we do is insurance forecast by using linear regression available from kaggle Medical Cost Personal Datasets . import tensorflow as tf import pandas as pd import matplotlib.pyplot as plt . insurance = pd.read_csv(&quot;https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv&quot;) insurance . age sex bmi children smoker region charges . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | . ... ... | ... | ... | ... | ... | ... | ... | . 1333 50 | male | 30.970 | 3 | no | northwest | 10600.54830 | . 1334 18 | female | 31.920 | 0 | no | northeast | 2205.98080 | . 1335 18 | female | 36.850 | 0 | no | southeast | 1629.83350 | . 1336 21 | female | 25.800 | 0 | no | southwest | 2007.94500 | . 1337 61 | female | 29.070 | 0 | yes | northwest | 29141.36030 | . 1338 rows × 7 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; This is a quite bigger dataset than the one we have previously worked with. . insurance_one_hot = pd.get_dummies(insurance) insurance_one_hot.head() . age bmi children charges sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest . 0 19 | 27.900 | 0 | 16884.92400 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 1 18 | 33.770 | 1 | 1725.55230 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | . 2 28 | 33.000 | 3 | 4449.46200 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | . 3 33 | 22.705 | 0 | 21984.47061 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . 4 32 | 28.880 | 0 | 3866.85520 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; X = insurance_one_hot.drop(&quot;charges&quot;, axis =1) y = insurance_one_hot[&quot;charges&quot;] . X.head() . age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest . 0 19 | 27.900 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 1 18 | 33.770 | 1 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | . 2 28 | 33.000 | 3 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 0 | . 3 33 | 22.705 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . 4 32 | 28.880 | 0 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; y.head() . 0 16884.92400 1 1725.55230 2 4449.46200 3 21984.47061 4 3866.85520 Name: charges, dtype: float64 . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.2, random_state = 42) len(X), len(X_train), len(X_test) . (1338, 1070, 268) . X_train . age bmi children sex_female sex_male smoker_no smoker_yes region_northeast region_northwest region_southeast region_southwest . 560 46 | 19.950 | 2 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 1285 47 | 24.320 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1142 52 | 24.860 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 969 39 | 34.320 | 5 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 486 54 | 21.470 | 3 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1095 18 | 31.350 | 4 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | . 1130 39 | 23.870 | 5 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | . 1294 58 | 25.175 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | . 860 37 | 47.600 | 2 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 1126 55 | 29.900 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | . 1070 rows × 11 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; insurance[&quot;smoker&quot;] , insurance[&quot;sex&quot;] . (0 yes 1 no 2 no 3 no 4 no ... 1333 no 1334 no 1335 no 1336 no 1337 yes Name: smoker, Length: 1338, dtype: object, 0 female 1 male 2 male 3 male 4 male ... 1333 male 1334 female 1335 female 1336 female 1337 female Name: sex, Length: 1338, dtype: object) . tf.random.set_seed(42) # 1. Create a model insurance_model = tf.keras.Sequential([ tf.keras.layers.Dense(10), tf.keras.layers.Dense(1) ]) # 2. Compile the model insurance_model.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.SGD(), metrics = [&quot;mae&quot;]) #3. Fit the model insurance_model.fit(X_train, y_train,epochs = 100, verbose = 0) . &lt;keras.callbacks.History at 0x7f778c0c3890&gt; . insurance_model.evaluate(X_test,y_test) . 9/9 [==============================] - 0s 2ms/step - loss: 7023.3291 - mae: 7023.3291 . [7023.3291015625, 7023.3291015625] . y_train.median(), y_train.mean() . (9575.4421, 13346.089736364489) . Right now it looks like our model is not performing well, lets try and improve it. . To try and improve our model, we&#39;ll run 2 experiments: . Add an extra layer with more hidden units and use the Adam optimizer | Train for longer (like 200 epochs) | We can also do our custom experiments to improve it. | tf.random.set_seed(42) # 1. Create the model insurance_model_2 = tf.keras.Sequential([ tf.keras.layers.Dense(100), tf.keras.layers.Dense(10), tf.keras.layers.Dense(1) ],name = &quot;insurace_model_2&quot;) # 2. Compile the model insurance_model_2.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;mae&quot;]) # 3. Fit the model insurance_model_2.fit(X_train, y_train, epochs = 100, verbose = 0) . &lt;keras.callbacks.History at 0x7f778b6dc350&gt; . insurance_model_2.evaluate(X_test, y_test) . 9/9 [==============================] - 0s 2ms/step - loss: 4924.3477 - mae: 4924.3477 . [4924.34765625, 4924.34765625] . tf.random.set_seed(42) # 1. Create the model insurance_model_3 = tf.keras.Sequential([ tf.keras.layers.Dense(100), tf.keras.layers.Dense(10), tf.keras.layers.Dense(1) ],name = &quot;insurace_model_2&quot;) # 2. Compile the model insurance_model_3.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;mae&quot;]) # 3. Fit the model history = insurance_model_3.fit(X_train, y_train, epochs = 200, verbose = 0) . insurance_model_3.evaluate(X_test, y_test) . 9/9 [==============================] - 0s 2ms/step - loss: 3491.2961 - mae: 3491.2961 . [3491.296142578125, 3491.296142578125] . pd.DataFrame(history.history).plot() plt.ylabel(&quot;loss&quot;) plt.xlabel(&quot;epochs&quot;) plt.title(&quot;Training curve of our model&quot;) . Text(0.5, 1.0, &#39;Training curve of our model&#39;) . Question: How long should you train for? . It depends, It really depends on problem you are working on. However, many people have asked this question before, so TensorFlow has a solution!, It is called EarlyStopping callback, which is a TensorFlow component you can add to your model to stop training once it stops improving a certain metric. . Preprocessing data (normalization and standardization) . Short review of our modelling steps in TensorFlow: . Get data ready(turn into tensors) | Build or pick a pretrained model (to suit your problem) | Fit the model to the data and make a prediction. | Evaluate the model. | Imporve through experimentation. | Save and reload your trained models. | we are going to focus on the step 1 to make our data set more rich for training. some steps involved in getting data ready: . Turn all data into numbers(neural networks can&#39;t handle strings). | Make sure all of your tensors are the right shape. | Scale features(normalize or standardize, neural networks tend to prefer normalization) -- this is the one thing we haven&#39;t done while preparing our data. | If you are not sure on which to use for scaling, you could try both and see which perform better . import tensorflow as tf import pandas as pd import matplotlib.pyplot as plt . insurance = pd.read_csv(&quot;https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv&quot;) insurance . age sex bmi children smoker region charges . 0 19 | female | 27.900 | 0 | yes | southwest | 16884.92400 | . 1 18 | male | 33.770 | 1 | no | southeast | 1725.55230 | . 2 28 | male | 33.000 | 3 | no | southeast | 4449.46200 | . 3 33 | male | 22.705 | 0 | no | northwest | 21984.47061 | . 4 32 | male | 28.880 | 0 | no | northwest | 3866.85520 | . ... ... | ... | ... | ... | ... | ... | ... | . 1333 50 | male | 30.970 | 3 | no | northwest | 10600.54830 | . 1334 18 | female | 31.920 | 0 | no | northeast | 2205.98080 | . 1335 18 | female | 36.850 | 0 | no | southeast | 1629.83350 | . 1336 21 | female | 25.800 | 0 | no | southwest | 2007.94500 | . 1337 61 | female | 29.070 | 0 | yes | northwest | 29141.36030 | . 1338 rows × 7 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; To prepare our data, we can borrow few classes from Scikit-Learn . from sklearn.compose import make_column_transformer from sklearn.preprocessing import MinMaxScaler, OneHotEncoder from sklearn.model_selection import train_test_split . Feature Scaling: . Scaling type what it does Scikit-Learn Function when to use . scale(refers to as normalization) | converts all values to between 0 and 1 whilst preserving the original distribution | MinMaxScaler | Use as default scaler with neural networks | . Standarization | Removes the mean and divides each value by the standard deviation | StandardScaler | Transform a feature to have close to normal distribution | . ct = make_column_transformer( (MinMaxScaler(), [&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;]), # Turn all values in these columns between 0 and 1 (OneHotEncoder(handle_unknown = &quot;ignore&quot;), [&quot;sex&quot;, &quot;smoker&quot;, &quot;region&quot;]) ) # Create our X and Y values # because we reimported our dataframe X = insurance.drop(&quot;charges&quot;, axis = 1) y = insurance[&quot;charges&quot;] # Build our train and test set X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42) # Fit the column transformer to our training data (only training data) ct.fit(X_train) # Transform training and test data with normalization(MinMaxScaler) and OneHotEncoder X_train_normal = ct.transform(X_train) X_test_normal = ct.transform(X_test) . X_train.loc[0] . age 19 sex female bmi 27.9 children 0 smoker yes region southwest Name: 0, dtype: object . X_train_normal[0], X_train_normal[12], X_train_normal[78] # we have turned all our data into numerical encoding and aso normalized the data . (array([0.60869565, 0.10734463, 0.4 , 1. , 0. , 1. , 0. , 0. , 1. , 0. , 0. ]), array([0.67391304, 0.37570621, 0. , 1. , 0. , 1. , 0. , 0. , 1. , 0. , 0. ]), array([0.43478261, 0.33118106, 0.2 , 0. , 1. , 1. , 0. , 0. , 0. , 1. , 0. ])) . X_train.shape, X_train_normal.shape . ((1070, 6), (1070, 11)) . Beautiful! our data has been normalized and One hot encoded. Let&#39;s build Neural Network on it and see how it goes. . tf.random.set_seed(42) # 1. Create the model insurance_model_4 = tf.keras.Sequential([ tf.keras.layers.Dense(100), tf.keras.layers.Dense(10), tf.keras.layers.Dense(1) ]) # 2. Compile the model insurance_model_4.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(), metrics = [&quot;mae&quot;]) # 3. Fit the model history = insurance_model_4.fit(X_train_normal, y_train, epochs= 100, verbose = 0) . insurance_model_4.evaluate(X_test_normal, y_test) . 9/9 [==============================] - 0s 2ms/step - loss: 3438.7844 - mae: 3438.7844 . [3438.784423828125, 3438.784423828125] . insurance_model_4.summary() . Model: &#34;sequential_33&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_110 (Dense) (None, 100) 1200 dense_111 (Dense) (None, 10) 1010 dense_112 (Dense) (None, 1) 11 ================================================================= Total params: 2,221 Trainable params: 2,221 Non-trainable params: 0 _________________________________________________________________ . pd.DataFrame(history.history).plot() plt.ylabel(&quot;loss&quot;) plt.xlabel(&quot;epochs&quot;) plt.title(&quot;Training curve of insurance_model_4&quot;) . Text(0.5, 1.0, &#39;Training curve of insurance_model_4&#39;) . Let&#39;s just plot some graphs. Since we have use them the least in this notebook. . X[&quot;age&quot;].plot(kind = &quot;hist&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f778b705ad0&gt; . X[&quot;bmi&quot;].plot(kind = &quot;hist&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f778ab04450&gt; . X[&quot;children&quot;].value_counts() . 0 574 1 324 2 240 3 157 4 25 5 18 Name: children, dtype: int64 . External Resources: . MIT introduction deep learning lecture 1 | Kaggle&#39;s datasets | Lion Bridge&#39;s collection of datasets | . Bibliography: . Learn TensorFlow and Deep Learning fundamentals with Python (code-first introduction) Part 1/2 . | Medical cost personal dataset . | TensorFlow documentation . | TensorFlow and Deep learning Daniel Bourke GitHub Repo . | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/deeplearning/neuralnetworks/tensorflow/python/linearregression/2022/02/11/Neural-Networks-Regression-with-TensorFlow.html",
            "relUrl": "/deeplearning/neuralnetworks/tensorflow/python/linearregression/2022/02/11/Neural-Networks-Regression-with-TensorFlow.html",
            "date": " • Feb 11, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "TensorFlow Fundamentals",
            "content": "Fundamental concepts of TensorFlow: . This notebook is an account of my working for the Tensorflow tutorial by Daniel Bourke on Youtube. The Notebook covers key concepts of tensorflow essential for Deep Learning. It also highlights key points of using the various methods of TensorFlow library and also notes the possible common errors we are going to encounter during tensorflow. The possible fixes for the errors are also included in the notebook. . TensorFlow: TensorFlow is google&#39;s open-source end to end machine learning library. The basic units of the library are tensors which are generalization of matrices to higher dimensions. TensorFlow library helps in doing the computation of tensors faster by accelerating the computation process through GPUs/TPUs. The other important library for scientific computing is NumPy and TensorFlow works well with NumPy. The only difference is tensor flow has high functionality can be used to quickly implement the code even for complex deep learning architectures, which can help us experiment more and spend more effort on making it better rather than focussing on building the Neural Networks from scratch. You can also pass on the python functions with tensorflow to accelerate the function calls. . Concepts covered in this Notebook: . Introduction to tensors | Getting information from tensors | Manipulating tensors | Tensors and Numpy | using @tf.function(a way to speed up your python functions) | Using GPUs with TensorFlow (or TPUs) | Solutions to Exercises given in the tutorial notebook. | . Introduction to Tensors . import tensorflow as tf print(tf.__version__) . 2.7.0 . scalar = tf.constant(7) scalar . &lt;tf.Tensor: shape=(), dtype=int32, numpy=7&gt; . a_scalar_1 = tf.constant(3) a_scalar_2 = tf.constant(4) . scalar.ndim . 0 . a_scalar_1.ndim . 0 . vector = tf.constant([10,101,11]) vector . &lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 10, 101, 11], dtype=int32)&gt; . vector.ndim . 1 . matrix = tf.constant([[2,3,4],[5,6,7],[8,9,0]]) matrix . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[2, 3, 4], [5, 6, 7], [8, 9, 0]], dtype=int32)&gt; . matrix.ndim . 2 . another_matrix = tf.constant([[10.,7.,4.],[3.,2.,4.]], dtype =tf.float16) another_matrix . &lt;tf.Tensor: shape=(2, 3), dtype=float16, numpy= array([[10., 7., 4.], [ 3., 2., 4.]], dtype=float16)&gt; . another_matrix_1 = tf.constant([[10.,7.,4.],[3.,2.,4.]], dtype =tf.float32) another_matrix_1 . &lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy= array([[10., 7., 4.], [ 3., 2., 4.]], dtype=float32)&gt; . The difference between both the dtypes are precision. The higher the no after the &quot;float&quot; the more exact the values inside the matrix stored in your computer. . another_matrix.ndim . 2 . Even though the matrix is (3,2) the ndim function gives the value 2. Because the number of elements in the shape gives us the number of dimensions of the matrix. Here, we have two elements (3,2) for the shape of the matrix so the ndim gives the output 2 . example_mat = tf.constant([[[1,2,3],[3,4,5]], [[6,7,3],[3,2,4]], [[3,2,1],[2,1,4]]]) example_mat . &lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[1, 2, 3], [3, 4, 5]], [[6, 7, 3], [3, 2, 4]], [[3, 2, 1], [2, 1, 4]]], dtype=int32)&gt; . example_mat.ndim . 3 . So, we have created a matrix with shape (3,2,3) there are three elements in the value of shape. so the ndim returned the value 3 . tensor = tf.constant([[[1.,0.3,0.5], [0.2,0.5,0.9], [3.,6.,7.]], [[0.2,0.5,0.8], [2.,3.5,6.7], [4.,8.,0.]], [[2.8,5.6,7.9], [0.6,7.9,6.8], [3.4,5.6,7.8]]], dtype = tf.float16) tensor . &lt;tf.Tensor: shape=(3, 3, 3), dtype=float16, numpy= array([[[1. , 0.3, 0.5], [0.2, 0.5, 0.9], [3. , 6. , 7. ]], [[0.2, 0.5, 0.8], [2. , 3.5, 6.7], [4. , 8. , 0. ]], [[2.8, 5.6, 7.9], [0.6, 7.9, 6.8], [3.4, 5.6, 7.8]]], dtype=float16)&gt; . tensor.ndim . 3 . so, now we created a tensor of 3 dimension. . What we have created so far: . Scalar: a single number | Vector: a number with direction | matrix: a two dimensional array of numbers | Tensor: an n-dimensional array of numbers 0-dimensional tensor is scalar | 1-dimensional tensor is vector | . | . Creating tensors with tf.Variable: . changeable_tensor = tf.Variable([10,7]) unchangeable_tensor = tf.constant([10,7]) changeable_tensor, unchangeable_tensor . (&lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([10, 7], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 7], dtype=int32)&gt;) . changeable_tensor[0] = 7 changeable_tensor . TypeError Traceback (most recent call last) &lt;ipython-input-18-9972a815a90d&gt; in &lt;module&gt;() 1 # Let&#39;s try to change one of the elements in our changeable tensor -&gt; 2 changeable_tensor[0] = 7 3 changeable_tensor TypeError: &#39;ResourceVariable&#39; object does not support item assignment . changeable_tensor[0].assign(7) changeable_tensor . &lt;tf.Variable &#39;Variable:0&#39; shape=(2,) dtype=int32, numpy=array([7, 7], dtype=int32)&gt; . unchangeable_tensor[0] = 7 . TypeError Traceback (most recent call last) &lt;ipython-input-20-007f4e4cfc7f&gt; in &lt;module&gt;() 1 # try changing the elements in unchangeable tensor -&gt; 2 unchangeable_tensor[0] = 7 TypeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object does not support item assignment . unchangeable_tensor[0].assign(7) unchangeable_tensor . AttributeError Traceback (most recent call last) &lt;ipython-input-21-958e786d8d1f&gt; in &lt;module&gt;() -&gt; 1 unchangeable_tensor[0].assign(7) 2 unchangeable_tensor /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in __getattr__(self, name) 440 from tensorflow.python.ops.numpy_ops import np_config 441 np_config.enable_numpy_behavior()&#34;&#34;&#34;.format(type(self).__name__, name)) --&gt; 442 self.__getattribute__(name) 443 444 @staticmethod AttributeError: &#39;tensorflow.python.framework.ops.EagerTensor&#39; object has no attribute &#39;assign&#39; . As you can see the difference between tf.Variable and tf.constant. The former one is mutable and you can change and manipulate the elements using the tf.Variable and the latter created an immutable object where you cannot change or manipulate the values of the type tf.Constant . Note: Rarely in practice you will decide whether to use tf.constant or tf.Variable to create tensors as TensorFlow does this for you. However, if in doubt, use tf.constant and change it later if needed . Creating random tensors: . Random tensors of some arbitary size which contain random numbers. These are useful during intializing random weights at beginning of neural networks. The Neural Network then learns the paramaters using gradient descent. . random_1 = tf.random.Generator.from_seed(42) random_1 = random_1.normal(shape = (3,2)) another_random_1 = tf.random.Generator.from_seed(42) another_random_1 = another_random_1.normal(shape = (3,2)) # Let&#39;s check if they are equal? random_1, another_random_1, random_1 == another_random_1 . (&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[-0.7565803 , -0.06854702], [ 0.07595026, -1.2573844 ], [-0.23193763, -1.8107855 ]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy= array([[-0.7565803 , -0.06854702], [ 0.07595026, -1.2573844 ], [-0.23193763, -1.8107855 ]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=bool, numpy= array([[ True, True], [ True, True], [ True, True]])&gt;) . So, the random tensors appear as random but they are infact pseudo random numbers. The seed acts like a starting trigger for the underlying random algorithm. Specifying the seed value will help us in producing the same results since the a random generator function produce the same random value everytime if we use a same seed value. . This can help when we are reproducing the same model from any where. The paramters that neural network is learning in each step will be different if we get different intialization values of our weights. If we used the same seed value as mentioned in the previously implemented model we can generate the same intialization at the beginning and produce the exact same results. . random_2 = tf.random.Generator.from_seed(42) # seed is set for reproducing the same result random_2 = random_2.normal(shape = (3,4)) . random_2 . &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[-0.7565803 , -0.06854702, 0.07595026, -1.2573844 ], [-0.23193763, -1.8107855 , 0.09988727, -0.50998646], [-0.7535805 , -0.57166284, 0.1480774 , -0.23362993]], dtype=float32)&gt; . random_3 = tf.random.Generator.from_seed(42) random_3 = random_3.normal(shape = (4,4)) random_3 . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[-0.7565803 , -0.06854702, 0.07595026, -1.2573844 ], [-0.23193763, -1.8107855 , 0.09988727, -0.50998646], [-0.7535805 , -0.57166284, 0.1480774 , -0.23362993], [-0.3522796 , 0.40621263, -1.0523509 , 1.2054597 ]], dtype=float32)&gt; . random_4 = tf.random.Generator.from_seed(21) random_4 = random_4.normal(shape = (10,10)) random_4 . &lt;tf.Tensor: shape=(10, 10), dtype=float32, numpy= array([[-1.322665 , -0.02279496, -0.1383193 , 0.44207528, -0.7531523 , 2.0261486 , -0.06997604, 0.85445154, 0.1175475 , 0.03493892], [-1.5700307 , 0.4457582 , 0.10944034, -0.8035768 , -1.7166729 , 0.3738578 , -0.14371012, -0.34646833, 1.1456194 , -0.416 ], [ 0.43369916, 1.0241015 , -0.74785167, -0.59090924, -1.2060374 , 0.8307429 , 1.0951619 , 1.3672234 , -0.54532146, 1.9302735 ], [-0.3151453 , -0.8761205 , -2.7316678 , -0.15730922, 1.3692921 , -0.4367834 , 0.8357487 , 0.20849545, 1.4040174 , -2.735283 ], [ 1.2232229 , -1.8653691 , 0.00511209, -1.0493753 , 0.7901182 , 1.585549 , 0.4356279 , 0.23645182, -0.1589871 , 1.302304 ], [ 0.9592239 , 0.85874265, -1.5181769 , 1.4020647 , 1.5570306 , -0.96762174, 0.495291 , -0.648484 , -1.8700892 , 2.7830641 ], [-0.645002 , 0.18022095, -0.14656258, 0.34374258, 0.41367555, 0.17573498, -1.0871261 , 0.45905176, 0.20386009, 0.562024 ], [-2.3001142 , -1.349454 , 0.81485 , 1.2790666 , 0.02203509, 1.5428121 , 0.78953624, 0.53897345, -0.48535708, 0.74055266], [ 0.31662667, -1.4391748 , 0.58923835, -1.4268045 , -0.7565803 , -0.06854702, 0.07595026, -1.2573844 , -0.23193763, -1.8107855 ], [ 0.09988727, -0.50998646, -0.7535805 , -0.57166284, 0.1480774 , -0.23362993, -0.3522796 , 0.40621263, -1.0523509 , 1.2054597 ]], dtype=float32)&gt; . random_5 = tf.random.Generator.from_seed(5) random_5 = random_5.normal(shape = (3,3,3)) random_5 . &lt;tf.Tensor: shape=(3, 3, 3), dtype=float32, numpy= array([[[ 1.0278524 , 0.27974114, -0.01347923], [ 1.845181 , 0.97061104, -1.0242516 ], [-0.6544423 , -0.29738766, -1.3240396 ]], [[ 0.28785667, -0.8757901 , -0.08857018], [ 0.69211644, 0.84215707, -0.06378496], [ 0.92800784, -0.6039789 , -0.1766927 ]], [[ 0.04221033, 0.29037967, -0.29604465], [-0.21134205, 0.01063002, 1.5165398 ], [ 0.27305737, -0.29925638, -0.3652325 ]]], dtype=float32)&gt; . random_6 = tf.random.Generator.from_seed(6) random_6 = random_6.normal(shape = (5,5,5)) random_6 . &lt;tf.Tensor: shape=(5, 5, 5), dtype=float32, numpy= array([[[ 0.97061104, -1.0242516 , -0.6544423 , -0.29738766, -1.3240396 ], [ 0.28785667, -0.8757901 , -0.08857018, 0.69211644, 0.84215707], [-0.06378496, 0.92800784, -0.6039789 , -0.1766927 , 0.04221033], [ 0.29037967, -0.29604465, -0.21134205, 0.01063002, 1.5165398 ], [ 0.27305737, -0.29925638, -0.3652325 , 0.61883307, -1.0130816 ]], [[ 0.28291714, 1.2132233 , 0.46988967, 0.37944323, -0.6664026 ], [ 0.6054596 , 0.19181173, 0.8045827 , 0.4769051 , -0.7812124 ], [-0.996891 , 0.33149973, -0.5445254 , 1.5222508 , 0.59303206], [-0.63509274, 0.3703566 , -1.0939722 , -0.4601445 , 1.5420506 ], [-0.16822556, -0.4390865 , -0.4129243 , 0.35877243, -1.9095894 ]], [[-0.2094769 , 0.8286217 , -0.06695071, -0.35105535, 1.0884082 ], [-1.3863064 , 0.88051325, -1.6833194 , 0.86754173, -0.19625713], [-1.322665 , -0.02279496, -0.1383193 , 0.44207528, -0.7531523 ], [ 2.0261486 , -0.06997604, 0.85445154, 0.1175475 , 0.03493892], [-1.5700307 , 0.4457582 , 0.10944034, -0.8035768 , -1.7166729 ]], [[ 0.3738578 , -0.14371012, -0.34646833, 1.1456194 , -0.416 ], [ 0.43369916, 1.0241015 , -0.74785167, -0.59090924, -1.2060374 ], [ 0.8307429 , 1.0951619 , 1.3672234 , -0.54532146, 1.9302735 ], [-0.3151453 , -0.8761205 , -2.7316678 , -0.15730922, 1.3692921 ], [-0.4367834 , 0.8357487 , 0.20849545, 1.4040174 , -2.735283 ]], [[ 1.2232229 , -1.8653691 , 0.00511209, -1.0493753 , 0.7901182 ], [ 1.585549 , 0.4356279 , 0.23645182, -0.1589871 , 1.302304 ], [ 0.9592239 , 0.85874265, -1.5181769 , 1.4020647 , 1.5570306 ], [-0.96762174, 0.495291 , -0.648484 , -1.8700892 , 2.7830641 ], [-0.645002 , 0.18022095, -0.14656258, 0.34374258, 0.41367555]]], dtype=float32)&gt; . shuffle the order of elements in a tensor . not_shuffled = tf.constant([[10,7], [3,4], [2,3]]) # shuffle our non-shuffled tensor: tf.random.shuffle(not_shuffled) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 3, 4], [ 2, 3], [10, 7]], dtype=int32)&gt; . not_shuffled . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4], [ 2, 3]], dtype=int32)&gt; . tf.random.shuffle(not_shuffled) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 3, 4], [10, 7], [ 2, 3]], dtype=int32)&gt; . tf.random.shuffle(not_shuffled, seed = 42) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 2, 3], [ 3, 4], [10, 7]], dtype=int32)&gt; . tf.random.shuffle(not_shuffled, seed = 42) # this kind of setting the seed only work at operation-level # we need to declare a global seed to make this work . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 2, 3], [ 3, 4], [10, 7]], dtype=int32)&gt; . Even though we set the same seed the value is getting changed. Why is this happening? refer to this link :tf.random.seed_set documentation . # Here we set the seed as global seed tf.random.set_seed(42) tf.random.shuffle(not_shuffled) . &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[ 3, 4], [ 2, 3], [10, 7]], dtype=int32)&gt; . Exercise working: . Exercise: Read through tensorflow docs on random seed generation. Practice 5 random seed generation examples. . Other ways to make tensors . # we need to pass the arguments : shape, dtype etc. # create a tensor of all ones tf.ones([10,7]) . &lt;tf.Tensor: shape=(10, 7), dtype=float32, numpy= array([[1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1.]], dtype=float32)&gt; . tf.zeros([10,7]) . &lt;tf.Tensor: shape=(10, 7), dtype=float32, numpy= array([[0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt; . Turn Numpy arrays into tensors: . The main difference between Numpy and TensorFlow tensors is that tensors can be run on GPUs/TPUs. . import numpy as np numpy_A = np.arange(1,25,dtype = np.int32) numpy_A # X = tf.constant(some_matrix) # capital for tensor or matrix # y = tf.constant(vector) # non-capital for vector . array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], dtype=int32) . A = tf.constant(numpy_A, shape = (2,3,4)) B = tf.constant(numpy_A) A,B . (&lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 1, 2, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12]], [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(24,), dtype=int32, numpy= array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], dtype=int32)&gt;) . A.ndim . 3 . The unmodified shape is the same shape as our NumPy vector. If you want to change the shape of the array with tf.constant, we need to make sure the product of the three values of dimensions should be equal to the no. of values in the unmodified array . So, anything we have in NumPy we can pass it to a tensor. . numpy_C = np.arange(1,101,dtype = np.float16) numpy_D = np.arange(1,37,dtype = np.float32) numpy_C, numpy_D . (array([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42., 43., 44., 45., 46., 47., 48., 49., 50., 51., 52., 53., 54., 55., 56., 57., 58., 59., 60., 61., 62., 63., 64., 65., 66., 67., 68., 69., 70., 71., 72., 73., 74., 75., 76., 77., 78., 79., 80., 81., 82., 83., 84., 85., 86., 87., 88., 89., 90., 91., 92., 93., 94., 95., 96., 97., 98., 99., 100.], dtype=float16), array([ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28., 29., 30., 31., 32., 33., 34., 35., 36.], dtype=float32)) . C = tf.constant(numpy_C, shape = (10,10)) D = tf.constant(numpy_D,shape = (6,6)) C,D . (&lt;tf.Tensor: shape=(10, 10), dtype=float16, numpy= array([[ 1., 2., 3., 4., 5., 6., 7., 8., 9., 10.], [ 11., 12., 13., 14., 15., 16., 17., 18., 19., 20.], [ 21., 22., 23., 24., 25., 26., 27., 28., 29., 30.], [ 31., 32., 33., 34., 35., 36., 37., 38., 39., 40.], [ 41., 42., 43., 44., 45., 46., 47., 48., 49., 50.], [ 51., 52., 53., 54., 55., 56., 57., 58., 59., 60.], [ 61., 62., 63., 64., 65., 66., 67., 68., 69., 70.], [ 71., 72., 73., 74., 75., 76., 77., 78., 79., 80.], [ 81., 82., 83., 84., 85., 86., 87., 88., 89., 90.], [ 91., 92., 93., 94., 95., 96., 97., 98., 99., 100.]], dtype=float16)&gt;, &lt;tf.Tensor: shape=(6, 6), dtype=float32, numpy= array([[ 1., 2., 3., 4., 5., 6.], [ 7., 8., 9., 10., 11., 12.], [13., 14., 15., 16., 17., 18.], [19., 20., 21., 22., 23., 24.], [25., 26., 27., 28., 29., 30.], [31., 32., 33., 34., 35., 36.]], dtype=float32)&gt;) . C.ndim, D.ndim . (2, 2) . Getting Information from tensors: . Attributes: When dealing with tensors you probably want to e aware of the following attributes: . Shape : The length of each of the dimensions of a tensor code: tensor.shape | . | Rank: The number of tensor dimensions. A scalar has a rank 0, vector has rank 1,a matrix is rank 2, a tensor has a rank n. code: tensor.ndim | . | Axis or dimension : A particular dimension of a tensor code: tensor[0], tensor[:,1] etc | . | Size : The total number of items in the tensor. code: tf.size(tensor) | . | . rank_4_tensor = tf.zeros(shape = [2,3,4,5]) rank_4_tensor . &lt;tf.Tensor: shape=(2, 3, 4, 5), dtype=float32, numpy= array([[[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]], [[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]]], dtype=float32)&gt; . rank_4_tensor[0] . &lt;tf.Tensor: shape=(3, 4, 5), dtype=float32, numpy= array([[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]], dtype=float32)&gt; . rank_4_tensor[:,1] . &lt;tf.Tensor: shape=(2, 4, 5), dtype=float32, numpy= array([[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]], dtype=float32)&gt; . rank_4_tensor.shape, rank_4_tensor.ndim, tf.size(rank_4_tensor) . (TensorShape([2, 3, 4, 5]), 4, &lt;tf.Tensor: shape=(), dtype=int32, numpy=120&gt;) . print(&quot;Datatype of every element&quot;, rank_4_tensor.dtype) print(&quot;Number of dimensions (rank): &quot;, rank_4_tensor.ndim) print(&quot;Shape of tensor: &quot;, rank_4_tensor.shape) print(&quot;Elements along the 0 axis:&quot;, rank_4_tensor.shape[0]) print(&quot;Elements along the last axis:&quot;, rank_4_tensor.shape[-1]) print(&quot;Total number of elements in our tensor:&quot;,tf.size(rank_4_tensor).numpy() ) . Datatype of every element &lt;dtype: &#39;float32&#39;&gt; Number of dimensions (rank): 4 Shape of tensor: (2, 3, 4, 5) Elements along the 0 axis: 2 Elements along the last axis: 5 Total number of elements in our tensor: 120 . # we can put all the print statements in a function to reuse it whenever we want def print_attributes_of_tensor(tensor_name): print(&quot;Datatype of every element&quot;, tensor_name.dtype) print(&quot;Number of dimensions (rank): &quot;, tensor_name.ndim) print(&quot;Shape of tensor: &quot;, tensor_name.shape) print(&quot;Elements along the 0 axis:&quot;, tensor_name.shape[0]) print(&quot;Elements along the last axis:&quot;, tensor_name.shape[-1]) print(&quot;Total number of elements in our tensor:&quot;,tf.size(tensor_name).numpy()) return 0 . print_attributes_of_tensor(rank_4_tensor) . Datatype of every element &lt;dtype: &#39;float32&#39;&gt; Number of dimensions (rank): 4 Shape of tensor: (2, 3, 4, 5) Elements along the 0 axis: 2 Elements along the last axis: 5 Total number of elements in our tensor: 120 . 0 . Now, we can reuse the function to print the attributes of any tensor by passing the tensor name as function argument. We can add more print statement to display more attributes of the tensor. . Indexing tensors: . Tensors can be indexed just like Python Lists . some_list = [1,2,3,4] some_list[:2] . [1, 2] . some_list[:1] . [1] . rank_4_tensor[:2,:2,:2,:2] . &lt;tf.Tensor: shape=(2, 2, 2, 2), dtype=float32, numpy= array([[[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]], [[[0., 0.], [0., 0.]], [[0., 0.], [0., 0.]]]], dtype=float32)&gt; . rank_4_tensor.shape . TensorShape([2, 3, 4, 5]) . rank_4_tensor[:,:1,:1,:] . &lt;tf.Tensor: shape=(2, 1, 1, 5), dtype=float32, numpy= array([[[[0., 0., 0., 0., 0.]]], [[[0., 0., 0., 0., 0.]]]], dtype=float32)&gt; . rank_2_tensor = tf.constant([[10,7], [3,4]]) rank_2_tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt; . print_attributes_of_tensor(rank_2_tensor) . Datatype of every element &lt;dtype: &#39;int32&#39;&gt; Number of dimensions (rank): 2 Shape of tensor: (2, 2) Elements along the 0 axis: 2 Elements along the last axis: 2 Total number of elements in our tensor: 4 . 0 . some_list, some_list[-1] . ([1, 2, 3, 4], 4) . rank_2_tensor[:,-1] . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([7, 4], dtype=int32)&gt; . rank_3_tensor = rank_2_tensor[..., tf.newaxis] rank_3_tensor . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[10], [ 7]], [[ 3], [ 4]]], dtype=int32)&gt; . we added a new dimension at the end &quot;...&quot; means indicating all the other previous present dimensions and the new axis gets added at the end . tf.expand_dims(rank_2_tensor, axis = -1) # &quot;-1&quot; means expand the final axis # see the documentation for more details . &lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy= array([[[10], [ 7]], [[ 3], [ 4]]], dtype=int32)&gt; . tf.expand_dims(rank_2_tensor, axis = 0) # expand the 0-axis . &lt;tf.Tensor: shape=(1, 2, 2), dtype=int32, numpy= array([[[10, 7], [ 3, 4]]], dtype=int32)&gt; . tf.expand_dims(rank_2_tensor, axis = 1) . &lt;tf.Tensor: shape=(2, 1, 2), dtype=int32, numpy= array([[[10, 7]], [[ 3, 4]]], dtype=int32)&gt; . . Manipulating tensors(tensors operations): . Basic Operations +,-,*,/ . tensor = tf.constant([[10,7], [3,4]]) tensor+10 . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[20, 17], [13, 14]], dtype=int32)&gt; . tensor*15 . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[150, 105], [ 45, 60]], dtype=int32)&gt; . tensor - 10 . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 0, -3], [-7, -6]], dtype=int32)&gt; . tensor /10 . &lt;tf.Tensor: shape=(2, 2), dtype=float64, numpy= array([[1. , 0.7], [0.3, 0.4]])&gt; . tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt; . tf.math.multiply(tensor,3) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[30, 21], [ 9, 12]], dtype=int32)&gt; . tf.add(tensor,tensor) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[20, 14], [ 6, 8]], dtype=int32)&gt; . Matrix Multiplication in TensorFlow: . In Machine learning, matrix multiplication is one of the most common tensor operations. . print(tensor) tf.linalg.matmul(tensor, tensor) # or tf.matmul also works . tf.Tensor( [[10 7] [ 3 4]], shape=(2, 2), dtype=int32) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[121, 98], [ 42, 37]], dtype=int32)&gt; . tensor, tensor . (&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt;) . tensor * tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[100, 49], [ 9, 16]], dtype=int32)&gt; . tensor @ tensor . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[121, 98], [ 42, 37]], dtype=int32)&gt; . tensor.shape . TensorShape([2, 2]) . X = tf.constant([[1,2], [3,4], [5,6]]) # create another (3,2) Y = tf.constant([[7,8], [9,10], [11,12]]) . X @ Y &#39;&#39;&#39; This gives an error because X and Y doesn&#39;t satisfy the criteria for matrix multiplication &#39;&#39;&#39; . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-77-04133f28c872&gt; in &lt;module&gt;() 1 # Try to matrix multiply tensors of same shape. -&gt; 2 X @ Y 3 &#39;&#39;&#39; This gives an error because X and Y doesn&#39;t satisfy 4 the criteria for matrix multiplication &#39;&#39;&#39; /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7108 7109 InvalidArgumentError: Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul] . tf.matmul(X,Y) &#39;&#39;&#39; This gives an error because X and Y doesn&#39;t satisfy the criteria for matrix multiplication &#39;&#39;&#39; . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-78-6bfc63f024cf&gt; in &lt;module&gt;() 1 # Try to matrix multiply tensors of same shape. -&gt; 2 tf.matmul(X,Y) 3 &#39;&#39;&#39; This gives an error because X and Y doesn&#39;t satisfy 4 the criteria for matrix multiplication &#39;&#39;&#39; /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7108 7109 InvalidArgumentError: Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul] . This fails because for two matrices to be multiplied the dimensions should satisfy these two criteria: . Inner dimensions must match | The resulting matrix has the shape of the inner dimensions | . tf.reshape(Y, shape = (2,3)) . &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt; . tf.matmul(X,Y) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-80-9e9781d51065&gt; in &lt;module&gt;() 1 # now we mutliply and check -&gt; 2 tf.matmul(X,Y) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7108 7109 InvalidArgumentError: Matrix size-incompatible: In[0]: [3,2], In[1]: [3,2] [Op:MatMul] . tf.matmul(X, tf.reshape(Y, shape = (2,3))) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 27, 30, 33], [ 61, 68, 75], [ 95, 106, 117]], dtype=int32)&gt; . This works !!! . tf.matmul(tf.reshape(X, shape = (2,3)), Y) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 58, 64], [139, 154]], dtype=int32)&gt; . X.shape, tf.reshape(Y, shape = (2,3)) . (TensorShape([3, 2]), &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[ 7, 8, 9], [10, 11, 12]], dtype=int32)&gt;) . You can see that the inner dimensions now match, and the output of the dot product is the same as outer . Note: Matrix Multiplication is also called the &quot;Dot Product&quot; . X, tf.transpose(X), tf.reshape(X, shape= (3,2)) . (&lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 2], [3, 4], [5, 6]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy= array([[1, 3, 5], [2, 4, 6]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(3, 2), dtype=int32, numpy= array([[1, 2], [3, 4], [5, 6]], dtype=int32)&gt;) . The dot product . You can perform matrix multiplication using: . tf.matmul() | tf.tensordot() | @ | . Perform the dot product on X and Y (requires X or Y to be transposed) . tf.tensordot(tf.transpose(X), Y , axes =1 ) . &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[ 89, 98], [116, 128]], dtype=int32)&gt; . we can use either transpose or reshape . tf.matmul(X, tf.transpose(Y)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 23, 29, 35], [ 53, 67, 81], [ 83, 105, 127]], dtype=int32)&gt; . tf.matmul(X, tf.reshape(Y, shape = (2,3))) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 27, 30, 33], [ 61, 68, 75], [ 95, 106, 117]], dtype=int32)&gt; . we are getting different values for the dot product in the above two cases. That means tf.reshape() and tf.transpose() does not exactly do the same thing. In some cases we might get output of both the functions same but not always. . print(&quot;Normal Y:&quot;) print(Y, &quot; n&quot;) print(&quot;Y reshaped to (2,3):&quot;) print(tf.reshape(Y, (2,3)),&quot; n&quot;) print(&quot;Y transposed:&quot;) print(tf.transpose(Y)) . Normal Y: tf.Tensor( [[ 7 8] [ 9 10] [11 12]], shape=(3, 2), dtype=int32) Y reshaped to (2,3): tf.Tensor( [[ 7 8 9] [10 11 12]], shape=(2, 3), dtype=int32) Y transposed: tf.Tensor( [[ 7 9 11] [ 8 10 12]], shape=(2, 3), dtype=int32) . tf.matmul(X, tf.transpose(Y)) . &lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy= array([[ 23, 29, 35], [ 53, 67, 81], [ 83, 105, 127]], dtype=int32)&gt; . Generally, when performing matrix multiplication on two tensors, and one of the axes doesn&#39;t line up, you will transpose rather than reshape one of the tensors to satisfy the matrix multiplication rules. . Changing the datatype of a tensor . The default datatype for tensors is int32 but however if you want to use other datatype for your tensor. we can change the datatype of the tensor. . B = tf.constant([1.7,7.4]) B.dtype . tf.float32 . C = tf.constant([7,10]) C.dtype . tf.int32 . D = tf.cast(B, dtype = tf.float16) D, D.dtype . (&lt;tf.Tensor: shape=(2,), dtype=float16, numpy=array([1.7, 7.4], dtype=float16)&gt;, tf.float16) . E = tf.cast(C, dtype = tf.float32) E.dtype . tf.float32 . E_float16 = tf.cast(E, dtype = tf.float16) E_float16 . &lt;tf.Tensor: shape=(2,), dtype=float16, numpy=array([ 7., 10.], dtype=float16)&gt; . Aggregating tensors . Aggregating tensors = condensing them from multiple values down to a smaller amount of values. . D = tf.constant([-7,10]) D . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([-7, 10], dtype=int32)&gt; . tf.abs(D) . &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 7, 10], dtype=int32)&gt; . Let&#39;s go through the following forms of aggregation: . Get the minimum | Get the maximum | Get the mean of a tensor | Get the sum of a tensor | . E = tf.constant(np.random.randint(0,100,size = 50)) E . &lt;tf.Tensor: shape=(50,), dtype=int64, numpy= array([50, 30, 8, 9, 11, 58, 26, 64, 89, 4, 96, 20, 19, 35, 25, 33, 53, 45, 15, 73, 59, 29, 22, 16, 54, 65, 16, 87, 25, 13, 50, 35, 77, 10, 88, 34, 49, 70, 99, 37, 3, 93, 98, 48, 50, 35, 66, 97, 37, 93])&gt; . tf.size(E), E.shape, E.ndim . (&lt;tf.Tensor: shape=(), dtype=int32, numpy=50&gt;, TensorShape([50]), 1) . tf.reduce_min(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=3&gt; . tf.reduce_max(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=99&gt; . tf.reduce_mean(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=46&gt; . tf.reduce_sum(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=2318&gt; . Exercise: Find the variance and standard deviation of our E tensor using TensorFlow methods . # Find the variance of our tensor import tensorflow_probability as tfp tfp.stats.variance(E) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=832&gt; . tf.math.reduce_std(E) # Error : The input must be either real or complex # so cast it to float32 . TypeError Traceback (most recent call last) &lt;ipython-input-120-a76c926197b7&gt; in &lt;module&gt;() -&gt; 1 tf.math.reduce_std(E) 2 # Error : The input must be either real or complex /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in reduce_variance(input_tensor, axis, keepdims, name) 2673 means = reduce_mean(input_tensor, axis=axis, keepdims=True) 2674 if means.dtype.is_integer: -&gt; 2675 raise TypeError(f&#34;Input must be either real or complex. &#34; 2676 f&#34;Received integer type {means.dtype}.&#34;) 2677 diff = input_tensor - means TypeError: Input must be either real or complex. Received integer type &lt;dtype: &#39;int64&#39;&gt;. . tf.math.reduce_std(tf.cast(E, dtype = tf.float32)) # The method works only if the tensor elements are either real or complex . &lt;tf.Tensor: shape=(), dtype=float32, numpy=28.844936&gt; . Find the positional maximum and minimum . We find this helpful for our output probabilities that come from neural network. . tf.random.set_seed(42) F = tf.random.uniform(shape =[50]) F . &lt;tf.Tensor: shape=(50,), dtype=float32, numpy= array([0.6645621 , 0.44100678, 0.3528825 , 0.46448255, 0.03366041, 0.68467236, 0.74011743, 0.8724445 , 0.22632635, 0.22319686, 0.3103881 , 0.7223358 , 0.13318717, 0.5480639 , 0.5746088 , 0.8996835 , 0.00946367, 0.5212307 , 0.6345445 , 0.1993283 , 0.72942245, 0.54583454, 0.10756552, 0.6767061 , 0.6602763 , 0.33695042, 0.60141766, 0.21062577, 0.8527372 , 0.44062173, 0.9485276 , 0.23752594, 0.81179297, 0.5263394 , 0.494308 , 0.21612847, 0.8457197 , 0.8718841 , 0.3083862 , 0.6868038 , 0.23764038, 0.7817228 , 0.9671384 , 0.06870162, 0.79873943, 0.66028714, 0.5871513 , 0.16461694, 0.7381023 , 0.32054043], dtype=float32)&gt; . tf.argmax(F) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=42&gt; . np.argmax(F) . 42 . F[tf.argmax(F)] . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.9671384&gt; . assert F[tf.argmax(F)] == tf.reduce_max(F) . No error so we got it right! . F[tf.argmax(F)] == tf.reduce_max(F) . &lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt; . tf.argmin(F) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=16&gt; . F[tf.argmin(F)] . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.009463668&gt; . Squeezing a tensor (removing all single dimensions) . tf.random.set_seed(42) G = tf.constant(tf.random.uniform(shape = [50]), shape = (1,1,1,1,50)) G . &lt;tf.Tensor: shape=(1, 1, 1, 1, 50), dtype=float32, numpy= array([[[[[0.6645621 , 0.44100678, 0.3528825 , 0.46448255, 0.03366041, 0.68467236, 0.74011743, 0.8724445 , 0.22632635, 0.22319686, 0.3103881 , 0.7223358 , 0.13318717, 0.5480639 , 0.5746088 , 0.8996835 , 0.00946367, 0.5212307 , 0.6345445 , 0.1993283 , 0.72942245, 0.54583454, 0.10756552, 0.6767061 , 0.6602763 , 0.33695042, 0.60141766, 0.21062577, 0.8527372 , 0.44062173, 0.9485276 , 0.23752594, 0.81179297, 0.5263394 , 0.494308 , 0.21612847, 0.8457197 , 0.8718841 , 0.3083862 , 0.6868038 , 0.23764038, 0.7817228 , 0.9671384 , 0.06870162, 0.79873943, 0.66028714, 0.5871513 , 0.16461694, 0.7381023 , 0.32054043]]]]], dtype=float32)&gt; . G.shape . TensorShape([1, 1, 1, 1, 50]) . G_squeezed = tf.squeeze(G) G_squeezed, G_squeezed.shape . (&lt;tf.Tensor: shape=(50,), dtype=float32, numpy= array([0.6645621 , 0.44100678, 0.3528825 , 0.46448255, 0.03366041, 0.68467236, 0.74011743, 0.8724445 , 0.22632635, 0.22319686, 0.3103881 , 0.7223358 , 0.13318717, 0.5480639 , 0.5746088 , 0.8996835 , 0.00946367, 0.5212307 , 0.6345445 , 0.1993283 , 0.72942245, 0.54583454, 0.10756552, 0.6767061 , 0.6602763 , 0.33695042, 0.60141766, 0.21062577, 0.8527372 , 0.44062173, 0.9485276 , 0.23752594, 0.81179297, 0.5263394 , 0.494308 , 0.21612847, 0.8457197 , 0.8718841 , 0.3083862 , 0.6868038 , 0.23764038, 0.7817228 , 0.9671384 , 0.06870162, 0.79873943, 0.66028714, 0.5871513 , 0.16461694, 0.7381023 , 0.32054043], dtype=float32)&gt;, TensorShape([50])) . One hot encoding tensors . What is one-hot encoding? . some_list = [0,1,2,3] # could be red, green , blue , purple # one hot encoding our list of indices tf.one_hot(some_list, depth = 4) . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[1., 0., 0., 0.], [0., 1., 0., 0.], [0., 0., 1., 0.], [0., 0., 0., 1.]], dtype=float32)&gt; . tf.one_hot(some_list, depth = 4, on_value = &quot;Yo I love deep learning&quot;, off_value = &quot;I also like to write&quot;) . &lt;tf.Tensor: shape=(4, 4), dtype=string, numpy= array([[b&#39;Yo I love deep learning&#39;, b&#39;I also like to write&#39;, b&#39;I also like to write&#39;, b&#39;I also like to write&#39;], [b&#39;I also like to write&#39;, b&#39;Yo I love deep learning&#39;, b&#39;I also like to write&#39;, b&#39;I also like to write&#39;], [b&#39;I also like to write&#39;, b&#39;I also like to write&#39;, b&#39;Yo I love deep learning&#39;, b&#39;I also like to write&#39;], [b&#39;I also like to write&#39;, b&#39;I also like to write&#39;, b&#39;I also like to write&#39;, b&#39;Yo I love deep learning&#39;]], dtype=object)&gt; . More on math functions: . squaring | log | square root | . H = tf.range(1,10) H . &lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)&gt; . tf.square(H) . &lt;tf.Tensor: shape=(9,), dtype=int32, numpy=array([ 1, 4, 9, 16, 25, 36, 49, 64, 81], dtype=int32)&gt; . tf.sqrt(H) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-142-f2dbaafeb52c&gt; in &lt;module&gt;() -&gt; 1 tf.sqrt(H) /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py in error_handler(*args, **kwargs) 151 except Exception as e: 152 filtered_tb = _process_traceback_frames(e.__traceback__) --&gt; 153 raise e.with_traceback(filtered_tb) from None 154 finally: 155 del filtered_tb /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7108 7109 InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128 ; NodeDef: {{node Sqrt}}; Op&lt;name=Sqrt; signature=x:T -&gt; y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:Sqrt] . we got an error here because tensors of dtype int32 is not allowed as arguments for sqrt function. So, we cast it to different datatype . tf.sqrt(tf.cast(H, dtype = tf.float32)) . &lt;tf.Tensor: shape=(9,), dtype=float32, numpy= array([0.99999994, 1.4142134 , 1.7320508 , 1.9999999 , 2.236068 , 2.4494896 , 2.6457512 , 2.8284268 , 3. ], dtype=float32)&gt; . tf.math.log(H) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-144-d7e970c5bd0b&gt; in &lt;module&gt;() 1 # Find the log -&gt; 2 tf.math.log(H) /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py in log(x, name) 5468 return _result 5469 except _core._NotOkStatusException as e: -&gt; 5470 _ops.raise_from_not_ok_status(e, name) 5471 except _core._FallbackException: 5472 pass /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 7105 def raise_from_not_ok_status(e, name): 7106 e.message += (&#34; name: &#34; + name if name is not None else &#34;&#34;) -&gt; 7107 raise core._status_to_exception(e) from None # pylint: disable=protected-access 7108 7109 InvalidArgumentError: Value for attr &#39;T&#39; of int32 is not in the list of allowed values: bfloat16, half, float, double, complex64, complex128 ; NodeDef: {{node Log}}; Op&lt;name=Log; signature=x:T -&gt; y:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:Log] . we also get the same error for this too. so cast the argument tensor to one of the allowed values. . tf.math.log(tf.cast(H, dtype = tf.float32)) . &lt;tf.Tensor: shape=(9,), dtype=float32, numpy= array([0. , 0.6931472, 1.0986123, 1.3862944, 1.609438 , 1.7917595, 1.9459102, 2.0794415, 2.1972246], dtype=float32)&gt; . Tensors and NumPy . NumPy is a package used for scientific computing. The most fundamental type in NumPy is numpy array. TensorFlow interacts beautifully with NumPy arrays. . J = tf.constant(np.array([3.,7.,10.])) J . &lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 3., 7., 10.])&gt; . np.array(J), type(np.array(J)) . (array([ 3., 7., 10.]), numpy.ndarray) . J.numpy() , type(J.numpy()) . (array([ 3., 7., 10.]), numpy.ndarray) . J = tf.constant([3.]) J.numpy()[0] . 3.0 . numpy_J = tf.constant(np.array([3.,7.,10.])) tensor_J = tf.constant([3.,7.,10.]) # Check the datatypes of each numpy_J.dtype , tensor_J.dtype . (tf.float64, tf.float32) . We can see above that creating tensors directly from tensorflow will create a default dtype of float32 values but if we pass in numpy array to tf.constant the default dtype of created tensor is float64 . Using @tf.function . In your TensorFlow adventures, you might come across Python functions which have the decorator @tf.function. . But in short, decorators modify a function in one way or another. . In the @tf.function decorator case, it turns a Python function into a callable TensorFlow graph. Which is a fancy way of saying, if you&#39;ve written your own Python function, and you decorate it with @tf.function, when you export your code (to potentially run on another device), TensorFlow will attempt to convert it into a fast(er) version of itself (by making it part of a computation graph). . For more on this, read the Better performnace with tf.function guide. . def function(x, y): return x ** 2 + y x = tf.constant(np.arange(0, 10)) y = tf.constant(np.arange(10, 20)) function(x, y) . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 10, 12, 16, 22, 30, 40, 52, 66, 82, 100])&gt; . @tf.function def tf_function(x, y): return x ** 2 + y tf_function(x, y) . &lt;tf.Tensor: shape=(10,), dtype=int64, numpy=array([ 10, 12, 16, 22, 30, 40, 52, 66, 82, 100])&gt; . If you noticed no difference between the above two functions (the decorated one and the non-decorated one) you&#39;d be right. . Much of the difference happens behind the scenes. One of the main ones being potential code speed-ups where possible. . Using GPUs: . We&#39;ve mentioned GPUs plenty of times throughout this notebook. . So how do you check if you&#39;ve got one available? . You can check if you&#39;ve got access to a GPU using tf.config.list_physical_devices(). . print(tf.config.list_physical_devices(&#39;GPU&#39;)) . [] . The PC I am working from has no GPU support. . import tensorflow as tf print(tf.config.list_physical_devices(&#39;GPU&#39;)) . [] . If you&#39;ve got access to a GPU, the cell above should output something like: . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;)] . You can also find information about your GPU using !nvidia-smi. . 🔑 Note: If you have access to a GPU, TensorFlow will automatically use it whenever possible. . Solutions to the Exercises given in the tutorial Notebook: . Create a vector, scalar, matrix and tensor with values of your choosing using tf.constant(). | A1 = tf.constant([3]) # scalar A2 = tf.constant([10, 7]) # vector A3 = tf.constant([[10,7], [3,4]]) # matrix A4 = tf.constant([[[10,7,3], [3,4,5]], [[2,3,4], [7,8,9]], [[1,2,3], [6,7,8]]]) # tensor of dimension 3 A1,A2,A3,A4 . (&lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([3], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 7], dtype=int32)&gt;, &lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy= array([[10, 7], [ 3, 4]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(3, 2, 3), dtype=int32, numpy= array([[[10, 7, 3], [ 3, 4, 5]], [[ 2, 3, 4], [ 7, 8, 9]], [[ 1, 2, 3], [ 6, 7, 8]]], dtype=int32)&gt;) . Find the shape, rank and size of the tensors you created in 1. | tf.shape(A1), tf.size(A1), tf.rank(A1) . (&lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;) . tf.shape(A2), tf.size(A2), tf.rank(A2) . (&lt;tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;) . tf.shape(A3), tf.size(A3), tf.rank(A3) . (&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=4&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;) . tf.shape(A4), tf.size(A4), tf.rank(A4) . (&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 2, 3], dtype=int32)&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=18&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;) . Create two tensors containing random values between 0 and 1 with shape [5, 300]. | tf.random.set_seed(42) B1 = tf.random.uniform([5,300], minval = 0, maxval = 1) # it works even if we not specify the min and max val since the function arguments defaults to 0 and 1 B2 = tf.random.uniform([5,300], minval = 0, maxval = 1) B1,B2 . (&lt;tf.Tensor: shape=(5, 300), dtype=float32, numpy= array([[0.6645621 , 0.44100678, 0.3528825 , ..., 0.31410468, 0.7593535 , 0.03699052], [0.532024 , 0.29129946, 0.10571766, ..., 0.54052293, 0.31425726, 0.2200619 ], [0.08404207, 0.03614604, 0.97732127, ..., 0.21516645, 0.9786098 , 0.00726748], [0.7396945 , 0.6653172 , 0.0787828 , ..., 0.7117733 , 0.07013571, 0.9409125 ], [0.15861344, 0.12024033, 0.27218235, ..., 0.8824879 , 0.1432488 , 0.44135118]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(5, 300), dtype=float32, numpy= array([[0.68789124, 0.48447883, 0.9309944 , ..., 0.6920762 , 0.33180213, 0.9212563 ], [0.27369928, 0.10631859, 0.6218617 , ..., 0.4382149 , 0.30427706, 0.51477313], [0.00920248, 0.37280262, 0.8177401 , ..., 0.56786287, 0.49201214, 0.9892651 ], [0.88608265, 0.08672249, 0.12160683, ..., 0.91770685, 0.72545695, 0.8280058 ], [0.36690474, 0.9200133 , 0.9646884 , ..., 0.69012 , 0.7137332 , 0.2584542 ]], dtype=float32)&gt;) . Multiply the two tensors you created in 3 using matrix multiplication. | tf.matmul(B1,tf.transpose(B2)) . &lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy= array([[80.33344 , 73.40498 , 77.15962 , 73.98368 , 80.90053 ], [75.146355, 68.80437 , 74.24302 , 71.841835, 75.60206 ], [79.7594 , 75.644554, 77.797585, 74.74873 , 80.559845], [75.085266, 69.06406 , 74.307755, 72.27616 , 76.05668 ], [85.056885, 74.266266, 78.00687 , 74.88678 , 83.13418 ]], dtype=float32)&gt; . tf.matmul(tf.transpose(B1), B2) . &lt;tf.Tensor: shape=(300, 300), dtype=float32, numpy= array([[1.317161 , 0.61993605, 1.2612379 , ..., 1.5290778 , 1.0735596 , 1.6227092 ], [1.0170685 , 0.42642498, 0.8181824 , ..., 1.1469344 , 0.8212255 , 1.1739546 ], [0.45034647, 0.8037954 , 1.4656199 , ..., 1.105671 , 0.88152766, 1.481925 ], ..., [1.3204696 , 1.1634867 , 1.7423928 , ..., 1.8386563 , 1.5207756 , 1.5979093 ], [0.73207504, 0.90400356, 1.8493464 , ..., 1.3821819 , 0.98218614, 1.924531 ], [1.0814031 , 0.5316744 , 0.7174167 , ..., 1.2942287 , 1.0804075 , 1.0476992 ]], dtype=float32)&gt; . Multiply the two tensors you created in 3 using dot product. | tf.tensordot(B1,tf.transpose(B2), axes = 1) . &lt;tf.Tensor: shape=(5, 5), dtype=float32, numpy= array([[80.33344 , 73.40498 , 77.15962 , 73.98368 , 80.90053 ], [75.146355, 68.80437 , 74.24302 , 71.841835, 75.60206 ], [79.7594 , 75.644554, 77.797585, 74.74873 , 80.559845], [75.085266, 69.06406 , 74.307755, 72.27616 , 76.05668 ], [85.056885, 74.266266, 78.00687 , 74.88678 , 83.13418 ]], dtype=float32)&gt; . Create a tensor with random values between 0 and 1 with shape [224, 224, 3]. | tf.random.set_seed(42) randtensor = tf.random.uniform([224,224,3]) randtensor . &lt;tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy= array([[[0.6645621 , 0.44100678, 0.3528825 ], [0.46448255, 0.03366041, 0.68467236], [0.74011743, 0.8724445 , 0.22632635], ..., [0.42612267, 0.09686017, 0.16105258], [0.1487099 , 0.04513884, 0.9497483 ], [0.4393103 , 0.28527975, 0.96971095]], [[0.73308516, 0.5657046 , 0.33238935], [0.8838178 , 0.87544763, 0.56711245], [0.8879347 , 0.47661996, 0.42041814], ..., [0.7716515 , 0.9116473 , 0.3229897 ], [0.43050945, 0.83253574, 0.45549798], [0.29816985, 0.9639522 , 0.3316357 ]], [[0.41132426, 0.2179662 , 0.53570235], [0.5112119 , 0.6484759 , 0.8894886 ], [0.42459428, 0.20189774, 0.85781324], ..., [0.02888799, 0.3995477 , 0.11355484], [0.68524575, 0.04945195, 0.17778492], [0.97627187, 0.79811585, 0.9411576 ]], ..., [[0.9019445 , 0.27011132, 0.8090267 ], [0.32395256, 0.6672456 , 0.940673 ], [0.7166116 , 0.8860713 , 0.6777594 ], ..., [0.8318608 , 0.39227867, 0.68916583], [0.1599741 , 0.46428144, 0.4656595 ], [0.8619243 , 0.24755931, 0.33835268]], [[0.47570062, 0.09377229, 0.11811328], [0.0523994 , 0.38206005, 0.12188685], [0.2757113 , 0.44918692, 0.9179864 ], ..., [0.4974177 , 0.4562863 , 0.8261535 ], [0.60251105, 0.27676368, 0.258716 ], [0.7977431 , 0.74125385, 0.76062095]], [[0.4755299 , 0.4661665 , 0.14167643], [0.9103775 , 0.41117966, 0.83182037], [0.79765654, 0.38330686, 0.5313202 ], ..., [0.94517136, 0.17730081, 0.00362825], [0.6170398 , 0.9977623 , 0.8315122 ], [0.6683676 , 0.68716586, 0.4447713 ]]], dtype=float32)&gt; . Find the min and max values of the tensor you created in 6. | tf.reduce_max(randtensor) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=0.999998&gt; . tf.reduce_min(randtensor) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=3.5762787e-07&gt; . Created a tensor with random values of shape [1, 224, 224, 3] then squeeze it to change the shape to [224, 224, 3]. | tf.random.set_seed(42) for_squeeze = tf.random.uniform([1,224,224,3]) for_squeeze . &lt;tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy= array([[[[0.6645621 , 0.44100678, 0.3528825 ], [0.46448255, 0.03366041, 0.68467236], [0.74011743, 0.8724445 , 0.22632635], ..., [0.42612267, 0.09686017, 0.16105258], [0.1487099 , 0.04513884, 0.9497483 ], [0.4393103 , 0.28527975, 0.96971095]], [[0.73308516, 0.5657046 , 0.33238935], [0.8838178 , 0.87544763, 0.56711245], [0.8879347 , 0.47661996, 0.42041814], ..., [0.7716515 , 0.9116473 , 0.3229897 ], [0.43050945, 0.83253574, 0.45549798], [0.29816985, 0.9639522 , 0.3316357 ]], [[0.41132426, 0.2179662 , 0.53570235], [0.5112119 , 0.6484759 , 0.8894886 ], [0.42459428, 0.20189774, 0.85781324], ..., [0.02888799, 0.3995477 , 0.11355484], [0.68524575, 0.04945195, 0.17778492], [0.97627187, 0.79811585, 0.9411576 ]], ..., [[0.9019445 , 0.27011132, 0.8090267 ], [0.32395256, 0.6672456 , 0.940673 ], [0.7166116 , 0.8860713 , 0.6777594 ], ..., [0.8318608 , 0.39227867, 0.68916583], [0.1599741 , 0.46428144, 0.4656595 ], [0.8619243 , 0.24755931, 0.33835268]], [[0.47570062, 0.09377229, 0.11811328], [0.0523994 , 0.38206005, 0.12188685], [0.2757113 , 0.44918692, 0.9179864 ], ..., [0.4974177 , 0.4562863 , 0.8261535 ], [0.60251105, 0.27676368, 0.258716 ], [0.7977431 , 0.74125385, 0.76062095]], [[0.4755299 , 0.4661665 , 0.14167643], [0.9103775 , 0.41117966, 0.83182037], [0.79765654, 0.38330686, 0.5313202 ], ..., [0.94517136, 0.17730081, 0.00362825], [0.6170398 , 0.9977623 , 0.8315122 ], [0.6683676 , 0.68716586, 0.4447713 ]]]], dtype=float32)&gt; . G_squeezed = tf.squeeze(for_squeeze) G_squeezed, G_squeezed.shape . (&lt;tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy= array([[[0.6645621 , 0.44100678, 0.3528825 ], [0.46448255, 0.03366041, 0.68467236], [0.74011743, 0.8724445 , 0.22632635], ..., [0.42612267, 0.09686017, 0.16105258], [0.1487099 , 0.04513884, 0.9497483 ], [0.4393103 , 0.28527975, 0.96971095]], [[0.73308516, 0.5657046 , 0.33238935], [0.8838178 , 0.87544763, 0.56711245], [0.8879347 , 0.47661996, 0.42041814], ..., [0.7716515 , 0.9116473 , 0.3229897 ], [0.43050945, 0.83253574, 0.45549798], [0.29816985, 0.9639522 , 0.3316357 ]], [[0.41132426, 0.2179662 , 0.53570235], [0.5112119 , 0.6484759 , 0.8894886 ], [0.42459428, 0.20189774, 0.85781324], ..., [0.02888799, 0.3995477 , 0.11355484], [0.68524575, 0.04945195, 0.17778492], [0.97627187, 0.79811585, 0.9411576 ]], ..., [[0.9019445 , 0.27011132, 0.8090267 ], [0.32395256, 0.6672456 , 0.940673 ], [0.7166116 , 0.8860713 , 0.6777594 ], ..., [0.8318608 , 0.39227867, 0.68916583], [0.1599741 , 0.46428144, 0.4656595 ], [0.8619243 , 0.24755931, 0.33835268]], [[0.47570062, 0.09377229, 0.11811328], [0.0523994 , 0.38206005, 0.12188685], [0.2757113 , 0.44918692, 0.9179864 ], ..., [0.4974177 , 0.4562863 , 0.8261535 ], [0.60251105, 0.27676368, 0.258716 ], [0.7977431 , 0.74125385, 0.76062095]], [[0.4755299 , 0.4661665 , 0.14167643], [0.9103775 , 0.41117966, 0.83182037], [0.79765654, 0.38330686, 0.5313202 ], ..., [0.94517136, 0.17730081, 0.00362825], [0.6170398 , 0.9977623 , 0.8315122 ], [0.6683676 , 0.68716586, 0.4447713 ]]], dtype=float32)&gt;, TensorShape([224, 224, 3])) . Create a tensor with shape [10] using your own choice of values, then find the index which has the maximum value. | tf.random.set_seed(42) nine_ans = tf.random.uniform([10], maxval = 10,dtype = tf.int32) nine_ans . &lt;tf.Tensor: shape=(10,), dtype=int32, numpy=array([7, 9, 1, 6, 2, 4, 3, 3, 1, 1], dtype=int32)&gt; . tf.argmax(nine_ans) . &lt;tf.Tensor: shape=(), dtype=int64, numpy=1&gt; . One-hot encode the tensor you created in 9. | tf.one_hot(tf.cast(nine_ans,dtype = tf.int32), depth = 10) . &lt;tf.Tensor: shape=(10, 10), dtype=float32, numpy= array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.], [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.], [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)&gt; . Bibliography: . Tensorflow Daniel Bourke 2021 Youtube | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/tensorflow/machinelearning/2022/02/09/TensorFlow-Fundamentals.html",
            "relUrl": "/tensorflow/machinelearning/2022/02/09/TensorFlow-Fundamentals.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Axis Bank Stock Data Analysis Project Blog Post",
            "content": "AxisBank Stock Data Analysis . The project is based on the dataset I obtained from kaggle. The Analysis I am performing is on the &#39;AXISBANK&#39; stock market data from 2019-2021.AXISBANK is one of the stocks listed in NIFTY50 index. The NIFTY 50 is a benchmark Indian stock market index that represents the weighted average of 50 of the largest Indian companies listed on the National Stock Exchange. It is one of the two main stock indices used in India, the other being the BSE SENSEX. The Analysis is performed on the stock quote data of &quot;AXIS BANK&quot; from the dataset of NIFTY50 Stock Market data obtained from kaggle repo. . Axis Bank Limited, formerly known as UTI Bank (1993–2007), is an Indian banking and financial services company headquartered in Mumbai, Maharashtra.It sells financial services to large and mid-size companies, SMEs and retail businesses. . The bank was founded on 3 December 1993 as UTI Bank, opening its registered office in Ahmedabad and a corporate office in Mumbai. The bank was promoted jointly by the Administrator of the Unit Trust of India (UTI), Life Insurance Corporation of India (LIC), General Insurance Corporation, National Insurance Company, The New India Assurance Company, The Oriental Insurance Corporation and United India Insurance Company. The first branch was inaugurated on 2 April 1994 in Ahmedabad by Manmohan Singh, then finance minister of India I chose this dataset because of the importance of NIFTY50 listed stocks on Indian economy. In most ways the NIFTY50 presents how well the Indian capital markets are doing. . Downloading the Dataset . In this section of the Jupyter notebook we are going to download an interesting data set from kaggle dataset repositories. We are using python library called OpenDatasets for downloading from kaggle. While downloading we are asked for kaggle user id and API token key for accessing the dataset from kaggle. Kaggle is a platform used for obtaining datasets and various other datascience tasks. . !pip install jovian opendatasets --upgrade --quiet . Let&#39;s begin by downloading the data, and listing the files within the dataset. . dataset_url = &#39;https://www.kaggle.com/rohanrao/nifty50-stock-market-data&#39; . import opendatasets as od od.download(dataset_url) . Skipping, found downloaded files in &#34;./nifty50-stock-market-data&#34; (use force=True to force download) . The dataset has been downloaded and extracted. . data_dir = &#39;./nifty50-stock-market-data&#39; . import os os.listdir(data_dir) . [&#39;HINDUNILVR.csv&#39;, &#39;GRASIM.csv&#39;, &#39;DRREDDY.csv&#39;, &#39;CIPLA.csv&#39;, &#39;ICICIBANK.csv&#39;, &#39;HDFC.csv&#39;, &#39;BAJAJ-AUTO.csv&#39;, &#39;RELIANCE.csv&#39;, &#39;WIPRO.csv&#39;, &#39;HCLTECH.csv&#39;, &#39;BPCL.csv&#39;, &#39;TECHM.csv&#39;, &#39;COALINDIA.csv&#39;, &#39;MM.csv&#39;, &#39;HINDALCO.csv&#39;, &#39;TATASTEEL.csv&#39;, &#39;INDUSINDBK.csv&#39;, &#39;HDFCBANK.csv&#39;, &#39;VEDL.csv&#39;, &#39;NESTLEIND.csv&#39;, &#39;LT.csv&#39;, &#39;ONGC.csv&#39;, &#39;UPL.csv&#39;, &#39;ITC.csv&#39;, &#39;BRITANNIA.csv&#39;, &#39;ZEEL.csv&#39;, &#39;BAJAJFINSV.csv&#39;, &#39;EICHERMOT.csv&#39;, &#39;TITAN.csv&#39;, &#39;TATAMOTORS.csv&#39;, &#39;NIFTY50_all.csv&#39;, &#39;IOC.csv&#39;, &#39;stock_metadata.csv&#39;, &#39;INFY.csv&#39;, &#39;POWERGRID.csv&#39;, &#39;MARUTI.csv&#39;, &#39;NTPC.csv&#39;, &#39;HEROMOTOCO.csv&#39;, &#39;SHREECEM.csv&#39;, &#39;ASIANPAINT.csv&#39;, &#39;ULTRACEMCO.csv&#39;, &#39;INFRATEL.csv&#39;, &#39;GAIL.csv&#39;, &#39;BAJFINANCE.csv&#39;, &#39;JSWSTEEL.csv&#39;, &#39;ADANIPORTS.csv&#39;, &#39;AXISBANK.csv&#39;, &#39;SUNPHARMA.csv&#39;, &#39;TCS.csv&#39;, &#39;BHARTIARTL.csv&#39;, &#39;KOTAKBANK.csv&#39;, &#39;SBIN.csv&#39;] . Let us save and upload our work to Jovian before continuing. . project_name = &quot;nifty50-stockmarket-data&quot; # change this (use lowercase letters and hyphens only) . !pip install jovian --upgrade -q . import jovian . jovian.commit(project=project_name) . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Data Preparation and Cleaning . Data Preparation and Cleansing constitutes the first part of the Data Analysis project for any dataset. We do this process inorder to obtain retain valuable data from the data frame, one that is relevant for our analysis. The process is also used to remove erroneous values from the dataset(ex. NaN to 0). After the preparation of data and cleansing, the data can be used for analysis.&lt;/br&gt; In our dataframe we have a lot of non-releavant information, so we are going to drop few columns in the dataframe and fix some of the elements in data frame for better analysis. We are also going to change the Date column into DateTime format which can be further used to group the data by months/year. . import pandas as pd import numpy as np . axis_df= pd.read_csv(data_dir + &quot;/AXISBANK.csv&quot;) . axis_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 5306 entries, 0 to 5305 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 Date 5306 non-null object 1 Symbol 5306 non-null object 2 Series 5306 non-null object 3 Prev Close 5306 non-null float64 4 Open 5306 non-null float64 5 High 5306 non-null float64 6 Low 5306 non-null float64 7 Last 5306 non-null float64 8 Close 5306 non-null float64 9 VWAP 5306 non-null float64 10 Volume 5306 non-null int64 11 Turnover 5306 non-null float64 12 Trades 2456 non-null float64 13 Deliverable Volume 4797 non-null float64 14 %Deliverble 4797 non-null float64 dtypes: float64(11), int64(1), object(3) memory usage: 621.9+ KB . axis_df.describe() . Prev Close Open High Low Last Close VWAP Volume Turnover Trades Deliverable Volume %Deliverble . count 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5306.000000 | 5.306000e+03 | 5.306000e+03 | 2456.000000 | 4.797000e+03 | 4797.000000 | . mean 585.763852 | 586.507388 | 596.476187 | 575.571598 | 585.897399 | 585.893931 | 586.077778 | 4.527938e+06 | 2.739871e+14 | 120602.231678 | 1.990907e+06 | 0.466962 | . std 436.714128 | 436.602194 | 443.044833 | 430.108921 | 436.609147 | 436.649765 | 436.611987 | 8.101940e+06 | 4.122431e+14 | 96106.654046 | 3.264587e+06 | 0.161808 | . min 22.150000 | 21.000000 | 23.700000 | 21.000000 | 22.150000 | 22.150000 | 22.170000 | 2.850000e+03 | 8.275250e+09 | 2698.000000 | 5.809000e+03 | 0.075000 | . 25% 230.950000 | 232.000000 | 235.125000 | 227.075000 | 230.550000 | 230.975000 | 231.115000 | 2.842172e+05 | 5.868745e+12 | 62228.250000 | 2.573130e+05 | 0.347500 | . 50% 519.450000 | 520.100000 | 528.400000 | 512.025000 | 519.425000 | 519.500000 | 519.505000 | 1.656966e+06 | 1.653257e+14 | 93186.500000 | 7.687680e+05 | 0.459800 | . 75% 877.312500 | 880.075000 | 897.987500 | 852.762500 | 877.275000 | 877.312500 | 875.807500 | 5.515245e+06 | 3.456528e+14 | 144973.250000 | 2.652520e+06 | 0.573900 | . max 2023.350000 | 2034.400000 | 2043.050000 | 2002.600000 | 2022.550000 | 2023.350000 | 2020.310000 | 1.205419e+08 | 7.179550e+15 | 990737.000000 | 9.490116e+07 | 0.983000 | . axis_df . Date Symbol Series Prev Close Open High Low Last Close VWAP Volume Turnover Trades Deliverable Volume %Deliverble . 0 2000-01-03 | UTIBANK | EQ | 24.70 | 26.7 | 26.70 | 26.70 | 26.70 | 26.70 | 26.70 | 112100 | 2.993070e+11 | NaN | NaN | NaN | . 1 2000-01-04 | UTIBANK | EQ | 26.70 | 27.0 | 28.70 | 26.50 | 27.00 | 26.85 | 27.24 | 234500 | 6.387275e+11 | NaN | NaN | NaN | . 2 2000-01-05 | UTIBANK | EQ | 26.85 | 26.0 | 27.75 | 25.50 | 26.40 | 26.30 | 26.24 | 170100 | 4.462980e+11 | NaN | NaN | NaN | . 3 2000-01-06 | UTIBANK | EQ | 26.30 | 25.8 | 27.00 | 25.80 | 25.90 | 25.95 | 26.27 | 102100 | 2.681730e+11 | NaN | NaN | NaN | . 4 2000-01-07 | UTIBANK | EQ | 25.95 | 25.0 | 26.00 | 24.25 | 25.00 | 24.80 | 25.04 | 62600 | 1.567220e+11 | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | EQ | 671.35 | 694.0 | 703.80 | 684.50 | 699.50 | 700.45 | 695.33 | 21646184 | 1.505120e+15 | 286480.0 | 5949937.0 | 0.2749 | . 5302 2021-04-27 | AXISBANK | EQ | 700.45 | 691.1 | 703.90 | 684.10 | 700.90 | 699.55 | 692.83 | 46559967 | 3.225830e+15 | 289445.0 | 18080082.0 | 0.3883 | . 5303 2021-04-28 | AXISBANK | EQ | 699.55 | 708.0 | 712.50 | 688.15 | 705.95 | 708.15 | 701.92 | 54060587 | 3.794635e+15 | 507747.0 | 17851331.0 | 0.3302 | . 5304 2021-04-29 | AXISBANK | EQ | 708.15 | 712.0 | 726.90 | 707.00 | 717.10 | 719.40 | 717.41 | 25939327 | 1.860920e+15 | 312079.0 | 7357520.0 | 0.2836 | . 5305 2021-04-30 | AXISBANK | EQ | 719.40 | 705.0 | 729.85 | 705.00 | 711.65 | 714.90 | 719.36 | 23011654 | 1.655365e+15 | 232879.0 | 6786072.0 | 0.2949 | . 5306 rows × 15 columns . axis_df[&#39;Symbol&#39;] = np.where(axis_df[&#39;Symbol&#39;] == &#39;UTIBANK&#39;, &#39;AXISBANK&#39;, axis_df[&#39;Symbol&#39;]) axis_df . Date Symbol Series Prev Close Open High Low Last Close VWAP Volume Turnover Trades Deliverable Volume %Deliverble . 0 2000-01-03 | AXISBANK | EQ | 24.70 | 26.7 | 26.70 | 26.70 | 26.70 | 26.70 | 26.70 | 112100 | 2.993070e+11 | NaN | NaN | NaN | . 1 2000-01-04 | AXISBANK | EQ | 26.70 | 27.0 | 28.70 | 26.50 | 27.00 | 26.85 | 27.24 | 234500 | 6.387275e+11 | NaN | NaN | NaN | . 2 2000-01-05 | AXISBANK | EQ | 26.85 | 26.0 | 27.75 | 25.50 | 26.40 | 26.30 | 26.24 | 170100 | 4.462980e+11 | NaN | NaN | NaN | . 3 2000-01-06 | AXISBANK | EQ | 26.30 | 25.8 | 27.00 | 25.80 | 25.90 | 25.95 | 26.27 | 102100 | 2.681730e+11 | NaN | NaN | NaN | . 4 2000-01-07 | AXISBANK | EQ | 25.95 | 25.0 | 26.00 | 24.25 | 25.00 | 24.80 | 25.04 | 62600 | 1.567220e+11 | NaN | NaN | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | EQ | 671.35 | 694.0 | 703.80 | 684.50 | 699.50 | 700.45 | 695.33 | 21646184 | 1.505120e+15 | 286480.0 | 5949937.0 | 0.2749 | . 5302 2021-04-27 | AXISBANK | EQ | 700.45 | 691.1 | 703.90 | 684.10 | 700.90 | 699.55 | 692.83 | 46559967 | 3.225830e+15 | 289445.0 | 18080082.0 | 0.3883 | . 5303 2021-04-28 | AXISBANK | EQ | 699.55 | 708.0 | 712.50 | 688.15 | 705.95 | 708.15 | 701.92 | 54060587 | 3.794635e+15 | 507747.0 | 17851331.0 | 0.3302 | . 5304 2021-04-29 | AXISBANK | EQ | 708.15 | 712.0 | 726.90 | 707.00 | 717.10 | 719.40 | 717.41 | 25939327 | 1.860920e+15 | 312079.0 | 7357520.0 | 0.2836 | . 5305 2021-04-30 | AXISBANK | EQ | 719.40 | 705.0 | 729.85 | 705.00 | 711.65 | 714.90 | 719.36 | 23011654 | 1.655365e+15 | 232879.0 | 6786072.0 | 0.2949 | . 5306 rows × 15 columns . axis_new_df = axis_df.drop([&#39;Last&#39;,&#39;Series&#39;, &#39;VWAP&#39;, &#39;Trades&#39;,&#39;Deliverable Volume&#39;,&#39;%Deliverble&#39;], axis=1) axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover . 0 2000-01-03 | AXISBANK | 24.70 | 26.7 | 26.70 | 26.70 | 26.70 | 112100 | 2.993070e+11 | . 1 2000-01-04 | AXISBANK | 26.70 | 27.0 | 28.70 | 26.50 | 26.85 | 234500 | 6.387275e+11 | . 2 2000-01-05 | AXISBANK | 26.85 | 26.0 | 27.75 | 25.50 | 26.30 | 170100 | 4.462980e+11 | . 3 2000-01-06 | AXISBANK | 26.30 | 25.8 | 27.00 | 25.80 | 25.95 | 102100 | 2.681730e+11 | . 4 2000-01-07 | AXISBANK | 25.95 | 25.0 | 26.00 | 24.25 | 24.80 | 62600 | 1.567220e+11 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | . 5302 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | . 5303 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | . 5304 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | . 5305 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | . 5306 rows × 9 columns . def getIndexes(dfObj, value): &#39;&#39;&#39; Get index positions of value in dataframe i.e. dfObj.&#39;&#39;&#39; listOfPos = list() # Get bool dataframe with True at positions where the given value exists result = dfObj.isin([value]) # Get list of columns that contains the value seriesObj = result.any() columnNames = list(seriesObj[seriesObj == True].index) # Iterate over list of columns and fetch the rows indexes where value exists for col in columnNames: rows = list(result[col][result[col] == True].index) for row in rows: listOfPos.append((row, col)) # Return a list of tuples indicating the positions of value in the dataframe return listOfPos . listOfPosition_axis = getIndexes(axis_df, &#39;2019-01-01&#39;) listOfPosition_axis . [(4729, &#39;Date&#39;)] . axis_new_df.drop(axis_new_df.loc[0:4728].index, inplace = True) . axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover . 4729 2019-01-01 | AXISBANK | 619.90 | 621.9 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | . 4730 2019-01-02 | AXISBANK | 627.30 | 623.0 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | . 4731 2019-01-03 | AXISBANK | 620.05 | 621.4 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | . 4732 2019-01-04 | AXISBANK | 607.95 | 612.0 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | . 4733 2019-01-07 | AXISBANK | 619.60 | 626.0 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 5301 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | . 5302 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | . 5303 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | . 5304 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | . 5305 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | . 577 rows × 9 columns . Summary of the operations done till now: . we have taken a csv file containing stock data of AXIS BANK from the data set of nifty50 stocks and performed data cleansing operations on them.&lt;/br&gt; | Originally, the data from the data set is noticed as stock price quotations from the year 2001 but for our analysis we have taken data for the years 2019-2021&lt;/br&gt; | Then we have dropped the columns that are not relevant for our analysis by using pandas dataframe operations. | axis_new_df.reset_index(drop=True, inplace=True) axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover . 0 2019-01-01 | AXISBANK | 619.90 | 621.9 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | . 1 2019-01-02 | AXISBANK | 627.30 | 623.0 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | . 2 2019-01-03 | AXISBANK | 620.05 | 621.4 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | . 3 2019-01-04 | AXISBANK | 607.95 | 612.0 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | . 4 2019-01-07 | AXISBANK | 619.60 | 626.0 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | . 572 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | . 573 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | . 574 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | . 575 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | . 576 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | . 577 rows × 9 columns . axis_new_df[&#39;Date&#39;] = pd.to_datetime(axis_new_df[&#39;Date&#39;]) # we changed the Dates into Datetime format from the object format axis_new_df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 577 entries, 0 to 576 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 Date 577 non-null datetime64[ns] 1 Symbol 577 non-null object 2 Prev Close 577 non-null float64 3 Open 577 non-null float64 4 High 577 non-null float64 5 Low 577 non-null float64 6 Close 577 non-null float64 7 Volume 577 non-null int64 8 Turnover 577 non-null float64 dtypes: datetime64[ns](1), float64(6), int64(1), object(1) memory usage: 40.7+ KB . axis_new_df[&#39;Daily Lag&#39;] = axis_new_df[&#39;Close&#39;].shift(1) # Added a new column Daily Lag to calculate daily returns of the stock axis_new_df[&#39;Daily Returns&#39;] = (axis_new_df[&#39;Daily Lag&#39;]/axis_new_df[&#39;Close&#39;]) -1 . axis_dailyret_df = axis_new_df.drop([&#39;Prev Close&#39;, &#39;Open&#39;,&#39;High&#39;, &#39;Low&#39;,&#39;Close&#39;,&#39;Daily Lag&#39;], axis = 1) . axis_dailyret_df . Date Symbol Volume Turnover Daily Returns . 0 2019-01-01 | AXISBANK | 12179223 | 7.609460e+14 | NaN | . 1 2019-01-02 | AXISBANK | 12386281 | 7.720546e+14 | 0.011693 | . 2 2019-01-03 | AXISBANK | 13228602 | 8.092834e+14 | 0.019903 | . 3 2019-01-04 | AXISBANK | 8381367 | 5.178678e+14 | -0.018802 | . 4 2019-01-07 | AXISBANK | 11735286 | 7.463059e+14 | -0.028002 | . ... ... | ... | ... | ... | ... | . 572 2021-04-26 | AXISBANK | 21646184 | 1.505120e+15 | -0.041545 | . 573 2021-04-27 | AXISBANK | 46559967 | 3.225830e+15 | 0.001287 | . 574 2021-04-28 | AXISBANK | 54060587 | 3.794635e+15 | -0.012144 | . 575 2021-04-29 | AXISBANK | 25939327 | 1.860920e+15 | -0.015638 | . 576 2021-04-30 | AXISBANK | 23011654 | 1.655365e+15 | 0.006295 | . 577 rows × 5 columns . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Exploratory Analysis and Visualization . Here we compute the mean, max/min stock quotes of the stock AXISBANK. We specifically compute the mean of the Daily returns column. we are going to do the analysis by first converting the index datewise to month wise to have a good consolidated dataframe to analyze in broad timeline. we are going to divide the data frame into three for the years 2019, 2020, 2021 respectively, in order to analyze the yearly performance of the stock. . Let&#39;s begin by importingmatplotlib.pyplot and seaborn. . import seaborn as sns import matplotlib import matplotlib.pyplot as plt %matplotlib inline sns.set_style(&#39;darkgrid&#39;) matplotlib.rcParams[&#39;font.size&#39;] = 10 matplotlib.rcParams[&#39;figure.figsize&#39;] = (15, 5) matplotlib.rcParams[&#39;figure.facecolor&#39;] = &#39;#00000000&#39; . Here we are going to explore the daily Returns column by plotting a line graph of daily returns v/s Months. Now we can see that daily returns are growing across months in the years 2019-2021. . axis_dailyret_plot=axis_dailyret_df.groupby(axis_dailyret_df[&#39;Date&#39;].dt.strftime(&#39;%B&#39;))[&#39;Daily Returns&#39;].sum().sort_values() plt.plot(axis_dailyret_plot) . [&lt;matplotlib.lines.Line2D at 0x7fd4abea4400&gt;] . axis_new_df[&#39;Year&#39;] = pd.DatetimeIndex(axis_new_df[&#39;Date&#39;]).year axis_new_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2019-01-01 | AXISBANK | 619.90 | 621.9 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | NaN | NaN | 2019 | . 1 2019-01-02 | AXISBANK | 627.30 | 623.0 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | 627.30 | 0.011693 | 2019 | . 2 2019-01-03 | AXISBANK | 620.05 | 621.4 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | 620.05 | 0.019903 | 2019 | . 3 2019-01-04 | AXISBANK | 607.95 | 612.0 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | 607.95 | -0.018802 | 2019 | . 4 2019-01-07 | AXISBANK | 619.60 | 626.0 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | 619.60 | -0.028002 | 2019 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 572 2021-04-26 | AXISBANK | 671.35 | 694.0 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | 671.35 | -0.041545 | 2021 | . 573 2021-04-27 | AXISBANK | 700.45 | 691.1 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | 700.45 | 0.001287 | 2021 | . 574 2021-04-28 | AXISBANK | 699.55 | 708.0 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | 699.55 | -0.012144 | 2021 | . 575 2021-04-29 | AXISBANK | 708.15 | 712.0 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | 708.15 | -0.015638 | 2021 | . 576 2021-04-30 | AXISBANK | 719.40 | 705.0 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | 719.40 | 0.006295 | 2021 | . 577 rows × 12 columns . axis2019_df = axis_new_df[axis_new_df.Year == 2019 ] axis2020_df = axis_new_df[axis_new_df.Year == 2020 ] axis2021_df = axis_new_df[axis_new_df.Year == 2021 ] . axis2019_df.reset_index(drop = True, inplace = True) axis2019_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2019-01-01 | AXISBANK | 619.90 | 621.90 | 630.20 | 621.90 | 627.30 | 12179223 | 7.609460e+14 | NaN | NaN | 2019 | . 1 2019-01-02 | AXISBANK | 627.30 | 623.00 | 628.50 | 617.50 | 620.05 | 12386281 | 7.720546e+14 | 627.30 | 0.011693 | 2019 | . 2 2019-01-03 | AXISBANK | 620.05 | 621.40 | 622.00 | 603.65 | 607.95 | 13228602 | 8.092834e+14 | 620.05 | 0.019903 | 2019 | . 3 2019-01-04 | AXISBANK | 607.95 | 612.00 | 624.75 | 609.50 | 619.60 | 8381367 | 5.178678e+14 | 607.95 | -0.018802 | 2019 | . 4 2019-01-07 | AXISBANK | 619.60 | 626.00 | 640.70 | 624.20 | 637.45 | 11735286 | 7.463059e+14 | 619.60 | -0.028002 | 2019 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 240 2019-12-24 | AXISBANK | 743.15 | 744.50 | 744.70 | 737.80 | 740.65 | 3642916 | 2.698164e+14 | 743.15 | 0.003375 | 2019 | . 241 2019-12-26 | AXISBANK | 740.65 | 737.50 | 740.65 | 733.90 | 736.50 | 7919368 | 5.836254e+14 | 740.65 | 0.005635 | 2019 | . 242 2019-12-27 | AXISBANK | 736.50 | 739.00 | 762.00 | 736.30 | 760.15 | 10736285 | 8.089429e+14 | 736.50 | -0.031112 | 2019 | . 243 2019-12-30 | AXISBANK | 760.15 | 760.90 | 765.75 | 751.05 | 754.10 | 10034206 | 7.598128e+14 | 760.15 | 0.008023 | 2019 | . 244 2019-12-31 | AXISBANK | 754.10 | 753.85 | 765.85 | 751.40 | 754.10 | 11660781 | 8.842549e+14 | 754.10 | 0.000000 | 2019 | . 245 rows × 12 columns . axis2020_df.reset_index(drop = True, inplace = True) axis2020_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2020-01-01 | AXISBANK | 754.10 | 754.90 | 759.95 | 747.20 | 748.70 | 4917748 | 3.697221e+14 | 754.10 | 0.007213 | 2020 | . 1 2020-01-02 | AXISBANK | 748.70 | 750.00 | 759.00 | 747.60 | 756.95 | 5156046 | 3.886739e+14 | 748.70 | -0.010899 | 2020 | . 2 2020-01-03 | AXISBANK | 756.95 | 753.15 | 756.25 | 740.50 | 742.95 | 8489729 | 6.336165e+14 | 756.95 | 0.018844 | 2020 | . 3 2020-01-06 | AXISBANK | 742.95 | 739.45 | 739.60 | 721.70 | 723.25 | 6356198 | 4.631502e+14 | 742.95 | 0.027238 | 2020 | . 4 2020-01-07 | AXISBANK | 723.25 | 728.00 | 738.00 | 721.05 | 725.75 | 9103360 | 6.630907e+14 | 723.25 | -0.003445 | 2020 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 247 2020-12-24 | AXISBANK | 592.45 | 595.00 | 614.00 | 594.15 | 610.20 | 15488083 | 9.422775e+14 | 592.45 | -0.029089 | 2020 | . 248 2020-12-28 | AXISBANK | 610.20 | 614.00 | 620.80 | 614.00 | 617.65 | 8444506 | 5.220025e+14 | 610.20 | -0.012062 | 2020 | . 249 2020-12-29 | AXISBANK | 617.65 | 620.70 | 632.40 | 618.60 | 630.20 | 13765454 | 8.632919e+14 | 617.65 | -0.019914 | 2020 | . 250 2020-12-30 | AXISBANK | 630.20 | 632.25 | 634.00 | 618.20 | 625.10 | 10262221 | 6.407812e+14 | 630.20 | 0.008159 | 2020 | . 251 2020-12-31 | AXISBANK | 625.10 | 622.40 | 625.95 | 616.00 | 620.45 | 12306502 | 7.636264e+14 | 625.10 | 0.007495 | 2020 | . 252 rows × 12 columns . axis2021_df.reset_index(drop=True, inplace=True) axis2021_df . Date Symbol Prev Close Open High Low Close Volume Turnover Daily Lag Daily Returns Year . 0 2021-01-01 | AXISBANK | 620.45 | 620.25 | 625.45 | 617.55 | 623.80 | 6047062 | 3.762202e+14 | 620.45 | -0.005370 | 2021 | . 1 2021-01-04 | AXISBANK | 623.80 | 627.80 | 633.00 | 621.30 | 624.70 | 14068156 | 8.811802e+14 | 623.80 | -0.001441 | 2021 | . 2 2021-01-05 | AXISBANK | 624.70 | 618.00 | 667.90 | 618.00 | 664.45 | 37973963 | 2.467637e+15 | 624.70 | -0.059824 | 2021 | . 3 2021-01-06 | AXISBANK | 664.45 | 662.00 | 667.15 | 649.15 | 654.25 | 20829645 | 1.373394e+15 | 664.45 | 0.015590 | 2021 | . 4 2021-01-07 | AXISBANK | 654.25 | 659.00 | 676.50 | 659.00 | 671.10 | 17887570 | 1.196294e+15 | 654.25 | -0.025108 | 2021 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 75 2021-04-26 | AXISBANK | 671.35 | 694.00 | 703.80 | 684.50 | 700.45 | 21646184 | 1.505120e+15 | 671.35 | -0.041545 | 2021 | . 76 2021-04-27 | AXISBANK | 700.45 | 691.10 | 703.90 | 684.10 | 699.55 | 46559967 | 3.225830e+15 | 700.45 | 0.001287 | 2021 | . 77 2021-04-28 | AXISBANK | 699.55 | 708.00 | 712.50 | 688.15 | 708.15 | 54060587 | 3.794635e+15 | 699.55 | -0.012144 | 2021 | . 78 2021-04-29 | AXISBANK | 708.15 | 712.00 | 726.90 | 707.00 | 719.40 | 25939327 | 1.860920e+15 | 708.15 | -0.015638 | 2021 | . 79 2021-04-30 | AXISBANK | 719.40 | 705.00 | 729.85 | 705.00 | 714.90 | 23011654 | 1.655365e+15 | 719.40 | 0.006295 | 2021 | . 80 rows × 12 columns . Summary of above exploratory Analysis: . In the above code cells, we performed plotting of the data by exploring a column from the data. We have divided the DataFrame into three data frames containing the stock quote data from year-wise i.e., for the years 2019, 2020, 2021. For dividing the DataFrame year-wise we have added a new column called &#39;Year&#39; which is generated from the DataTime values of the column &quot;Date&quot;. . axis_range_df = axis_dailyret_df[&#39;Daily Returns&#39;].max() - axis_dailyret_df[&#39;Daily Returns&#39;].min() axis_range_df . 0.55021480949789 . axis_mean_df = axis_dailyret_df[&#39;Daily Returns&#39;].mean() axis_mean_df . 0.0002473086431186587 . In the above two code cells, we have computed the range i.e. the difference between maximum and minimum value of the column. We have also calculated the mean of the daily returns of the Axis Bank stock. . Exploratory Analysis of stock quotes year-wise for Axis Bank: . In this section we have plotted the Closing values of the stock throughout the year for the years 2019,2020,2021. We have only partial data for 2021(i.e. till Apr 2021). We have also done a plot to compare the performance throughout the year for the years 2019 and 2020(since we had full data for the respective years). . plt.plot(axis2019_df[&#39;Date&#39;],axis2019_df[&#39;Close&#39;] ) plt.title(&#39;Closing Values of stock for the year 2019&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . plt.plot(axis2020_df[&#39;Date&#39;],axis2020_df[&#39;Close&#39;]) plt.title(&#39;Closing Values of stock for the year 2020&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . plt.plot(axis2021_df[&#39;Date&#39;],axis2021_df[&#39;Close&#39;]) plt.title(&#39;Closing Values of stock for the year 2021 Till April Month&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . TODO - Explore one or more columns by plotting a graph below, and add some explanation about it . plt.style.use(&#39;fivethirtyeight&#39;) plt.plot(axis2019_df[&#39;Date&#39;], axis2019_df[&#39;Close&#39;],linewidth=3, label = &#39;2019&#39;) plt.plot(axis2020_df[&quot;Date&quot;],axis2020_df[&#39;Close&#39;],linewidth=3, label = &#39;2020&#39;) plt.legend(loc=&#39;best&#39; ) plt.title(&#39;Closing Values of stock for the years 2019 and 2020&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . print(plt.style.available) . [&#39;Solarize_Light2&#39;, &#39;_classic_test_patch&#39;, &#39;bmh&#39;, &#39;classic&#39;, &#39;dark_background&#39;, &#39;fast&#39;, &#39;fivethirtyeight&#39;, &#39;ggplot&#39;, &#39;grayscale&#39;, &#39;seaborn&#39;, &#39;seaborn-bright&#39;, &#39;seaborn-colorblind&#39;, &#39;seaborn-dark&#39;, &#39;seaborn-dark-palette&#39;, &#39;seaborn-darkgrid&#39;, &#39;seaborn-deep&#39;, &#39;seaborn-muted&#39;, &#39;seaborn-notebook&#39;, &#39;seaborn-paper&#39;, &#39;seaborn-pastel&#39;, &#39;seaborn-poster&#39;, &#39;seaborn-talk&#39;, &#39;seaborn-ticks&#39;, &#39;seaborn-white&#39;, &#39;seaborn-whitegrid&#39;, &#39;tableau-colorblind10&#39;] . Let us save and upload our work to Jovian before continuing . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Asking and Answering Questions . In this section, we are going to answer some of the questions regarding the dataset using various data analysis libraries like Numpy, Pandas, Matplotlib and seaborn. By using the tools we can see how useful the libraries come in handy while doing Inference on a dataset. . Instructions (delete this cell) . Ask at least 5 interesting questions about your dataset | Answer the questions either by computing the results using Numpy/Pandas or by plotting graphs using Matplotlib/Seaborn | Create new columns, merge multiple dataset and perform grouping/aggregation wherever necessary | Wherever you&#39;re using a library function from Pandas/Numpy/Matplotlib etc. explain briefly what it does | . Q1: What was the change in price and volume of the stock traded overtime? . plt.plot(axis2019_df[&#39;Date&#39;], axis2019_df[&#39;Close&#39;],linewidth=3, label = &#39;2019&#39;) plt.plot(axis2020_df[&quot;Date&quot;],axis2020_df[&#39;Close&#39;],linewidth=3, label = &#39;2020&#39;) plt.plot(axis2021_df[&quot;Date&quot;], axis2021_df[&#39;Close&#39;],linewidth = 3, label = &#39;2021&#39;) plt.legend(loc=&#39;best&#39; ) plt.title(&#39;Closing Price of stock for the years 2019-2021(Till April)&#39;) plt.xlabel(None) plt.ylabel(&#39;Closing price of the stock&#39;) . Text(0, 0.5, &#39;Closing price of the stock&#39;) . print(&#39;The Maximum closing price of the stock during 2019-2021 is&#39;,axis_new_df[&#39;Close&#39;].max()) print(&#39;The Minimum closing price of the stock during 2019-2021 is&#39;,axis_new_df[&#39;Close&#39;].min()) print(&#39;The Index for the Maximum closing price in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Close&#39;].max())) print(&#39;The Index for the Minimum closing price in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Close&#39;].min())) print(axis_new_df.iloc[104]) print(axis_new_df.iloc[303]) . The Maximum closing price of the stock during 2019-2021 is 822.8 The Minimum closing price of the stock during 2019-2021 is 303.15 The Index for the Maximum closing price in the dataframe is [(105, &#39;Prev Close&#39;), (104, &#39;Close&#39;), (105, &#39;Daily Lag&#39;)] The Index for the Minimum closing price in the dataframe is [(304, &#39;Prev Close&#39;), (303, &#39;Close&#39;), (304, &#39;Daily Lag&#39;)] Date 2019-06-04 00:00:00 Symbol AXISBANK Prev Close 812.65 Open 807.55 High 827.75 Low 805.5 Close 822.8 Volume 9515354 Turnover 778700415970000.0 Daily Lag 812.65 Daily Returns -0.012336 Year 2019 Name: 104, dtype: object Date 2020-03-24 00:00:00 Symbol AXISBANK Prev Close 308.65 Open 331.95 High 337.5 Low 291.0 Close 303.15 Volume 50683611 Turnover 1578313503950000.0 Daily Lag 308.65 Daily Returns 0.018143 Year 2020 Name: 303, dtype: object . As we can see from the above one of the two plots there was a dip in the closing price during the year 2020. The Maximum Closing price occurred on 2019-06-04(Close = 822.8). The lowest of closing price during the years occurred on 2020-03-24(Close = 303.15). This can say that the start of the pandemic has caused the steep down curve for the stock&#39;s closing price. | . plt.plot(axis2019_df[&quot;Date&quot;],axis2019_df[&quot;Volume&quot;],linewidth=2, label = &#39;2019&#39;) plt.plot(axis2020_df[&quot;Date&quot;],axis2020_df[&quot;Volume&quot;],linewidth=2, label = &#39;2020&#39;) plt.plot(axis2021_df[&quot;Date&quot;],axis2021_df[&quot;Volume&quot;],linewidth=2, label = &#39;2021&#39;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Volume of stock traded in the years 2019-2021(till April)&#39;) plt.ylabel(&#39;Volume&#39;) plt.xlabel(None) . Text(0.5, 0, &#39;&#39;) . print(&#39;The Maximum volume of the stock traded during 2019-2021 is&#39;,axis_new_df[&#39;Volume&#39;].max()) print(&#39;The Minimum volume of the stock traded during 2019-2021 is&#39;,axis_new_df[&#39;Volume&#39;].min()) print(&#39;The Index for the Maximum volume stock traded in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Volume&#39;].max())) print(&#39;The Index for the Minimum volume stock traded in the dataframe is&#39;,getIndexes(axis_new_df, axis_new_df[&#39;Volume&#39;].min())) print(axis_new_df.iloc[357]) print(axis_new_df.iloc[200]) . The Maximum volume of the stock traded during 2019-2021 is 96190274 The Minimum volume of the stock traded during 2019-2021 is 965772 The Index for the Maximum volume stock traded in the dataframe is [(357, &#39;Volume&#39;)] The Index for the Minimum volume stock traded in the dataframe is [(200, &#39;Volume&#39;)] Date 2020-06-16 00:00:00 Symbol AXISBANK Prev Close 389.6 Open 404.9 High 405.0 Low 360.4 Close 381.55 Volume 96190274 Turnover 3654065942305001.0 Daily Lag 389.6 Daily Returns 0.021098 Year 2020 Name: 357, dtype: object Date 2019-10-27 00:00:00 Symbol AXISBANK Prev Close 708.6 Open 711.0 High 715.05 Low 708.55 Close 710.1 Volume 965772 Turnover 68696126654999.992188 Daily Lag 708.6 Daily Returns -0.002112 Year 2019 Name: 200, dtype: object . As we can see from the above graph a lot of volume of trade happened during 2020. That means the stock was transacted a lot during the year 2020. The highest Volumed of stock is traded on 2020-06-16(Volume =96190274) and the Minimum volume of the stock traded during 2019-2021 is on 2019-10-27(Volume = 965772) . Q2: What was the daily return of the stock on average? . The daily return measures the price change in a stock&#39;s price as a percentage of the previous day&#39;s closing price. A positive return means the stock has grown in value, while a negative return means it has lost value. we will also attempt to calculate the maximum daily return of the stock during 2019-2021. . plt.plot(axis_new_df[&#39;Date&#39;],axis_new_df[&#39;Daily Returns&#39;], linewidth=2 ,label = &#39;Daily Returns&#39;) plt.legend(loc=&#39;best&#39; ) plt.title(&#39;Daily Returns of stock for the years 2019-2021(Till April)&#39;) plt.xlabel(None) plt.ylabel(&#39;Daily Returns of the stock&#39;) . Text(0, 0.5, &#39;Daily Returns of the stock&#39;) . plt.plot(axis_new_df[&#39;Date&#39;],axis_new_df[&#39;Daily Returns&#39;], linestyle=&#39;--&#39;, marker=&#39;o&#39;) plt.title(&#39;Daily Returns of stock for the years 2019-2021(Till April)&#39;) plt.xlabel(None) plt.ylabel(&#39;Daily Returns of the stock&#39;) . Text(0, 0.5, &#39;Daily Returns of the stock&#39;) . print(&#39;The Maximum daily return during the years 2020 is&#39;,axis_new_df[&#39;Daily Returns&#39;].max()) index = getIndexes(axis_new_df, axis_new_df[&#39;Daily Returns&#39;].max()) axis_new_df.iloc[302] . The Maximum daily return during the years 2020 is 0.3871699335817269 . Date 2020-03-23 00:00:00 Symbol AXISBANK Prev Close 428.15 Open 385.35 High 392.0 Low 302.0 Close 308.65 Volume 37622791 Turnover 1253563689110000.0 Daily Lag 428.15 Daily Returns 0.38717 Year 2020 Name: 302, dtype: object . def getIndexes(dfObj, value): &#39;&#39;&#39; Get index positions of value in dataframe i.e. dfObj.&#39;&#39;&#39; listOfPos = list() # Get bool dataframe with True at positions where the given value exists result = dfObj.isin([value]) # Get list of columns that contains the value seriesObj = result.any() columnNames = list(seriesObj[seriesObj == True].index) # Iterate over list of columns and fetch the rows indexes where value exists for col in columnNames: rows = list(result[col][result[col] == True].index) for row in rows: listOfPos.append((row, col)) # Return a list of tuples indicating the positions of value in the dataframe return listOfPos . As we can see from the plot there were high daily returns for the stock around late March 2020 and then there was ups and downs from April- July 2020 . we can see that the most changes in daily returns occurred during April 2020 - July 2020 and at other times the daily returns were almost flat. The maximum daily returns for the stock during 2019-2021 occurred on 2020-03-23(observed from the pandas table above). . Avgdailyret_2019 =axis2019_df[&#39;Daily Returns&#39;].sum()/len(axis2019_df[&#39;Daily Returns&#39;]) Avgdailyret_2020 =axis2020_df[&#39;Daily Returns&#39;].sum()/len(axis2020_df[&#39;Daily Returns&#39;]) Avgdailyret_2021 =axis2021_df[&#39;Daily Returns&#39;].sum()/len(axis2021_df[&#39;Daily Returns&#39;]) # create a dataset data_dailyret = {&#39;2019&#39;: Avgdailyret_2019, &#39;2020&#39;:Avgdailyret_2020, &#39;2021&#39;:Avgdailyret_2021} Years = list(data_dailyret.keys()) Avgdailyret = list(data_dailyret.values()) # plotting a bar chart plt.figure(figsize=(10, 7)) plt.bar(Years, Avgdailyret, color =&#39;maroon&#39;,width = 0.3) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Average Daily Returns of the Stock Traded&quot;) plt.title(&quot;Average Daily Returns of the Stock over the years 2019-2021(Till April) (in 10^7)&quot;) plt.show() . plt.figure(figsize=(12, 7)) sns.distplot(axis_new_df[&#39;Daily Returns&#39;].dropna(), bins=100, color=&#39;purple&#39;) plt.title(&#39; Histogram of Daily Returns&#39;) plt.tight_layout() . /opt/conda/lib/python3.9/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . Q3: What is the Average Trading volume of the stock for past three years? . Avgvol_2019 =axis2019_df[&#39;Volume&#39;].sum()/len(axis2019_df[&#39;Volume&#39;]) Avgvol_2020 =axis2020_df[&#39;Volume&#39;].sum()/len(axis2020_df[&#39;Volume&#39;]) Avgvol_2021 =axis2021_df[&#39;Volume&#39;].sum()/len(axis2021_df[&#39;Volume&#39;]) # create a dataset data_volume = {&#39;2019&#39;: Avgvol_2019, &#39;2020&#39;:Avgvol_2020, &#39;2021&#39;:Avgvol_2021} Years = list(data_volume.keys()) AvgVol = list(data_volume.values()) # plotting a bar chart plt.figure(figsize=(13, 7)) plt.bar(Years, AvgVol, color =&#39;maroon&#39;,width = 0.3) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Average Volume of the Stock Traded&quot;) plt.title(&quot;Average Trading volume of the Stock over the years 2019-2021(Till April) (in 10^7)&quot;) plt.show() . From the above plot we can say that more volume of the Axis Bank stock is traded during the year 2020. We can see a significant rise in the trading volume of the stock from 2019 to 2020. . Q4: What is the Average Closing price of the stock for past three years? . Avgclose_2019 =axis2019_df[&#39;Close&#39;].sum()/len(axis2019_df[&#39;Close&#39;]) Avgclose_2020 =axis2020_df[&#39;Close&#39;].sum()/len(axis2020_df[&#39;Close&#39;]) Avgclose_2021 =axis2021_df[&#39;Close&#39;].sum()/len(axis2021_df[&#39;Close&#39;]) # create a dataset data_volume = {&#39;2019&#39;: Avgclose_2019, &#39;2020&#39;:Avgclose_2020, &#39;2021&#39;:Avgclose_2021} Years = list(data_volume.keys()) AvgClose = list(data_volume.values()) # plotting a bar chart plt.figure(figsize=(13, 7)) plt.bar(Years, AvgClose, color =&#39;maroon&#39;,width = 0.3) plt.xlabel(&quot;Years&quot;) plt.ylabel(&quot;Average Closding Price of the Stock Traded&quot;) plt.title(&quot;Average Closing price of the Stock over the years 2019-2021(Till April) (in 10^7)&quot;) plt.show() . We have seen the Trading Volume of the stock is more during the year 2020. In contrast, the Year 2020 has the lowest average closing price among the other two. But for the years 2019 and 2021 the Average closing price is almost same, there is not much change in the value. . Let us save and upload our work to Jovian before continuing. . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . Inferences and Conclusion . Inferences : The above data analysis is done on the data set of stock quotes for AXIS BANK during the years 2019-2021. From the Analysis we can say that during the year 2020 there has been a lot of unsteady growth, there has been rise in the volume of stock traded on the exchange, that means there has been a lot of transactions of the stock. The stock has seen a swift traffic in buy/sell during the year 2020 and has fallen back to normal in the year 2021. In contrast to the volume of the stock the closing price of the stock has decreased during the year 2020, which can be concluded as the volume of the stock traded has no relation to the price change of the stock(while most people think there can be a correlation among the two values). The price decrease for the stock may have been due to the pandemic rise in India during the year 2020. . import jovian . jovian.commit() . [jovian] Updating notebook &#34;sandeshkatakam/axisbank-stockmarket-data-analysis&#34; on https://jovian.ai [jovian] Committed successfully! https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis . &#39;https://jovian.ai/sandeshkatakam/axisbank-stockmarket-data-analysis&#39; . References and Future Work . Future Ideas for the Analyis: . I am planning to go forward with this basic Analysis of the AXISBANK stock quotes and build a Machine Learning model predicting the future stock prices. | I plan to automate the Data Analysis process for every stock in the NIFTY50 Index by defining reusable functions and automating the Analysis procedures. | Study more strong correlations between the different quotes of the stock and analyze how and why they are related in that fashion. | . REFRENCES/LINKS USED FOR THIS PROJECT : . https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html | https://stackoverflow.com/questions/16683701/in-pandas-how-to-get-the-index-of-a-known-value | https://towardsdatascience.com/working-with-datetime-in-pandas-dataframe-663f7af6c587 | https://thispointer.com/python-find-indexes-of-an-element-in-pandas-dataframe/ | https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html#timeseries-friendly-merging | https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html | https://towardsdatascience.com/financial-analytics-exploratory-data-analysis-of-stock-data-d98cbadf98b9 | https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.transpose.html | https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html | https://pandas.pydata.org/docs/reference/api/pandas.merge.html | https://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe | https://www.interviewqs.com/ddi-code-snippets/extract-month-year-pandas | https://stackoverflow.com/questions/18172851/deleting-dataframe-row-in-pandas-based-on-column-value | https://queirozf.com/entries/matplotlib-examples-displaying-and-configuring-legends | https://jakevdp.github.io/PythonDataScienceHandbook/04.06-customizing-legends.html | https://matplotlib.org/stable/tutorials/intermediate/legend_guide.html | https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/subplots_demo.html | https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html | https://stackoverflow.com/questions/332289/how-do-you-change-the-size-of-figures-drawn-with-matplotlib | https://www.investopedia.com/articles/investing/093014/stock-quotes-explained.asp | https://stackoverflow.com/questions/44908383/how-can-i-group-by-month-from-a-datefield-using-python-pandas | https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html | https://note.nkmk.me/en/python-pandas-dataframe-rename/ | https://stackoverflow.com/questions/24748848/pandas-find-the-maximum-range-in-all-the-columns-of-dataframe | https://stackoverflow.com/questions/29233283/plotting-multiple-lines-in-different-colors-with-pandas-dataframe | https://jakevdp.github.io/PythonDataScienceHandbook/04.14-visualization-with-seaborn.html | https://www.geeksforgeeks.org/python-pandas-extracting-rows-using-loc/ | . import jovian . jovian.commit() .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2022/02/04/data-analysis-course-project.html",
            "relUrl": "/jupyter/2022/02/04/data-analysis-course-project.html",
            "date": " • Feb 4, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Logistic Regression with a Neural Network Mindset",
            "content": "Logistic Regression with a Neural Network mindset . This notebook demonstrates, how to build a logistic regression classifier to recognize cats. This notebook will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning. This notebook is a modified version of the assignment I had done for the course: Neural Netoworks and Deep learning . Instructions: . Use np.dot(X,Y) to calculate dot products. | . Roadmap: . Build the general architecture of a learning algorithm, including: Initializing parameters | Calculating the cost function and its gradient | Using an optimization algorithm (gradient descent) | . | Gather all three functions above into a main model function, in the right order. | . . 1 - Packages . First, let&#39;s run the cell below to import all the packages that you will need during this assignment. . numpy is the fundamental package for scientific computing with Python. | h5py is a common package to interact with a dataset that is stored on an H5 file. | matplotlib is a famous library to plot graphs in Python. | PIL and scipy are used here to test your model with your own picture at the end. | . import numpy as np import copy import matplotlib.pyplot as plt import h5py import scipy from PIL import Image from scipy import ndimage from lr_utils import load_dataset from public_tests import * %matplotlib inline %load_ext autoreload %autoreload 2 . . 2 - Overview . Problem Statement: Given a dataset (&quot;data.h5&quot;) containing: . - a training set of m_train images labeled as cat (y=1) or non-cat (y=0) - a test set of m_test images labeled as cat or non-cat - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). . build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. . Let&#39;s get more familiar with the dataset. Load the data by running the following code. . train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() . NameError Traceback (most recent call last) ~ AppData Local Temp/ipykernel_20868/377641723.py in &lt;module&gt; 1 # Loading the data (cat/non-cat) -&gt; 2 train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset() NameError: name &#39;load_dataset&#39; is not defined . We added &quot;_orig&quot; at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don&#39;t need any preprocessing). . Each line of our train_set_x_orig and test_set_x_orig is an array representing an image. We can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images. . index = 25 plt.imshow(train_set_x_orig[index]) print (&quot;y = &quot; + str(train_set_y[:, index]) + &quot;, it&#39;s a &#39;&quot; + classes[np.squeeze(train_set_y[:, index])].decode(&quot;utf-8&quot;) + &quot;&#39; picture.&quot;) . Many software bugs in deep learning come from having matrix/vector dimensions that don&#39;t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. . . Exercise 1 . Find the values for: . - m_train (number of training examples) - m_test (number of test examples) - num_px (= height = width of a training image) . Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]. . m_train = train_set_x_orig.shape[0] num_px = train_set_x_orig.shape[1] m_test = test_set_x_orig.shape[0] print (&quot;Number of training examples: m_train = &quot; + str(m_train)) print (&quot;Number of testing examples: m_test = &quot; + str(m_test)) print (&quot;Height/Width of each image: num_px = &quot; + str(num_px)) print (&quot;Each image is of size: (&quot; + str(num_px) + &quot;, &quot; + str(num_px) + &quot;, 3)&quot;) print (&quot;train_set_x shape: &quot; + str(train_set_x_orig.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x shape: &quot; + str(test_set_x_orig.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) . Expected Output for m_train, m_test and num_px: . m_train | 209 | . m_test | 50 | . num_px | 64 | . For convenience, we should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $*$ num_px $*$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns. . . Exercise 2 . Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $*$ num_px $*$ 3, 1). . A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$*$c$*$d, a) is to use: . X_flatten = X.reshape(X.shape[0], -1).T # X.T is the transpose of X . train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T test_set_x_flatten = test_set_x_orig.reshape( test_set_x_orig.shape[0],-1).T # Check that the first 10 pixels of the second image are in the correct place assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), &quot;Wrong solution. Use (X.shape[0], -1).T.&quot; assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), &quot;Wrong solution. Use (X.shape[0], -1).T.&quot; print (&quot;train_set_x_flatten shape: &quot; + str(train_set_x_flatten.shape)) print (&quot;train_set_y shape: &quot; + str(train_set_y.shape)) print (&quot;test_set_x_flatten shape: &quot; + str(test_set_x_flatten.shape)) print (&quot;test_set_y shape: &quot; + str(test_set_y.shape)) . Expected Output: . train_set_x_flatten shape | (12288, 209) | . train_set_y shape | (1, 209) | . test_set_x_flatten shape | (12288, 50) | . test_set_y shape | (1, 50) | . To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255. . One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). . Let&#39;s standardize our dataset. . train_set_x = train_set_x_flatten / 255. test_set_x = test_set_x_flatten / 255. . What you need to remember: . Common steps for pre-processing a new dataset are: . Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...) | Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1) | &quot;Standardize&quot; the data | . . 3 - General Architecture of the learning algorithm . It&#39;s time to design a simple algorithm to distinguish cat images from non-cat images. . You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network! . Mathematical expression of the algorithm: . For one example $x^{(i)}$: $$z^{(i)} = w^T x^{(i)} + b tag{1}$$ $$ hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) tag{2}$$ $$ mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} log(a^{(i)}) - (1-y^{(i)} ) log(1-a^{(i)}) tag{3}$$ . The cost is then computed by summing over all training examples: $$ J = frac{1}{m} sum_{i=1}^m mathcal{L}(a^{(i)}, y^{(i)}) tag{6}$$ . Key steps: In this exercise, you will carry out the following steps: . - Initialize the parameters of the model - Learn the parameters for the model by minimizing the cost - Use the learned parameters to make predictions (on the test set) - Analyse the results and conclude . . 4 - Building the parts of our algorithm . The main steps for building a Neural Network are: . Define the model structure (such as number of input features) | Initialize the model&#39;s parameters | Loop: Calculate current loss (forward propagation) | Calculate current gradient (backward propagation) | Update parameters (gradient descent) | . | You often build 1-3 separately and integrate them into one function we call model(). . . 4.1 - Helper functions . Exercise 3 - sigmoid . Using your code from &quot;Python Basics&quot;, implement sigmoid(). As you&#39;ve seen in the figure above, you need to compute $sigmoid(z) = frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions. Use np.exp(). . def sigmoid(z): &quot;&quot;&quot; Compute the sigmoid of z Arguments: z -- A scalar or numpy array of any size. Return: s -- sigmoid(z) &quot;&quot;&quot; dummy = (1 + np.exp(-z)) s = 1/dummy return s . print (&quot;sigmoid([0, 2]) = &quot; + str(sigmoid(np.array([0,2])))) sigmoid_test(sigmoid) . x = np.array([0.5, 0, 2.0]) output = sigmoid(x) print(output) . . 4.2 - Initializing parameters . Exercise 4 - initialize_with_zeros . Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don&#39;t know what numpy function to use, look up np.zeros() in the Numpy library&#39;s documentation. . def initialize_with_zeros(dim): &quot;&quot;&quot; This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) of type float &quot;&quot;&quot; w = (np.zeros((dim,1))) b = 0.0 return w, b . dim = 2 w, b = initialize_with_zeros(dim) assert type(b) == float print (&quot;w = &quot; + str(w)) print (&quot;b = &quot; + str(b)) initialize_with_zeros_test(initialize_with_zeros) . . 4.3 - Forward and Backward propagation . Now that your parameters are initialized, you can do the &quot;forward&quot; and &quot;backward&quot; propagation steps for learning the parameters. . . Exercise 5 - propagate . Implement a function propagate() that computes the cost function and its gradient. . Hints: . Forward Propagation: . You get X | You compute $A = sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$ | You calculate the cost function: $J = - frac{1}{m} sum_{i=1}^{m}(y^{(i)} log(a^{(i)})+(1-y^{(i)}) log(1-a^{(i)}))$ | . Here are the two formulas you will be using: . $$ frac{ partial J}{ partial w} = frac{1}{m}X(A-Y)^T tag{7}$$ $$ frac{ partial J}{ partial b} = frac{1}{m} sum_{i=1}^m (a^{(i)}-y^{(i)}) tag{8}$$ . def propagate(w, b, X, Y): &quot;&quot;&quot; Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation. np.log(), np.dot() &quot;&quot;&quot; m = X.shape[1] # FORWARD PROPAGATION (FROM X TO COST) # compute activation # compute cost by using np.dot to perform multiplication. A = sigmoid(np.dot(w.T,X) +b) cost = (-1/m)* np.sum((Y*(np.log(A)) + (1-Y)* np.log(1-A))) # BACKWARD PROPAGATION (TO FIND GRAD) dw = (1/m)* np.dot(X,((A-Y).T)) db = (1/m)* np.sum((A-Y)) cost = np.squeeze(np.array(cost)) grads = {&quot;dw&quot;: dw, &quot;db&quot;: db} return grads, cost . w = np.array([[1.], [2]]) b = 1.5 X = np.array([[1., -2., -1.], [3., 0.5, -3.2]]) Y = np.array([[1, 1, 0]]) grads, cost = propagate(w, b, X, Y) assert type(grads[&quot;dw&quot;]) == np.ndarray assert grads[&quot;dw&quot;].shape == (2, 1) assert type(grads[&quot;db&quot;]) == np.float64 print (&quot;dw = &quot; + str(grads[&quot;dw&quot;])) print (&quot;db = &quot; + str(grads[&quot;db&quot;])) print (&quot;cost = &quot; + str(cost)) propagate_test(propagate) . Expected output . dw = [[ 0.25071532] [-0.06604096]] db = -0.1250040450043965 cost = 0.15900537707692405 . . 4.4 - Optimization . You have initialized your parameters. | You are also able to compute a cost function and its gradient. | Now, you want to update the parameters using gradient descent. | . . Exercise 6 - optimize . Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $ theta$, the update rule is $ theta = theta - alpha text{ } d theta$, where $ alpha$ is the learning rate. . def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False): &quot;&quot;&quot; This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b. &quot;&quot;&quot; w = copy.deepcopy(w) b = copy.deepcopy(b) costs = [] for i in range(num_iterations): # Cost and gradient calculation grads, cost = propagate(w,b,X,Y) # Retrieve derivatives from grads dw = grads[&quot;dw&quot;] db = grads[&quot;db&quot;] # update rule w = w - (learning_rate * dw) b = b - (learning_rate * db) # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training iterations if print_cost: print (&quot;Cost after iteration %i: %f&quot; %(i, cost)) params = {&quot;w&quot;: w, &quot;b&quot;: b} grads = {&quot;dw&quot;: dw, &quot;db&quot;: db} return params, grads, costs . params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False) print (&quot;w = &quot; + str(params[&quot;w&quot;])) print (&quot;b = &quot; + str(params[&quot;b&quot;])) print (&quot;dw = &quot; + str(grads[&quot;dw&quot;])) print (&quot;db = &quot; + str(grads[&quot;db&quot;])) print(&quot;Costs = &quot; + str(costs)) optimize_test(optimize) . . Exercise 7 - predict . The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function. There are two steps to computing predictions: . Calculate $ hat{Y} = A = sigma(w^T X + b)$ . | Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this). . | def predict(w, b, X): &#39;&#39;&#39; Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X &#39;&#39;&#39; m = X.shape[1] Y_prediction = np.zeros((1, m)) w = w.reshape(X.shape[0], 1) # Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture A = sigmoid(np.dot(w.T, X) + b) for i in range(A.shape[1]): # Convert probabilities A[0,i] to actual predictions p[0,i] # if A[0, i] &gt; ____ : # Y_prediction[0,i] = # else: # Y_prediction[0,i] = if A[0,i]&gt; 0.5: Y_prediction[0,i]= 1 else: Y_prediction[0,i] = 0 return Y_prediction . w = np.array([[0.1124579], [0.23106775]]) b = -0.3 X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]]) print (&quot;predictions = &quot; + str(predict(w, b, X))) predict_test(predict) . What to remember: . You&#39;ve implemented several functions that: . Initialize (w,b) | Optimize the loss iteratively to learn parameters (w,b): Computing the cost and its gradient | Updating the parameters using gradient descent | . | Use the learned (w,b) to predict the labels for a given set of examples | . . 5 - Merge all functions into a model . You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order. . . Exercise 8 - model . Implement the model function. Use the following notation: . - Y_prediction_test for your predictions on the test set - Y_prediction_train for your predictions on the train set - parameters, grads, costs for the outputs of optimize() . def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False): &quot;&quot;&quot; Builds the logistic regression model by calling the function you&#39;ve implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to True to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. &quot;&quot;&quot; # initialize parameters with zeros w,b = initialize_with_zeros(X_train.shape[0]) # Gradient descent params ,grads ,costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost) # Retrieve parameters w and b from dictionary &quot;params&quot; w = params[&quot;w&quot;] b = params[&quot;b&quot;] # Predict test/train set examples Y_prediction_test = predict(w,b,X_test) Y_prediction_train = predict(w,b,X_train) # Print train/test Errors if print_cost: print(&quot;train accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print(&quot;test accuracy: {} %&quot;.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = {&quot;costs&quot;: costs, &quot;Y_prediction_test&quot;: Y_prediction_test, &quot;Y_prediction_train&quot; : Y_prediction_train, &quot;w&quot; : w, &quot;b&quot; : b, &quot;learning_rate&quot; : learning_rate, &quot;num_iterations&quot;: num_iterations} return d . from public_tests import * model_test(model) . If you pass all the tests, run the following cell to train your model. . logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True) . Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you&#39;ll build an even better classifier next week! . Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set. . index = 1 plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3))) print (&quot;y = &quot; + str(test_set_y[0,index]) + &quot;, you predicted that it is a &quot;&quot; + classes[int(logistic_regression_model[&#39;Y_prediction_test&#39;][0,index])].decode(&quot;utf-8&quot;) + &quot; &quot; picture.&quot;) . Let&#39;s also plot the cost function and the gradients. . costs = np.squeeze(logistic_regression_model[&#39;costs&#39;]) plt.plot(costs) plt.ylabel(&#39;cost&#39;) plt.xlabel(&#39;iterations (per hundreds)&#39;) plt.title(&quot;Learning rate =&quot; + str(logistic_regression_model[&quot;learning_rate&quot;])) plt.show() . Interpretation: You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. . . 6 - Further analysis . We built our first image classification model. Let&#39;s analyze it further, and examine possible choices for the learning rate $ alpha$. . Choice of learning rate . Reminder: In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $ alpha$ determines how rapidly we update the parameters. If the learning rate is too large we may &quot;overshoot&quot; the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That&#39;s why it is crucial to use a well-tuned learning rate. . Let&#39;s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens. . learning_rates = [0.01, 0.001, 0.0001] models = {} for lr in learning_rates: print (&quot;Training a model with learning rate: &quot; + str(lr)) models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False) print (&#39; n&#39; + &quot;-&quot; + &#39; n&#39;) for lr in learning_rates: plt.plot(np.squeeze(models[str(lr)][&quot;costs&quot;]), label=str(models[str(lr)][&quot;learning_rate&quot;])) plt.ylabel(&#39;cost&#39;) plt.xlabel(&#39;iterations (hundreds)&#39;) legend = plt.legend(loc=&#39;upper center&#39;, shadow=True) frame = legend.get_frame() frame.set_facecolor(&#39;0.90&#39;) plt.show() . Interpretation: . Different learning rates give different costs and thus different predictions results. | If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). | A lower cost doesn&#39;t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. | In deep learning, we usually recommend that you: Choose the learning rate that better minimizes the cost function. | If your model overfits, use other techniques to reduce overfitting. (We&#39;ll talk about this in later videos.) | . | . . 7 - Test with your own image . You can use your own image and see the output of your model. To do that: 1. Click on &quot;File&quot; in the upper bar of this notebook, then click &quot;Open&quot; to go on your Coursera Hub. 2. Add your image to this Jupyter Notebook&#39;s directory, in the &quot;images&quot; folder 3. Change your image&#39;s name in the following code 4. Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)! . my_image = &quot;docpic.jpg&quot; # We preprocess the image to fit your algorithm. fname = &quot;images/&quot; + my_image image = np.array(Image.open(fname).resize((num_px, num_px))) plt.imshow(image) image = image / 255. image = image.reshape((1, num_px * num_px * 3)).T my_predicted_image = predict(logistic_regression_model[&quot;w&quot;], logistic_regression_model[&quot;b&quot;], image) print(&quot;y = &quot; + str(np.squeeze(my_predicted_image)) + &quot;, your algorithm predicts a &quot;&quot; + classes[int(np.squeeze(my_predicted_image)),].decode(&quot;utf-8&quot;) + &quot; &quot; picture.&quot;) . What to remember : . Preprocessing the dataset is important. | You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). | Tuning the learning rate (which is an example of a &quot;hyperparameter&quot;) can make a big difference to the algorithm. You will see more examples of this later in this course! | Bibliography: . http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/ | https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2021/11/20/Logistic-Regression-with-a-Neural-Network-mindset.html",
            "relUrl": "/jupyter/2021/11/20/Logistic-Regression-with-a-Neural-Network-mindset.html",
            "date": " • Nov 20, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Pandas Data Analysis Basics Tutorial",
            "content": "Pandas Data Analysis short tutorial: . This is an assignment, a part of the course &quot;Data Analysis with Python: Zero to Pandas&quot; . This tutorial demonstrates data analysis using example from two data sets using the Pandas Library. All the important operations are described in markdown cells. . !pip install pandas --upgrade . Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (1.3.4) Requirement already satisfied: numpy&gt;=1.17.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (1.20.3) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2021.1) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas) (2.8.2) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.16.0) . import pandas as pd . In this tutorial, we&#39;re going to analyze an operate on data from a CSV file. Let&#39;s begin by downloading the CSV file. . from urllib.request import urlretrieve urlretrieve(&#39;https://hub.jovian.ml/wp-content/uploads/2020/09/countries.csv&#39;, &#39;countries.csv&#39;) . (&#39;countries.csv&#39;, &lt;http.client.HTTPMessage at 0x7ff1c18db130&gt;) . Let&#39;s load the data from the CSV file into a Pandas data frame. . countries_df = pd.read_csv(&#39;countries.csv&#39;) . countries_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | . ... ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | . 210 rows × 6 columns . Q1: How many countries does the dataframe contain? . num_countries,colparameters = countries_df.shape . print(&#39;There are {} countries in the dataset&#39;.format(num_countries)) . There are 210 countries in the dataset . Q2: Retrieve a list of continents from the dataframe? . continents = pd.Series(countries_df.continent).unique() . continents . array([&#39;Asia&#39;, &#39;Europe&#39;, &#39;Africa&#39;, &#39;North America&#39;, &#39;South America&#39;, &#39;Oceania&#39;], dtype=object) . Q3: What is the total population of all the countries listed in this dataset? . total_population = countries_df.population.sum() . print(&#39;The total population is {}.&#39;.format(int(total_population))) . The total population is 7757980095. . Q: What is the overall life expectancy across in the world? . print((countries_df.population*countries_df.life_expectancy).sum()/countries_df.population.sum()) . 72.72165193409664 . overall_life = countries_df[&#39;life_expectancy&#39;].mean() . overall_life . 73.52985507246376 . Q4: Create a dataframe containing 10 countries with the highest population. . most_populous_df = (countries_df.sort_values(by=[&#39;population&#39;],ascending = False)).head(10) . most_populous_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita . 41 China | Asia | 1.439324e+09 | 76.91 | 4.34 | 15308.712 | . 90 India | Asia | 1.380004e+09 | 69.66 | 0.53 | 6426.674 | . 199 United States | North America | 3.310026e+08 | 78.86 | 2.77 | 54225.446 | . 91 Indonesia | Asia | 2.735236e+08 | 71.72 | 1.04 | 11188.744 | . 145 Pakistan | Asia | 2.208923e+08 | 67.27 | 0.60 | 5034.708 | . 27 Brazil | South America | 2.125594e+08 | 75.88 | 2.20 | 14103.452 | . 141 Nigeria | Africa | 2.061396e+08 | 54.69 | NaN | 5338.454 | . 15 Bangladesh | Asia | 1.646894e+08 | 72.59 | 0.80 | 3523.984 | . 157 Russia | Europe | 1.459345e+08 | 72.58 | 8.05 | 24765.954 | . 125 Mexico | North America | 1.289328e+08 | 75.05 | 1.38 | 17336.469 | . Q5: Add a new column in countries_df to record the overall GDP per country (product of population &amp; per capita GDP). . countries_df[&#39;gdp&#39;] = countries_df.population * countries_df.gdp_per_capita . countries_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | 7.022622e+10 | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | 3.396791e+10 | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | 6.101364e+11 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | 1.912651e+11 | . ... ... | ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | 6.007624e+11 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | NaN | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | 4.411699e+10 | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | 6.782303e+10 | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | 2.823622e+10 | . 210 rows × 7 columns . Q6: Create a data frame that counts the number countries in each continent? . country_counts_df = countries_df.groupby(&#39;continent&#39;).count() . country_counts_df . location population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp . continent . Africa 55 | 55 | 55 | 40 | 53 | 53 | . Asia 47 | 47 | 47 | 43 | 45 | 45 | . Europe 51 | 51 | 48 | 43 | 42 | 42 | . North America 36 | 36 | 36 | 23 | 27 | 27 | . Oceania 8 | 8 | 8 | 3 | 4 | 4 | . South America 13 | 13 | 13 | 12 | 12 | 12 | . Q7: Create a data frame showing the total population of each continent. . continent_populations_df = countries_df.groupby(&#39;continent&#39;).sum() . continent_populations_df = continent_populations_df[&#39;population&#39;] . continent_populations_df . continent Africa 1.339424e+09 Asia 4.607388e+09 Europe 7.485062e+08 North America 5.912425e+08 Oceania 4.095832e+07 South America 4.304611e+08 Name: population, dtype: float64 . Let&#39;s download another CSV file containing overall Covid-19 stats for various countires, and read the data into another Pandas data frame. . urlretrieve(&#39;https://hub.jovian.ml/wp-content/uploads/2020/09/covid-countries-data.csv&#39;, &#39;covid-countries-data.csv&#39;) . (&#39;covid-countries-data.csv&#39;, &lt;http.client.HTTPMessage at 0x7ff1789d6c10&gt;) . covid_data_df = pd.read_csv(&#39;covid-countries-data.csv&#39;) . covid_data_df . location total_cases total_deaths total_tests . 0 Afghanistan | 38243.0 | 1409.0 | NaN | . 1 Albania | 9728.0 | 296.0 | NaN | . 2 Algeria | 45158.0 | 1525.0 | NaN | . 3 Andorra | 1199.0 | 53.0 | NaN | . 4 Angola | 2729.0 | 109.0 | NaN | . ... ... | ... | ... | ... | . 207 Western Sahara | 766.0 | 1.0 | NaN | . 208 World | 26059065.0 | 863535.0 | NaN | . 209 Yemen | 1976.0 | 571.0 | NaN | . 210 Zambia | 12415.0 | 292.0 | NaN | . 211 Zimbabwe | 6638.0 | 206.0 | 97272.0 | . 212 rows × 4 columns . Q8: Count the number of countries for which the total_tests data is missing. . Hint: Use the .isna method. . total_tests_miss = covid_data_df.isna() #total_tests_missing = (total_tests_miss[total_tests_miss.total_tests == True]).count() total_tests_missing = total_tests_miss[&#39;total_tests&#39;].values.sum() . print(&quot;The data for total tests is missing for {} countries.&quot;.format(int(total_tests_missing))) . The data for total tests is missing for 122 countries. . Let&#39;s merge the two data frames, and compute some more metrics. . Q9: Merge countries_df with covid_data_df on the location column. . combined_df = countries_df.merge(covid_data_df,on=&#39;location&#39;) . combined_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | 7.022622e+10 | 38243.0 | 1409.0 | NaN | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | 3.396791e+10 | 9728.0 | 296.0 | NaN | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | 6.101364e+11 | 45158.0 | 1525.0 | NaN | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | 1199.0 | 53.0 | NaN | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | 1.912651e+11 | 2729.0 | 109.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | 6.007624e+11 | 1046.0 | 35.0 | 261004.0 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | NaN | 766.0 | 1.0 | NaN | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | 4.411699e+10 | 1976.0 | 571.0 | NaN | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | 6.782303e+10 | 12415.0 | 292.0 | NaN | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | 2.823622e+10 | 6638.0 | 206.0 | 97272.0 | . 210 rows × 10 columns . Q10: Add columns tests_per_million, cases_per_million and deaths_per_million into combined_df. . combined_df[&#39;tests_per_million&#39;] = combined_df[&#39;total_tests&#39;] * 1e6 / combined_df[&#39;population&#39;] . combined_df[&#39;cases_per_million&#39;] = combined_df[&#39;total_cases&#39;] * 1e6 / combined_df[&#39;population&#39;] . combined_df[&#39;deaths_per_million&#39;] = combined_df[&#39;total_deaths&#39;] * 1e6 / combined_df[&#39;population&#39;] . combined_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 0 Afghanistan | Asia | 38928341.0 | 64.83 | 0.50 | 1803.987 | 7.022622e+10 | 38243.0 | 1409.0 | NaN | NaN | 982.394806 | 36.194710 | . 1 Albania | Europe | 2877800.0 | 78.57 | 2.89 | 11803.431 | 3.396791e+10 | 9728.0 | 296.0 | NaN | NaN | 3380.359997 | 102.856349 | . 2 Algeria | Africa | 43851043.0 | 76.88 | 1.90 | 13913.839 | 6.101364e+11 | 45158.0 | 1525.0 | NaN | NaN | 1029.804468 | 34.776824 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | 1199.0 | 53.0 | NaN | NaN | 15518.022390 | 685.950948 | . 4 Angola | Africa | 32866268.0 | 61.15 | NaN | 5819.495 | 1.912651e+11 | 2729.0 | 109.0 | NaN | NaN | 83.033462 | 3.316470 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 205 Vietnam | Asia | 97338583.0 | 75.40 | 2.60 | 6171.884 | 6.007624e+11 | 1046.0 | 35.0 | 261004.0 | 2681.403324 | 10.745996 | 0.359570 | . 206 Western Sahara | Africa | 597330.0 | 70.26 | NaN | NaN | NaN | 766.0 | 1.0 | NaN | NaN | 1282.373228 | 1.674116 | . 207 Yemen | Asia | 29825968.0 | 66.12 | 0.70 | 1479.147 | 4.411699e+10 | 1976.0 | 571.0 | NaN | NaN | 66.250993 | 19.144391 | . 208 Zambia | Africa | 18383956.0 | 63.89 | 2.00 | 3689.251 | 6.782303e+10 | 12415.0 | 292.0 | NaN | NaN | 675.317108 | 15.883415 | . 209 Zimbabwe | Africa | 14862927.0 | 61.49 | 1.70 | 1899.775 | 2.823622e+10 | 6638.0 | 206.0 | 97272.0 | 6544.605918 | 446.614587 | 13.859989 | . 210 rows × 13 columns . Q11: Create a dataframe with 10 countires that have highest number of tests per million people. . highest_tests_df = combined_df.nlargest(10,[&quot;tests_per_million&quot;]) . highest_tests_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 197 United Arab Emirates | Asia | 9890400.0 | 77.97 | 1.200 | 67293.483 | 6.655595e+11 | 71540.0 | 387.0 | 7177430.0 | 725696.635121 | 7233.276713 | 39.128852 | . 14 Bahrain | Asia | 1701583.0 | 77.29 | 2.000 | 43290.705 | 7.366273e+10 | 52440.0 | 190.0 | 1118837.0 | 657527.137965 | 30818.361490 | 111.660730 | . 115 Luxembourg | Europe | 625976.0 | 82.25 | 4.510 | 94277.965 | 5.901574e+10 | 7928.0 | 124.0 | 385820.0 | 616349.508607 | 12665.022301 | 198.090662 | . 122 Malta | Europe | 441539.0 | 82.53 | 4.485 | 36513.323 | 1.612206e+10 | 1931.0 | 13.0 | 188539.0 | 427004.183096 | 4373.339614 | 29.442473 | . 53 Denmark | Europe | 5792203.0 | 80.90 | 2.500 | 46682.515 | 2.703946e+11 | 17195.0 | 626.0 | 2447911.0 | 422621.755488 | 2968.645954 | 108.076323 | . 96 Israel | Asia | 8655541.0 | 82.97 | 2.990 | 33132.320 | 2.867782e+11 | 122539.0 | 969.0 | 2353984.0 | 271962.665303 | 14157.289533 | 111.951408 | . 89 Iceland | Europe | 341250.0 | 82.99 | 2.910 | 46482.958 | 1.586231e+10 | 2121.0 | 10.0 | 88829.0 | 260304.761905 | 6215.384615 | 29.304029 | . 157 Russia | Europe | 145934460.0 | 72.58 | 8.050 | 24765.954 | 3.614206e+12 | 1005000.0 | 17414.0 | 37176827.0 | 254750.159763 | 6886.653091 | 119.327539 | . 199 United States | North America | 331002647.0 | 78.86 | 2.770 | 54225.446 | 1.794877e+13 | 6114406.0 | 185744.0 | 83898416.0 | 253467.507769 | 18472.377957 | 561.155633 | . 10 Australia | Oceania | 25499881.0 | 83.44 | 3.840 | 44648.710 | 1.138537e+12 | 25923.0 | 663.0 | 6255797.0 | 245326.517406 | 1016.592979 | 26.000121 | . Q12: Create a dataframe with 10 countires that have highest number of positive cases per million people. . highest_cases_df = combined_df.nlargest(10,[&quot;cases_per_million&quot;]) . highest_cases_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 155 Qatar | Asia | 2881060.0 | 80.23 | 1.20 | 116935.600 | 3.368985e+11 | 119206.0 | 199.0 | 634745.0 | 220316.480740 | 41375.743650 | 69.071800 | . 14 Bahrain | Asia | 1701583.0 | 77.29 | 2.00 | 43290.705 | 7.366273e+10 | 52440.0 | 190.0 | 1118837.0 | 657527.137965 | 30818.361490 | 111.660730 | . 147 Panama | North America | 4314768.0 | 78.51 | 2.30 | 22267.037 | 9.607710e+10 | 94084.0 | 2030.0 | 336345.0 | 77952.047480 | 21805.112117 | 470.477208 | . 40 Chile | South America | 19116209.0 | 80.18 | 2.11 | 22767.037 | 4.352194e+11 | 414739.0 | 11344.0 | 2458762.0 | 128621.841287 | 21695.671982 | 593.423100 | . 162 San Marino | Europe | 33938.0 | 84.97 | 3.80 | 56861.470 | 1.929765e+09 | 735.0 | 42.0 | NaN | NaN | 21657.139490 | 1237.550828 | . 9 Aruba | North America | 106766.0 | 76.29 | NaN | 35973.781 | 3.840777e+09 | 2211.0 | 12.0 | NaN | NaN | 20708.839893 | 112.395332 | . 105 Kuwait | Asia | 4270563.0 | 75.49 | 2.00 | 65530.537 | 2.798523e+11 | 86478.0 | 535.0 | 621616.0 | 145558.325682 | 20249.789079 | 125.276222 | . 150 Peru | South America | 32971846.0 | 76.74 | 1.60 | 12236.706 | 4.034668e+11 | 663437.0 | 29259.0 | 584232.0 | 17719.117092 | 20121.318048 | 887.393445 | . 27 Brazil | South America | 212559409.0 | 75.88 | 2.20 | 14103.452 | 2.997821e+12 | 3997865.0 | 123780.0 | 4797948.0 | 22572.268255 | 18808.224105 | 582.331314 | . 199 United States | North America | 331002647.0 | 78.86 | 2.77 | 54225.446 | 1.794877e+13 | 6114406.0 | 185744.0 | 83898416.0 | 253467.507769 | 18472.377957 | 561.155633 | . Q13: Create a dataframe with 10 countires that have highest number of deaths cases per million people? . highest_deaths_df = combined_df.nlargest(10,[&quot;deaths_per_million&quot;]) . highest_deaths_df . location continent population life_expectancy hospital_beds_per_thousand gdp_per_capita gdp total_cases total_deaths total_tests tests_per_million cases_per_million deaths_per_million . 162 San Marino | Europe | 33938.0 | 84.97 | 3.80 | 56861.470 | 1.929765e+09 | 735.0 | 42.0 | NaN | NaN | 21657.139490 | 1237.550828 | . 150 Peru | South America | 32971846.0 | 76.74 | 1.60 | 12236.706 | 4.034668e+11 | 663437.0 | 29259.0 | 584232.0 | 17719.117092 | 20121.318048 | 887.393445 | . 18 Belgium | Europe | 11589616.0 | 81.63 | 5.64 | 42658.576 | 4.943965e+11 | 85817.0 | 9898.0 | 2281853.0 | 196887.713967 | 7404.645676 | 854.040375 | . 3 Andorra | Europe | 77265.0 | 83.73 | NaN | NaN | NaN | 1199.0 | 53.0 | NaN | NaN | 15518.022390 | 685.950948 | . 177 Spain | Europe | 46754783.0 | 83.56 | 2.97 | 34272.360 | 1.602397e+12 | 479554.0 | 29194.0 | 6416533.0 | 137238.001939 | 10256.790198 | 624.406705 | . 198 United Kingdom | Europe | 67886004.0 | 81.32 | 2.54 | 39753.244 | 2.698689e+12 | 338676.0 | 41514.0 | 13447568.0 | 198090.434075 | 4988.892850 | 611.525168 | . 40 Chile | South America | 19116209.0 | 80.18 | 2.11 | 22767.037 | 4.352194e+11 | 414739.0 | 11344.0 | 2458762.0 | 128621.841287 | 21695.671982 | 593.423100 | . 97 Italy | Europe | 60461828.0 | 83.51 | 3.18 | 35220.084 | 2.129471e+12 | 271515.0 | 35497.0 | 5214766.0 | 86248.897403 | 4490.684602 | 587.097697 | . 27 Brazil | South America | 212559409.0 | 75.88 | 2.20 | 14103.452 | 2.997821e+12 | 3997865.0 | 123780.0 | 4797948.0 | 22572.268255 | 18808.224105 | 582.331314 | . 182 Sweden | Europe | 10099270.0 | 82.80 | 2.22 | 46949.283 | 4.741535e+11 | 84532.0 | 5820.0 | NaN | NaN | 8370.109919 | 576.279276 | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/jupyter/2021/10/05/Pandas-Data-Analysis-Basics-Tutorial.html",
            "relUrl": "/jupyter/2021/10/05/Pandas-Data-Analysis-Basics-Tutorial.html",
            "date": " • Oct 5, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "Guide to Numpy Operations",
            "content": "A Short Guide to Numpy Array operations. . Introducing Numpy Arrays: . Numpy is a python library used to create, modify and interact with Arrays. It is a very vast and powerful library which is very essential for data analysis tasks. The Numpy library basic routines are very simple and easy to learn. Numpy is also known for scientific computation since it can handle large amount of data in the form of arrays. Numpy is mostly written in C which makes it run time fast and execute operations faster. It is very useful for Mathematics since it offers a vast collection of mathematical functions and random number generators. In this notebook, I try to demonstrate some standard numpy operations essential for operating on matrices. Below are the five operations that I am going to demonstrate: . np.matmul | np.linalg.solve | np.column_stack(tup) | np.bmat | np.log | . The demonstrating includes the syntax of using these operations and how to pass arguments for these functions. It also demonstrates in what special cases do we get errors using these operations and also how to solve fix the erorrs efficiently. . Let&#39;s begin by importing Numpy and listing out the functions covered in this notebook. . import numpy as np . function1 = np.matmul function2 = np.linalg.solve function3 = np.column_stack function4 = np.bmat function5 = np.emath . Function 1 - np.matmul . The np.matmul operator is a matrix multiplication operator used to multiply arrays.The @ is short form for using this operator. The function has the following syntax: np.matmul(a,b)-- where a,b are arrays that are to be multiplied. . arr1 = [[15, 24], [33, 4.]] arr2 = [[5, 16, 37], [18, 9, 20]] mult=np.matmul(arr1,arr2) print(mult) . [[ 507. 456. 1035.] [ 237. 564. 1301.]] . Explanation: The matmul operator takes two 2d arrays i.e. arr1 and arr2 abd multiplied by matrix multiplication. Inorder for arrays to be able to perform matrix multiplication the 2nd dimension of arr1 should be equal to first dimension of arr2. Here they satisfy the condition and the output is generated. If the dimensions of two arrays do not satisfy the property we get a traceback error msg telling us there is a mismatch in dimensions. . np.matmul([2j, 3j], [2j, 3j]) . (-13+0j) . The vector vector gives us scalar inner product. Given two vector arrays the np.matmul operation does dot product or also called as scalar product. The 0j in the output is the complex part of the output since there are no complex numbers involved we get zero complex part in our output . arr1 = [[15,24,32], [3, 4.,12]] arr2 = [[5, 6, 7], [8, 9, 10]] np.matmul(arr1,arr2) . ValueError Traceback (most recent call last) /tmp/ipykernel_36/2247344183.py in &lt;module&gt; 6 [8, 9, 10]] 7 -&gt; 8 np.matmul(arr1,arr2) ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)-&gt;(n?,m?) (size 2 is different from 3) . It breaks because matrix multiplication takes place only when the second dimension of the first array is equal to first dimension of the second array. . This function is used to multiply arrays, for dot product of two vectors and multiplication of 3d arrays etc. . Function 2 - np.linalg.solve . This operator is used to solve linear equations and give out the value of unknowns from the system of linear equations. For example ax = b be a system of linear equations and the above function solves the equation for x. a, b are matrices formed from the system of linear equations.In mathematical terms, a is the coefficient matrix and x is the vector containing unknowns and b is the right hand side values of the system of equations. The function has the following syntax: np.linalg.solve(a,b) where a,b are arrays coefficient matrix , dependant variable matrix. . a = [[5,6], [7,8]] b = [12,16] x = np.linalg.solve(a,b) print(x) . [0. 2.] . Here we found the solution for the system of linear equations where a is coefficient matrix and b is the solution matrix. the system of linear equations are of the form: 5m + 6n = 12 7m + 8n = 16 and the matrix a is made from the coefficients of the equations the output gives a vector x of two elements that are the solutions of m and n respectively. . Example 2 - working . a = [[2,3,4],[8,4,2],[5,3,4]] b = [12,16,18] x = np.linalg.solve(a,b) print(x) . [ 2. -1.6 3.2] . The above example is based on three linear equations in three variables. the np.linalg.solve all kinds of systems of linear equations. . a = [[1,2,3],[4,5,6],[7,8,9]] b = [12,16,18] x = np.linalg.solve(a,b) . LinAlgError Traceback (most recent call last) /tmp/ipykernel_36/3857377319.py in &lt;module&gt; 2 a = [[1,2,3],[4,5,6],[7,8,9]] 3 b = [12,16,18] -&gt; 4 x = np.linalg.solve(a,b) &lt;__array_function__ internals&gt; in solve(*args, **kwargs) /opt/conda/lib/python3.9/site-packages/numpy/linalg/linalg.py in solve(a, b) 391 signature = &#39;DD-&gt;D&#39; if isComplexType(t) else &#39;dd-&gt;d&#39; 392 extobj = get_linalg_error_extobj(_raise_linalgerror_singular) --&gt; 393 r = gufunc(a, b, signature=signature, extobj=extobj) 394 395 return wrap(r.astype(result_t, copy=False)) /opt/conda/lib/python3.9/site-packages/numpy/linalg/linalg.py in _raise_linalgerror_singular(err, flag) 86 87 def _raise_linalgerror_singular(err, flag): &gt; 88 raise LinAlgError(&#34;Singular matrix&#34;) 89 90 def _raise_linalgerror_nonposdef(err, flag): LinAlgError: Singular matrix . It breaks down above because solving of linear equations has some conditions. If the matrix is singular the output does not give any solution. singular matrix is one that is not invertible. This means that the system of equations you are trying to solve does not have a unique solution. . This function proves very useful when we need to solve n number of linear equations in n variables and gives the solution. There are a lot of uses for the function Mathematically since it reduced the load of solving linear equations by row reduction process, which is very tedious and most times do not provide accurate solution. . Function 3 - np.column_stack . np.column_stack is used to stack two 1-D arrays into a 2D array of column wise.It is used in the following format np.column_stack(tuple). The parameter is given as tuple type. The 1D array are entered as tuples. The function has the following syntax: np.column_stack(tuple) . a = np.array((4,5,6)) b = np.array((12,14,17)) c = np.column_stack((a,b)) print(c) . [[ 4 12] [ 5 14] [ 6 17]] . The two arrays a and b are stacked side by side / column wise and the output array is a 2D array containing one column(a) and the other column(b) . a = np.array((3,34,53,72,65,81)) b = np.array((2,5,43,76,83,92)) c = np.column_stack((a,b)) print(c) . [[ 3 2] [34 5] [53 43] [72 76] [65 83] [81 92]] . The above example stacks two given arrays in the form of tuples and stacks them side by side. . a = np.array((1,2,3,4)) b = np.array((12,13,14)) c = np.column_stack((a,b)) print(c) . ValueError Traceback (most recent call last) /tmp/ipykernel_36/354580763.py in &lt;module&gt; 2 a = np.array((1,2,3,4)) 3 b = np.array((12,13,14)) -&gt; 4 c = np.column_stack((a,b)) 5 print(c) &lt;__array_function__ internals&gt; in column_stack(*args, **kwargs) /opt/conda/lib/python3.9/site-packages/numpy/lib/shape_base.py in column_stack(tup) 654 arr = array(arr, copy=False, subok=True, ndmin=2).T 655 arrays.append(arr) --&gt; 656 return _nx.concatenate(arrays, 1) 657 658 &lt;__array_function__ internals&gt; in concatenate(*args, **kwargs) ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 4 and the array at index 1 has size 3 . The function breaks down because one of the array is longer in size than the other. The concatenation or stacking takes place only when both the arrays to be stacked has same size. . The function is very useful when we need to stack two list .Similar to appending but here we are appending columns. The function can be used to add the column to pre-existing ones. . Function 4 - np.bmat . It builds a matrix object from a string, nested sequence or an array. The function has the following syntax: np.bmat(object,ldict,gdict)where ldict and gdict are dictionaries and they are optional parameters and object is either string or array-like. . A = np.mat(&#39;1 1; 1 1&#39;) B = np.mat(&#39;2 2; 2 2&#39;) C = np.mat(&#39;3 4; 5 6&#39;) D = np.mat(&#39;7 8; 9 0&#39;) np.bmat([[A, B], [C, D]]) . matrix([[1, 1, 2, 2], [1, 1, 2, 2], [3, 4, 7, 8], [5, 6, 9, 0]]) . The above example takes four arrays A,B,C,D and builds up a matrix by stacking A and B, C and D column wise and then stacking the arrays row wise one top of the another. . A = np.mat(&#39;12 13; 14 15&#39;) B = np.mat(&#39;23 28; 24 22&#39;) C = np.mat(&#39;34 45; 57 69&#39;) D = np.mat(&#39;7 8; 9 0&#39;) np.bmat([[B, A], [D, C]]) . matrix([[23, 28, 12, 13], [24, 22, 14, 15], [ 7, 8, 34, 45], [ 9, 0, 57, 69]]) . The above example takes the arrays A,B,C,D and builds the matrix by stacking them as per the order of the parameters given in the paranthesis of the function. . A = np.mat(&#39;12 13; 14 15 18&#39;) B = np.mat(&#39;23 28; 24 22&#39;) C = np.mat(&#39;34 45; 57 69&#39;) D = np.mat(&#39;7 8; 9 0 12&#39;) np.bmat([[B, A], [D, C]]) . ValueError Traceback (most recent call last) /tmp/ipykernel_36/1923516388.py in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) -&gt; 2 A = np.mat(&#39;12 13; 14 15 18&#39;) 3 B = np.mat(&#39;23 28; 24 22&#39;) 4 C = np.mat(&#39;34 45; 57 69&#39;) 5 D = np.mat(&#39;7 8; 9 0 12&#39;) /opt/conda/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py in asmatrix(data, dtype) 67 68 &#34;&#34;&#34; &gt; 69 return matrix(data, dtype=dtype, copy=False) 70 71 /opt/conda/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py in __new__(subtype, data, dtype, copy) 140 141 if isinstance(data, str): --&gt; 142 data = _convert_from_string(data) 143 144 # now convert data to an array /opt/conda/lib/python3.9/site-packages/numpy/matrixlib/defmatrix.py in _convert_from_string(data) 28 Ncols = len(newrow) 29 elif len(newrow) != Ncols: &gt; 30 raise ValueError(&#34;Rows not the same size.&#34;) 31 count += 1 32 newdata.append(newrow) ValueError: Rows not the same size. . It breaks because there the sizes of the rows are different so they cannot be stacked. The sizes of the rows or columns that are getting stacked must be of the same size inorder to stack them and build the matrix. To fix this we need to remove the extra rows in the arrays or add NaN element to others to make the rows of same size. . The function is useful when we have some groups of arrays and we want to stack them into a matrix to perform operations and gain insights from that build matrix. . Function 5 - np.log . The function is used to calculate log values of the elements of the array The sytax for the function is: np.log(x)-- x is array-like . np.log([1, np.e, np.e**2]) . array([0., 1., 2.]) . In the above example the function took the input parameters and calculated the individual log values of the elements and gives the output. . np.log([1, 0, np.e]) . /tmp/ipykernel_36/1939558749.py:2: RuntimeWarning: divide by zero encountered in log np.log([1,0, np.e]) . array([ 0., -inf, 1.]) . The above example demonstrates that the np.log function even takes 0 and gives the -inf as the output. It gives a warning because the function detects a zero division because of log (0) computation. . np.log([1, np.e, a]) . /tmp/ipykernel_36/1874325113.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray. np.log([1, np.e, a]) . AttributeError Traceback (most recent call last) AttributeError: &#39;int&#39; object has no attribute &#39;log&#39; The above exception was the direct cause of the following exception: TypeError Traceback (most recent call last) /tmp/ipykernel_36/1874325113.py in &lt;module&gt; 1 # Example 3 - breaking (to illustrate when it breaks) -&gt; 2 np.log([1, np.e, a]) TypeError: loop of ufunc does not support argument 0 of type int which has no callable log method . The above example breaks down because log doesn&#39;t support strings. To fix this replace string by integer or float value. . This function is useful to calculate the log values of an array so that we can manipulate the array and do log operations on the array . Conclusion . The Notebook covers the following: np.matmul function: which is used for dot product and multiplication of arrays, given the two arrays satisfies the size condition for matrix multiplication. np.linalg.solve : which is used to solve system of linear equations given the coefficient matrix is non-singular. np.column_stack: which is used to stack or concatenate two 1D arrays column-wise and give a 2D array, np.bmat: builds a matrix from strings or arrays. np.log: computed the log values of the elements in the array. The following functions are very useful while manipulating matrices and computation of matrices. . Reference Links . Provide links to your references and other interesting articles about Numpy arrays: . Numpy official tutorial : https://numpy.org/doc/stable/user/quickstart.html | Numpy offical documnetation :https://numpy.org/doc/stable/reference/ | Numpy documentation for the following functions: | 1.np.matmul : https://numpy.org/doc/stable/reference/generated/numpy.matmul.html#numpy.matmul | 2.np.linalg.solve: https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html#numpy.linalg.solve | 3.np.column_stack: https://numpy.org/doc/stable/reference/generated/numpy.column_stack.html | 4.np.bmat: https://numpy.org/doc/stable/reference/generated/numpy.bmat.html#numpy.bmat | 5.np.log: https://numpy.org/doc/stable/reference/generated/numpy.log.html#numpy.log | .",
            "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/numpy/python/linearalgebra/math/2021/10/03/Guide-to-Numpy-Operations.html",
            "relUrl": "/numpy/python/linearalgebra/math/2021/10/03/Guide-to-Numpy-Operations.html",
            "date": " • Oct 3, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am an Undergraduate student majoring in physics with a minor in math. My primary research interests are Quantum Computing and Quantum Information Theory. I am also interested in working in the domains of Quantum Machine Learning, Scientific Machine learning and other interdisciplinary domains. I am equipped with strong programming skills working with Python, C++, Julia and MATLAB . I have previous experience in machine learning projects particularly in the Deep learning domain. I have done various projects using time-series datasets and analyzing the data using standard Data science and ML Techniques. I am currently working on Scientific Machine learning projects using Julia Programming language. . On the way back from the office today, almost got into a car accident when I saw this, I had to pull off and take a picture. 🤯Am I being pranked? #JuliaLang pic.twitter.com/R52JVLTE0k . &mdash; Logan Kilpatrick 🇺🇦 (@OfficialLoganK) February 10, 2022",
          "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sandeshkatakam.github.io/My-Machine_learning-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

  
  

}